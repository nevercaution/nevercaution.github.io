{"componentChunkName":"component---src-pages-404-js","path":"/404/","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"rawMarkdownBody":"\n## 폴피리녹스\n1차 항암제는 폴피리녹스였다. 이 항암제는 2박 3일동안 병원에 있으면서 3가지 항암제를 나눠서 맞아야 한다. 그렇기에 입원을 해야만 했는데, 아산병원은 워낙에 환자가 많아 입원이 대부분 밀리곤 했다. 2주에 한번씩 입원해서 항암제를 투약해야 하지만 병실이 없을 때가 많아 예약해놓은 입원 날에 입원한 날이 그리 많지 않았다. 체력적으로 못버틸 때는 은근히 밀리기도 바라긴 했었지만 내 몸에 문제가 있어서가 아닌 단지 병실이 없다는 이유 하나만으로 항암일정이 밀리면 속으로 걱정이 되기도 했다. 이래도 괜찮을까 싶기도 하고.. 혹시나 밀려서 종양이 줄어들지 않거나 혹시나 커지면 어쩌나 하는 생각이 들었다.\n\n중간에 몇번은 담도염이 생겨 응급실에 입원하기도 했는데 아산병원에는 응급 치료실이 따로 있다. 그래서 일반적인 응급실에 있는게 아니고 6층에 나와같이 중증환자들이 응급으로 치료받을 수 있는 병실이 따로 있다. 응급 치료실은 치명적인 단점이 하나 있는데, 응급실에 들어온 후 24시간이 지나면 나의 몸상태와는 상관없이 무조건 나가야 한다는 것이다. 물론 그 사이에 병실이 생기면 입원이 가능하지만 거의 병실이 나지 않는다. 이럴 경우엔 두가지 선택을 할 수 있는데 몸이 아파도 어쩔 수 없이 집에 가는 방법과 아산병원과 제휴(?) 를 맺은 근처의 병원에 입원을 해서 치료를 진행하는 방법이 있다. 한번은 아산병원 근처에 있는 조금 작은 병원에 몇일 입원 한적이 있는데, 입원을 하고 보니 나와같이 아산병원에서 온 사람들이 꽤 있었다. 그들도 역시 아산병원에 병실이 나기를 기다리며 이 병원에 입원한 케이스였는데, 이 병원은 이런 케이스의 환자를 꽤나 흔하게 받는듯 했다. 앞뒤 사정은 모르겠으나 환자를 많이 받아서 인지 병원은 확장공사 때문에 하루종일 시끄럽고 분주했다. 그리고 여담이지만 아산병원만큼 큰 병원이 아니어서 그런지 서비스나 청결등 몇몇 부분이 부족하다고 느껴졌다. 다행히 나는 이 병원에 몇일있지 않아 아산병원으로 다시 갈 수 있었다. 별로 그 병원에 대해 좋은 기억이 없어서 그런지 다시는 가고 싶지는 않은 병원이다. (병원밥은 원래 맛이 없지만 특히 그 병원은 그 정도가 심했다. 심지어 수저도 주지 않아 개인이 따로 챙겨야 한다.)\n\n응급실에 입원할 때마다 CT를 찍는다. 내가 어디가 아프다고 하는것보다 훨씬 정확하게 내 몸의 상태를 확인할 수 있어서 인데 담도염 때문이었는지 찍을 때마다 종양의 크기가 커져있다는 소견이 나왔다. 그리고 응급실 의료진은 나에 대해 정확히 모르기 때문에 정확한 진단을 내리진 못하는듯 보였다. 입원할 때마다 내 몸속에 있는 스탠트가 잘못되어 옆구리에 관을 뚫어야 한다는 소견을 내놓곤 했는데 그러다가 주치의가 나를 다시 보면 응급실에서의 소견은 잘못된 경우가 대부분이었다.\n그러다 최근에 열이 40도까지 올라 생에 최초로 119에 전화를 걸어 응급차를 타고 응급실에 갔었다. 역시나 CT를 찍었고 지난번과 같이 옆구리에 담관즙을 받을 수 있는 관을 뚫어야 한다는 소견과 함께 종양이 커진 것 같다는 소견을 내놓았다. 나는 이번에도 역시나 응급실에서의 의견에 크게 무게를 두지 않고 있었다. 그리고 운이 좋게 새벽에 병실이 생겨 바로 입원을 할 수 있었다. 그런데 이번에는 상황이 조금 달랐다. 다행히도 옆구리에 관을 뚫지는 않았지만 종양이 단순히 염증 때문에 부은 것 같지는 않다고 했다. 그 이유로는 항생제를 쓰고 조취를 취하니 열도 잡혔기 때문이라고 했는데 주치의는 종양이 진행을 했다고 판단을 했다.\n\n여러 생각이 들었다. 나는 염증 때문에 종양이 부었다고 생각을 했고 항암제에 내성이 생겼다고는 생각하지 않았다. 하지만 이건 내 생각일 뿐이고 주치의의 판단을 전적으로 신뢰하기 때문에 내성이 생겼다는 판단에 동의를 하고 2차 항암제로 바꾼다는 문서에 서명을 했다. 조금 찜찜한 것은 주치의 역시 담도안에 있는 스탠트가 막혀 옆구리에 담관즙을 뺄 수 있는 관을 뚫자고 했었다. 나는 속으로 아.. 망했다 생각을 하고 있었는데 영상을 판독하는 쪽에서 스탠트는 문제가 없이 정상 동작을 하고 있으며 지금은 굳이 옆구리를 뚫지 않아도 된다고 했다. 이렇게 주치의의 판단이 왔다갔다 한 부분에 대해 나에 대해 정확하고 꼼꼼하게 상황을 판단하고 있는걸까 생각이 들기도 했다. 결국 폴피리녹스는 내성이 생겼다고 결론이 났고 종양은 부은게 아닌 진행이 되었다고 판단되었다. 2차 항암제는 젬자, 아브락산이다. 이 항암제 폴피리녹스와는 다른 점이 있다.\n* 장점\n    - 투약시간이 한시간정도 걸려 입원을 하지 않아도 된다. 즉 내 몸의 상태만 받아준다면 항암일정이 밀릴일은 없다.\n    - 폴피리녹스보다 비교적 강도가 약한 항암제여서 몸에 심한 부작용이 덜하다.\n    - 1주에 한번씩 투약을 해야 해서 외래를 볼 시간이 잦아졌다.\n* 단점\n    - 몸에 있는 모든 털이 빠진다고 한다. 지금까지는 겉으로는 암환자 티가 나지 않았는데 이제는 겉으로도 확 드러날 것이다.\n    - 1주에 한번씩 투약해야 하기 때문에 약의 강도는 덜해도 역시나 부작용이 부담이 된다.\n    - 2차 항암제는 보험이 되지 않아 병원비가 기하급수적으로 늘어난다.\n    - 나는 예전부터 몸살기운에 특히 약했다. 이 항암제의 부작용은 몸살기운이다..\n\n이론상 내 몸이 버텨만 준다면 일상생활을 비슷하게나마 할 수도 있을 것이다. 하지만 지난번에 심한 설사를 겪고나서부터 몸무게가 5키로 더 줄었고 지금은 밥을 반공기 이상만 먹어도 소화를 시키지 못해 구토를 한다. 이는 십이지장 스탠트가 막혀서 일까 생각도 했다. 외래에서 강력하게 얘기해볼 생각이다. 이전에 병원에 입원해 있을 때도 꽤나 자세하게 얘기했지만 워낙에 주치의를 만나기 힘들어서 잘 전달이 안된듯 하다. 심하게 설사를 하기전에는 밥은 잘먹었다. 심지어 명동교자에서 국수 사리를 시켜 국물까지 다먹기도 했었다. 항암 부작용 때문에 위가 정상적으로 동작하지 못하는 것도 있겠지만 그보다는 스탠트의 문제가 있지 않을까 하는게 내 생각이다.\n항암제를 바꿔서 병원비가 백단위로 뛰었다. 그래서 큰 부담이 되지만 그래도 희망적인 것은 2차 항암제가 잘 듣는듯한 느낌이다. 항암제 투약을 한번뿐이 하지 않았긴 해서 느낌적인 느낌이지만 통증도 많이 줄었고 몸속에 뭔가가 움직이는 듯한 느낌이 온다. 이대로 종양이 줄어 수술을 할 수 있게 될것이고 좀 더 좋은 케이스는 완전관해를 목표로도 할 수 있곘다. 주치의는 2차 항암제 역시 어려운 싸움이라고 했지만 느낌이 좋다.\n\n## 링피트\n11월이 지나가고 있고 수능은 끝이 났다. 이제 겨울이 왔다. 소화를 잘 시키지 못해 항상 밥을 먹으면 소화를 시키고자 한시간 정도 걷는다. 그런데 이제 추워져서 옷을 단단히 입고 나가야 한다. 낮에는 햇빛이 있어 그나마 따뜻한데 밤에는 해도 빨리 떨어지고 추위도 더 강해진다. 그러다가 링피트를 발견했다. 홈피트니스란다. 집에서 운동을 할 수 있다는 말에 냉큼 구매를 했다. 집에 도착하자마자 티비에 연결해서 플레이를 해보았다. 나름 스토리도 있고 몬스터를 때리려면 운동을 해야한다. 그래픽이 뛰어나거나 하진 않지만 집에서 운동을 할 수 있고 게임을 하다보면 자연스럽게 운동을 할 수 있어 꽤나 만족스럽다. 내가 운동을 하고 있다기 보단 게임을 즐기고 있다는 느낌이 강하다. 이제 점점 더 추워져 밖에 나가기 힘들어 질듯 한데 질리지 않고 꾸준히 해볼 생각이다.\n\n## 끝으로..\n앞으로 머리카락도 빠지고 눈썹도 없어질텐데 그런 것에는 아직 큰 의미를 두지 않는다. 다만 내가 먹고 싶은 음식을 잘 먹고 몸무게가 좀 늘었으면 좋겠다. 지금은 조금씩 꾸준히 먹으면서 체력을 보충할 생각이다. 다시 밥도 한그릇 뚝딱 먹고 도서관 가서 허고 싶은 공부도 계속 하고 싶다. 요 근래 계속 병원에 가고 몸상태도 좋지 않아서 공부에 많이 소흘해졌다. 물론 여전히 종양이 줄어들어 완전관해가 되거나 수술이 가능해지는게 첫번째 목표이다.","fields":{"slug":"/diary-3/"},"frontmatter":{"title":"2차 항암과 링피트","published":true}}},{"node":{"rawMarkdownBody":"\n## 통잔 잔고가 줄어간다.\n병원생활을 하면 돈이 많이 든다. 지금은 정말로 다행이도 회사에서 실비 보험을 지원해주고 있기 때문에 병원비 부담으로 인해 가계가 기울지는 않고 있다. 그리고 중증 병에 걸리면 나라에서 병원비를 깎아주기 때문에 병원비가 크게 줄어 든다. 그런데 왜 통장 잔고가 줄어드는가?\n그 이유는 병가를 냈기 때문에 아프기 전 보다 수입이 크게 줄었기 때문이다. 회사에서는 병가를 최대 1년까지 낼 수 있고 그 후로는 휴직으로 전환된다. 휴직으로 전환되면 그나마 있던 수입이 제로가 된다. 정기적으로 나가는 돈은 그대로 이지만 휴직으로 전환이 되면 정말 손가락만 빨고 살아야 할지도 모른다. 그러면 지금껏 돈이 어디로 나갔는지 생각을 해보자.\n* 병원비\n    - 병원비를 제외한 약값이나 비타민, 의료 보조제, 의료 용품등\n* 취미 비용\n    - 전자드럼\n    - 몸무게가 크게 줄어들어 새로 산 옷들\n    - 사고 싶었던 브랜드의 옷들\n* 고정 생활비\n    - 고정적으로 나가는 대출 이자와 가스비, 전기세등 숨만 쉬어도 나가는 돈\n    - 먹고 살기 위한 식료품과 생활용품들 구매\n\n처음에는 집에서만 생활을 하면서 심심하기도 했고 병에 대한 원망의 표출을 소비로 풀곤했다. 대책없는 소비에 대해 비난을 하면 할말은 없지만 중병을 얻었다는 이유로 그간 사고 싶었던 옷이나 물건들을 큰 고민 없이 샀다. 물론 그런 소비를 했다고 지금 후회를 하는건 절대 아니다. 사고 싶었던 것들이었고 그래도 현재는 수입이 조금이나마 있기 때문에 그 값을 치룰 수 있을 것이라 생각했기 때문이다. 그런데 기본 생활이 외에도 병원비 뿐만 아니라 약값이나 기타 암치료에 대한 돈이 은근히 든다.\n지금까지는 소비에 대한 큰 고민이나 계획을 세우지 않고 생활했었는데 최근에 카드값을 지불하면서 통장잔고가 뚝뚝 줄어들어가는 것을 보며 계획없이 살다간 나중엔 정말로 돈이 없는 최악의 상황에 마주하게 될 것이란 생각이 들었다. 물론 그런 상황이 오기전에 종양이 줄어들고 없어져 완치가 되거나 수술을 통해 다시 사회생활을 할 수 있게 되면 최악의 상황을 마주하진 않게 될것이다. 하지만 지금부터라도 최악의 상황에 대비해 준비를 할 필요가 있다.\n\n## 그러면 어떻게 앞으로를 대처해야 할까?\n최근 작은 책상과 의자를 샀다. 방에 놓고 컴퓨터를 올려놓고 사용하거나 책을 읽기 위한 목적이다. 평소에 몸 상태가 괜찮을 땐 집근처 도서관에 가서 공부를 하거나 하는데 집에서도 어느정도 작업을 할 수 있는 공간이 필요하다고 생각했다. 그래서 지금까지는 업무와 관련된 공부나 지금껏 하고 싶었던 부분의 공부를 했었는데 앞으로는 알바(?) 같은 걸로 조금이나마 수입에 도움이 되는 작업을 할 생각이다. 지금 당장 시작하기에는 정보가 너무 없기 때문에 처음엔 비교적 간단하게 시작할 수 있는 일을 찾아볼 생각이다. 그리고 공부하는 방향도 그런 작업들에 도움이 될법한 파트를 볼 생각이다. 내가 약한 프론트 엔드쪽을 봐야 할지는 아직 모르겠다. 어떤 것을 공부하고 준비해야 하는지 대략적으로 알아볼 생각이다. 물론 미리 준비를 한다고 해서 모든 부분에 대응을 할수는 없겠지만 그래도 조사를 통해 뭘 해야 일거리를 구할 수 있는지 찾아봐야 겠다.\n\n## 계획을 세워야 겠다.\n아프고 나서 술을 마시지 않으니 생각할 수 있는 시간이 많아졌다. 그리고 밤에 자기 위해 누워 눈을 감으면 마음이 편안해지고 무한한 상상을 한다. 머릿속에는 정말로 많은 생각들이 스쳐 지나간다. 지금 나는 하고 싶은 것이 많다. 앞으로는 생각들과 하고자 하는 일들에 대해 중구난방으로 할게 아니라 계획을 세워 해야겠다. 큰 일을 하기 위해 일을 작게 쪼개고, 작은 일들을 하나씩 해 나가야 겠다. 여담 이지만 wunderlist 나 최근에는 애플의 미리알림어플등을 유용하게 쓰고 있다. 그래도 어플보다는 집에 메모공간을 마련해서 언제든 내가 해야할 목표를 적어놔야 겠다. 눈에 잘 보이는 곳에 메모공간을 두고 해야할 일들을 적어놓는 건 효과가 좋다. 메모의 힘은 크다. 지금까지 직접 내 손으로 해야할 일들을 적으면 대부분 이루워 졌다. (물론 목표를 적는 메모에는 `완치`를 가장 먼저 적어 놓을 것이다!)\n\n머릿속에 생각이 많다. 기록해야 할 것들도 많은데 일단은 생각의 정리가 필요하다. 그래도 두서없이 적으면 생각의 정리가 조금은 되기 때문에 일기는 막(?) 쓸 생각이다.\n그런데 과연 알바를 하는게 최선일지는 아직 잘 모르겠다. 지금도 생각이 오락가락하다. 재미있을 것 같기도 하면서 한쪽으로는 조금 두렵기도 하다.\n\n\n\n\n\n\n\n\n\n\n\n","fields":{"slug":"/diary-2/"},"frontmatter":{"title":"두번째 일기","published":true}}},{"node":{"rawMarkdownBody":"\n## 아프고 나서\n암선고를 받은지 4개월 정도 되었다. 회사엔 병가를 냈다. 집에만 계속 있다보니 답답한 마음이 크다. 항암제를 투여받기 위해 2주마다 병원에 입원을 해야하고 입원하고 나서 최소 2박 3일 입원을 해야하기 때문에 아무래도 회사에 다시 복귀하는 건 쉽지 않아보인다. 그리고 몇번 응급실도 갔었고 퇴원을 하더라도 항암의 부작용 때문에 몇일은 집에서 골골거리고 있다. 회사에 다시 일하고 싶다고는 몇번 얘기를 했었는데, 다른 사람들이 불편해할 수도 있다고 해서 현재는 회사 메신저를 읽어보기나 할 뿐 회사일에 아무것도 관여하고 있지 않는다. 당장 출근을 해서 아프기 전처럼 일을 하지는 못하겠지만 재택근무라도 하고 싶은 마음이 있긴 한데 몸이 따라주질 않으니 속상하기도 하고 이런 내 자신에 화가 나기도 한다. 그래서 아쉬운 대로 집 근처 도서관에 가서 회사일 관련 공부를 하고 있다. 도움이 되는 인강도 몇개 봤는데 생각보다 유용해서 가끔 회사분들 만나서 인강을 추천하기도 했다. 몇몇 내용은 포스팅을 작성하기도 했는데, 내가 알고 있다고 생각했던 내용이라고 생각했던 것들도 직접 설명을 하려고 하니 생각보다 쉽지 않더라. 디테일도 신경써야 하고 정확하게 기술하는게 은근 쉽지 않다. 내 시점으로만 작성을 하려니 내용이 장황해지기도 하고 삼천포로 흘러가기도 한다. 고기도 먹어본 사람이 잘 안다고 글도 많이 써야 느는듯 듯 하다.\n\n지금까지 회사 관련된 기술 포스팅 말고도 개인적인 생각이나 일기도 써야지 생각만 해왔는데 계속해서 미루다가 이번 기회에 다시 한번 도전해볼까 한다. 요즘 드는 기분이나 생각들을 기록하는 것도 좋아 보인다. 나중에 보면 재미 있지 않을까 하는 생각도 있다. 처음에는 일기를 종이 일기장에 쓸까 고민했었다. 습관까지는 아니지만 예전부터 힘든일이 있을 때 종종 쓰곤 했는데, 그렇다 보니 일기를 쓰는 주기가 너무 길어지곤 했다. 집에는 지금까지 써왔던 종이 일기장을 보면 처음 써왔던 시기는 꽤나 옛날이지만 글이 많이 없다. 아무래도 평소에는 손이 잘 가지 않는다.  대신 블로그에 일기를 쓰면 좋겠다 싶은데, 온라인 상에 글을 쓰는건 성격이 아주 같지는 않겠지만 그래도 뭐라도 써놓고 보면 속이 시원한 느낌이다.\n\n처음에 아프고 나서는 집에서 아무것도 하지 않았다. 췌장염과 십이지장염등이 와서 몸이 많이 안좋기도 했었다. 무엇보다 암을 부정했고 심적으로도 꽤나 날이 서있었기 때문에 아무것도 하고 싶은 생각이 들지 않았다. 아프기 전에는 보지도 않았던 티비를 붙잡고 하루종일 영화채널을 보거나 재미없는 예능들을 보기만 했었는데 요새는 그나마 몸이 조금 나아져서 뭐라도 하고 싶단 생각이 든다. 그중 하나가 개발 관련 공부를 하는건데, 도서관에 가서 공부를 하고 있으면 정말이지 시간가는 줄 모르고 공부를 한다. 이 때만큼은 내가 정상인인 것 같고 아픈 생각이 들지 않는다. 공부해야 할 것들을 정리하고 하나씩 보고 있는데 하고 싶고 공부하고 싶은 것들이 너무 많다. 아프기전에는 짬을 내서 공부하느라 시간이 많이 없어 조금씩 밖에 보지 못했는데 요새는 몸 상태만 괜찮으면 하루종일 볼 수 있으니 꽤나 만족스럽다. 나중에 회사에 다시 복귀 하기 되면 아무래도 지금만큼 재미가 있진 않겠지. 회사에서는 내가 하고 싶은 것만 할 수는 없으니까..\n\n## 집에서 할 수 있는 것\n공부를 할 땐 무조건 도서관이나 동네 카페에 가서 했었는데 집에도 작업을 할 수 있는 환경을 만들어 놓는게 좋다고 생각이 들었다. 그렇다고 집에만 틀어박혀 있지는 않겠지만 컴퓨터 뿐 아니라 책을 읽을 책상 정도는 필요하다고 생각을 해서 집에 작은 책상을 하나 사려고 한다. 그리고 살까 말까 고민하는 것들이 있는데 회사에서만 썼던 해피해킹 키보드를 하나 살까 고민중이다. 키보드를 사야하는 이유에 대해서는 아는 사람들만 알 것이다. 병가를 내서 수입도 확 줄고 병원비가 많이 들긴 하는데 갈 때 내가 모아놨던 돈 들고 가는 것도 아닌데 하는 (위험한) 생각을 하면 못 살것도 없다. 이미 이런 마인드로 집에 전자드럼도 샀다. 몸 상태가 괜찮을 때마다 치고 있는데, 재미가 쏠쏠하다. 나중엔 직장인 밴드도 하고 싶다. 그리고 아이패드도 살까 말까 엄청 고민중이다. 인강을 듣거나 간단한 글을 작성하는 수준이라면 아이패드로도 충분히 할 수 있어서 이다. 그러기 위해 가능할지는 모르겠지만 집에서 놀고 있는 라즈베리파이도 다시 세팅해볼 생각이다. 그리고 아이패드에서 터미널 어플을 쓰면 간단한 작업정도를 할 수 있지 않을까 생각도 든다. 아이패드나 키보드는 살지 말지 아직 모른다. 마음같아서는 바로 사고 싶지만 좀 더 내 의지를 지켜봐야 겠다. 그래도 책상은 하나 살 생각이다. 수입이 줄었기 때문에 회사에서 받아주지 않는다면 알바라도 해야하기 때문이다. 숨만 쉬어도 대출이자가 나가고 먹고 살아야 하기 때문에 뭐라도 해야 한다.. 아직 해보지는 않았지만 모아놨던 돈을 까먹고 있는 상태에서 통장에 돈이 줄어들 때마다 마음이 불안해진다.\n\n일기를 너무 자주 쓰지는 않겠지만 그렇다고 너무 뜸하게 쓰지는 않도록 노력하려고 한다. 쓸말이 없고 내용이 짧아도 되도록 글을 써볼까 하는 생각이다.\n\n\n\n","fields":{"slug":"/diary-1/"},"frontmatter":{"title":"첫번 째 일기","published":true}}},{"node":{"rawMarkdownBody":"\n## spring boot starter\n`spring-boot-starter` 는 의존성과 설정들을 자동으로 해주는 모듈이다. 이미 `spring-boot` 를 사용하고 있는 프로젝트들이라면 기본적으로 포함되어 있는 모듈들이다. 기본적으로 설정해주어야 하는 의존성들을 잡아주고 필요시에는 해당 프로젝트에서 재정의가 가능하다.\n기본적으로 대부분의 의존성들은 `spring-boot-starter` 에 정의되어 있다. 그럼에도 새로운 프로젝트를 생성할 때 이 모든 의존성이 잡히지 않는 이유는 사용자가 해당 기능을 사용하고자 할 때만 의존성들이 들어가고 설정들이 먹히기 때문이다.\n`spring-boot-starter` 는 다음과 같이 구성할 수 있다.\n\n```\nspring-boot-autoconfigure\nspring-boot-starter\n```\n`autoconfigure` 와 `starter` 모듈로 나눠지는데, 하나로 합쳐서 만들 수도 있다.\n[naming 규칙이 조금 까다로운데](https://docs.spring.io/spring-boot/docs/current/reference/html/using-boot-build-systems.html#using-boot-starter) 왠만하면 맞춰서 만드는게 좋을듯 하다. 조금만 훑어보면 다른 thirdparty 프로젝트들 중에 naming 규칙을 따르지 않은 모듈들도 간혹 보이는데 아마도 이러한 규칙이 미처 정의되기 전에 만들어졌기 때문일 것이다. 우리가 만들 starter 는 충실히 공식문서의 제안에 따르도록 하자.\n\n## customize\n위에서 봤듯이 `starter` 의 구성에 따라 프로젝트 구조를 다르게 가져갈 수 있다. 프로젝트 구조는 [spring boot starter github](https://github.com/spring-projects/spring-boot/tree/master/spring-boot-project/spring-boot-starters) 를 참고해서 구조를 잡고 `spring-boot-starter` 를 작성해본다. `starter` 모듈들이 여러개로 늘어날 수 있기 때문에 `custom-spring-boot-starters` 로 한번 더 묶어주도록 한다.\n\n### 구조\n보통 프로젝트 설정시 한개 이상의 의존성 설정을 잡을 확률이 높기 때문에 예제에서는 두개의 모듈을 만들고 `starter` 와 연결해서 만들어 본다. 기본적은 프로젝트 구조는 다음과 같다.\n\n```\n.\n└── spring-boot-project\n    │\n    ├── custom-logger\n    ├── page-database\n    │\n    ├── custom-spring-boot-autoconfigure\n    ├── custom-spring-boot-starters\n    │   ├── custom-spring-boot-starter-custom-log\n    │   └── custom-spring-boot-starter-page-database\n    │\n    └── sample-web\n```\n\n* `custom-logger` 와 `page-database` 는 외부 라이브러리이다.\n\t* `custom-logger` : 사용자 임의의 로그 서비스\n\t* `page-database` : 공용 저장소에 접근하는 서비스\n* `custom-spring-boot-autoconfigure` 는 모듈의 의존성을 잡아준다.\n* `custom-spring-boot-starters` 는 `cusom-starter` 의 pom 들을 하나로 묶어준다.\n* `custom-spring-boot-starter-*` 는 프로젝트에서 사용할 수 있는 모듈 의존성을 잡아준다.\n* `sample-web` 은 위의 만들어진 두개의 모듈을 사용할 예제 프로젝트이다.\n\n>`autconfigure` 에서의 의존성은 모듈에서 사용할 repository 들에 대한 의존성을 뜻하고,\n>`starter` 에서의 의존성은 `sample-web` 에서 모듈을 사용할 수 있게 하는 모듈 의존성의 묶음이다.\n>각 `starters` 안의 `custom-spring-boot-starter` 는 `sample-web` 에서 사용할 수 있도록 의존성을 묶어주는 pom 덩어리이다.\n\n\n## 구현해보자\n예제 프로젝트에서는 외부 모듈과 `sample-web` 이 하나의 프로젝트에 같이 포함이 되어 있는데, 실제로 제작하고 사용할 때는 `starters` 와 `autoconfigure` 만 있게 된다. `custom-logger` 와 `page-database` 두개의 외부 모듈을 사용해서 각 `starter` 를 만들어 주고 `autoconfigure` 에서 의존성과 설정을 잡아준다. 구현 순서는 다음과 같다.\n\n* 전체 구조를 잡아줄 `spring-boot-project` 를 잡아준다.\n* 외부 모듈과 연결해서 의존성과 configuration 을 잡아줄 `autoconfigure` 를 만든다.\n* `autoconfigure` 와 외부모듈을 의존성으로 가진 pom 프로젝트 `starter` 를 만든다.\n* 각 `starter` 들을 하나로 묶어줄 `spring-boot-starters` 를 만든다.\n\n### spring boot project\n`spring-boot-starter` 를 묶어주는 부모 프로젝트이다. 프로젝트를 처음 생성하면 기본적으로 [spring-boot-parent](https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#using-boot-maven) 로 정의되어 있다. 여러가지 기능을 제공하지만 `starter` 에서는 전체기능이 필요하지 않기 때문에 부모를 직접 정의해서 사용하도록 한다. 부모 프로젝트를 직접 정의하기 때문에 빈껍데기만 있게 된다. [의존성 관리](https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#using-boot-dependency-management) 는 추가해주도록 한다.\n\n> `spring-boot-project > pom.xml`\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>com.nevercaution.boot</groupId>\n    <artifactId>spring-boot-project</artifactId>\n    <version>0.0.1-SNAPSHOT</version>\n    <name>Custom Spring Boot Project</name>\n    <packaging>pom</packaging>\n\n\n    <properties>\n        <java.version>11</java.version>\n        <spring-boot.version>2.1.8.RELEASE</spring-boot.version>\n    </properties>\n\n    <modules>\n        <!-- Starter module -->\n        <module>custom-spring-boot-autoconfigure</module>\n        <module>custom-spring-boot-starters</module>\n\n        <!-- custom modules -->\n        <module>custom-logger</module>\n        <module>page-database</module>\n\n        <!--  sample project -->\n        <module>sample-web</module>\n\n    </modules>\n    <dependencyManagement>\n        <dependencies>\n            <dependency>\n                <!-- Import dependency management from Spring Boot -->\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-dependencies</artifactId>\n                <version>${spring-boot.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n</project>\n```\n\n\n이제 이 maven pom 타입의 프로젝트를 부모로 사용하도록 한다. 필요한 property 들은 여기서 정의한다.\n\n### autoconfigure\n모듈에 필요한 의존성들을 가져오고 필요한 설정들을 해줄 수 있다. 정의된 모듈들의 의존성은 `optional` 로 설정되어 있어 사용자가 원하는 모듈만 선택적으로 사용할 수 있다.\n\n> `autoconfigure > pom.xml`\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n    <parent>\n        <groupId>com.nevercaution.boot</groupId>\n        <artifactId>spring-boot-project</artifactId>\n        <version>0.0.1-SNAPSHOT</version>\n    </parent>\n\n    <artifactId>custom-spring-boot-autoconfigure</artifactId>\n    <version>0.0.1-SNAPSHOT</version>\n    <name>Custom Spring Boot AutoConfigure</name>\n    <packaging>jar</packaging>\n\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <maven.compiler.source>1.8</maven.compiler.source>\n        <maven.compiler.target>1.8</maven.compiler.target>\n    </properties>\n\n    <dependencies>\n        <!-- Compile -->\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-autoconfigure</artifactId>\n        </dependency>\n        <!-- Custom log -->\n        <dependency>\n            <groupId>com.nevercaution.modules</groupId>\n            <artifactId>custom-logger</artifactId>\n            <version>0.0.1-SNAPSHOT</version>\n            <optional>true</optional>\n        </dependency>\n        <dependency>\n            <groupId>org.projectlombok</groupId>\n            <artifactId>lombok</artifactId>\n            <optional>true</optional>\n        </dependency>\n        <dependency>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-api</artifactId>\n            <optional>true</optional>\n        </dependency>\n        <!-- Database  -->\n        <dependency>\n            <groupId>com.nevercaution.modules</groupId>\n            <artifactId>page-database</artifactId>\n            <version>0.0.1-SNAPSHOT</version>\n            <optional>true</optional>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-data-jpa</artifactId>\n            <optional>true</optional>\n        </dependency>\n        <dependency>\n            <groupId>mysql</groupId>\n            <artifactId>mysql-connector-java</artifactId>\n            <version>8.0.17</version>\n            <optional>true</optional>\n        </dependency>\n\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n</project>\n```\n`custom-logger` 와 `database` 로 의존성을 각각 가져왔다. 각자 해당 외부 라이브러리를 걸었고 `database` 의 경우에는 모듈에 필요한 의존성을 추가로 정의했다. 모든 의존성은 `optional` 로 걸어준다.\n\n\n### configuration\n`starter` 에서 설정에 필요한 값들을 받아와서 설정을 하고 bean 을 생성해 넘겨주는 역할을 한다.\n\n> `autoconfigure > CustomLogProperties.java`\n\n```java\n@ConfigurationProperties(prefix = CustomLogProperties.CUSTOM_LOG_PREFIX)\npublic class CustomLogProperties {\n    public static final String CUSTOM_LOG_PREFIX = \"custom-log\";\n\n    private String name;\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n}\n```\n\n> `autoconfigure > CustomLogConfiguration.java`\n\n```java\n@Configuration\n@ConditionalOnClass({CustomLogService.class})\n@ConditionalOnProperty(prefix = \"custom-log\", name = {\"name\"})\n@EnableConfigurationProperties(CustomLogProperties.class)\npublic class CustomLogConfiguration {\n\n    @Autowired\n    private CustomLogProperties properties;\n\n    @Bean\n    @ConditionalOnMissingBean\n    public CustomLogService customLogService() {\n        return new CustomLogService(properties.getName());\n    }\n}\n```\n`custom-log.name` 에 정의된 값을 property 로 받아올 수 있게 설정하고 configuration 에서 필요한 service 에게 넘겨 bean 을 생성해주면 된다. 받아온 설정값을 사용하고자 하는 service 에 넘겨 bean 을 만들어 준다. 사용된 configuration annotation 들을 간단하게 정리해본다.\n`ConditionalOnProperty` 는 해당 속성값이 있을 때 동작한다.\n`EnableConfigurationProperties` 에 정의된 class 에 속성값이 정의되어 있고 그 값을 받아올 수 있다.\n`ConditionalOnMissingBean` 는 해당 bean 이 정의가 되지 않았을 경우에 동작한다. 만약 다른 곳에서 CustomLogService 를 정의했다면 해당 bean 은 생성되지 않는다.\n\n### resources\n\n> `autoconfigure > application.properties`\n\n```properties\ncustom-log.name=teddy\n\ncustom.datasource.username=user\ncustom.datasource.password=password\n```\n`autoconfigure` 에서 정의된 변수들의 기본값들을 정의할 수 있다. 이 곳에 기재된 값들을 `sample-web` 등에서 (재)정의해서 사용할 수 있고, 정의된 속성들은 위의 `configuration` 에서 사용된다.\n\n> `autoconfigure > META-INF/spring-configuration-metadata.json`\n\n```json\n{\n  \"properties\": [\n    {\n      \"sourceType\": \"com.nevercaution.boot.autoconfigure.config.log.CustomLogProperties\",\n      \"name\": \"custom-log.name\",\n      \"type\": \"java.lang.String\",\n      \"description\": \"custom log configuration\"\n    },\n    {\n      \"name\": \"custom.datasource.username\",\n      \"type\": \"java.lang.String\",\n      \"description\": \"for JPA configuration\"\n    },\n    {\n      \"name\": \"custom.datasource.password\",\n      \"type\": \"java.lang.String\",\n      \"description\": \"for JPA configuration\"\n    }\n  ]\n}\n```\n`application.properties` 에 정의된 속성들에 대한 정의를 할 수 있다. type 은 java 에서 기본적으로 정의된 타입을 사용할 수도 있고 사용자가 정의한 타입으로도 사용할 수 있다. 이 속성들을 꼼꼼히 작성해주면 IDE 에서 제공하는 자동완성으로 편하게 속성값들을 정의할 수 있다.\n\n\n> `autoconfigure > META-INF/spring.factories`\n\n```factories\norg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\\n  com.nevercaution.boot.autoconfigure.config.log.CustomLogConfiguration, \\\n  com.nevercaution.boot.autoconfigure.config.database.PageDataSourceConfiguration\n```\n\n프로젝트가 빌드될 때 `EnableAutoConfiguration` 에 사용되어질 configuration 들을 명시해준다. boot 에서 따라가야할 설정들을 정의해주면 된다. (boot 에서 annotation 만으로 이 설정값을 잡아줄 수 있었으면 좋았을텐데..)\n\n### custom-spring-boot-starters\n여러 `starter` 의 pom 들을 하나로 묶어주는 모듈이다. 만들고자 하는 `starter` 가 하나라면 굳이 만들지 않고 `starter` 만 있으면 된다. 하지만 1개 이상의 `starter` 들이 모여 있다면 하나로 묶어주는 편이 좋겠다.\n\n> `spring-boot-starters > pom.xml`\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n    <parent>\n        <groupId>com.nevercaution.boot</groupId>\n        <artifactId>spring-boot-project</artifactId>\n        <version>0.0.1-SNAPSHOT</version>\n    </parent>\n\n    <artifactId>custom-spring-boot-starters</artifactId>\n    <version>0.0.1-SNAPSHOT</version>\n    <name>Custom Spring Boot Starters</name>\n    <packaging>pom</packaging>\n\n    <modules>\n        <module>custom-spring-boot-starter-custom-log</module>\n        <module>custom-spring-boot-starter-page-database</module>\n    </modules>\n</project>\n```\n\n### custom-spring-boot-starter-custom-log\n`autoconfigure` 가 만들어 졌으면 `starter` 를 만들어 주면 된다.\n\n> `custom-spring-boot-starter-custom-log > pom.xml`\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n    <parent>\n        <groupId>com.nevercaution.boot</groupId>\n        <artifactId>custom-spring-boot-starters</artifactId>\n        <version>0.0.1-SNAPSHOT</version>\n    </parent>\n\n    <artifactId>custom-spring-boot-starter-custom-log</artifactId>\n    <version>0.0.1-SNAPSHOT</version>\n    <name>Custom Spring Boot Starter Custom Log</name>\n\n    <dependencies>\n        <dependency>\n            <groupId>com.nevercaution.modules</groupId>\n            <artifactId>custom-logger</artifactId>\n            <version>0.0.1-SNAPSHOT</version>\n        </dependency>\n        <dependency>\n            <groupId>com.nevercaution.boot</groupId>\n            <artifactId>custom-spring-boot-autoconfigure</artifactId>\n            <version>0.0.1-SNAPSHOT</version>\n        </dependency>\n    </dependencies>\n</project>\n```\n\n위에서 정의한 부모프로젝트로 잡아주고 `autoconfigure` 에서 정의한 의존성을 그대로 가져오면서 `optional` 을 제거해주면 된다. 이 설정파일에서는 필요한 의존성들은 이미 `custom-logger` 쪽에 정의가 되어 있다.\n\n\n### sample-web\n자! 드디어 다 만들었다! 만들어진 `starter` 를 예제 프로젝트에 붙여서 사용하기만 하면 된다.\n\n> `sample-web pom.xml`\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>2.1.8.RELEASE</version>\n        <relativePath/> <!-- lookup parent from repository -->\n    </parent>\n\n    <groupId>com.nevercaution</groupId>\n    <artifactId>sample-web</artifactId>\n    <version>0.0.1-SNAPSHOT</version>\n    <name>sample-web</name>\n    <description>Demo project for Spring Boot</description>\n\n    <properties>\n        <java.version>11</java.version>\n    </properties>\n\n    <dependencies>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter</artifactId>\n        </dependency>\n\n        <dependency>\n            <groupId>com.nevercaution.boot</groupId>\n            <artifactId>custom-spring-boot-starter-custom-log</artifactId>\n            <version>0.0.1-SNAPSHOT</version>\n        </dependency>\n        <dependency>\n            <groupId>com.nevercaution.boot</groupId>\n            <artifactId>custom-spring-boot-starter-page-database</artifactId>\n            <version>0.0.1-SNAPSHOT</version>\n        </dependency>\n\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n</project>\n```\n\n의존성은 다른 `spring-boot-starter` 와 같이 걸어주면 된다. 개발을 위해 로컬에서 테스트를 해야 할 때는 로컬 저장소에 jar 파일들이 있어야 하므로 `mvn install` 을 해주면 된다.\n\n> `sample-web application.properties`\n\n```properties\n# custom log\ncustom-log.name=k-page\n\n# data source\ncustom.datasource.username=user\ncustom.datasource.password=password\ncustom.datasource.url=jdbc:mysql://${MYSQL_HOST:localhost}:3306/test?useTimezone=true&serverTimezone=UTC\ncustom.datasource.hibernate-dialect=org.hibernate.dialect.MySQL5InnoDBDialect\n```\n\n위에서 설명했듯이 각 `starter` configuration 의 설정들을 잡아줄 수 있다. 그리고 `spring.factories` 를 잘 적어주면 IDE 에서 자동완성이 잘 된다. :)\n\n자 이제 실행해보면 `sample-web Application` 에서 우리가 만든 `starter` 들을 가져오고 실제로 bean 들이 생성되었는지를 확인해볼 수 있다.\n\n> `sample-web SampleWebApplication.java`\n\n```java\n\n@SpringBootApplication\npublic class SampleWebApplication implements ApplicationRunner {\n\n    @Autowired\n    private CustomLogService customLogService;\n\n    @Autowired\n    private ApplicationContext context;\n\n    public static void main(String[] args) {\n        SpringApplication.run(SampleWebApplication.class, args);\n    }\n\n    @Override\n    public void run(ApplicationArguments args) throws Exception {\n\n        List.of(context.getBeanDefinitionNames()).forEach(b -> {\n            System.out.println(\"bean name : \" + b);\n        });\n\n        customLogService.addLog(\"hello log!\", LogLevel.INFO);\n    }\n}\n```\n\n> `Application log`\n\n```bash\n2019-10-04 11:57:06.575  INFO 56866 --- [           main] c.n.sampleweb.SampleWebApplication       : Started SampleWebApplication in 4.039 seconds (JVM running for 5.688)\n\n...\nbean name : sampleWebApplication\nbean name : pageUserService\nbean name : pageUserRepository\nbean name : custom.datasource-com.nevercaution.boot.autoconfigure.config.database.PageDataSourceProperties\nbean name : com.nevercaution.boot.autoconfigure.config.log.CustomLogConfiguration\nbean name : customLogService\nbean name : custom-log-com.nevercaution.boot.autoconfigure.config.log.CustomLogProperties\n...\n\n2019-10-04 11:57:06.579  INFO 56866 --- [           main] c.n.m.customlogger.CustomLogService      : INFO, custom Log. k-page, hello log!\n```\n우리가 등록한 `service` 자 잘 올라간 것을 확인할 수 있다.\n\n\n\n## 결론\n귀찮거나 복잡한 설정들과 의존성을 잡아주기 때문에 `starter` 로 만들어 사용하게 되면 개발에 좀 더 집중할 수 있다. 지금까지는 여러팀에서 같은 모듈을 두고 설정을 복붙해서 만들거나 필요한 부분만 발췌해서 사용해왔는데 이런 수고가 적게 든다. `starter` 를 만들 때 설정값들을 필수로 받을 수도 있지만 사용자가 딱히 재정의를 하지 않더라도 기본적인 설정값들은 디폴트로 줄 수 있다. 굳이 설정값을 받아야 하는 모듈과 그렇지 않은 모듈을 잘 판단해서 만드는 것이 좋다. (만약 저장소를 디폴트로 엉뚱한 주소를 잡아놨다면 이는 빌드시에는 별다른 문제가 없겠지만 런타임에는 반드시 문제가 될 것이다.)\n정리하면 모든 모듈을 반드시 `starter` 로 만들어야 할 필요는 없다. 적절히 팀에서 필요한 선을 정해두고 만들어서 사용해야 용이할 것이다. 모듈들이 각자 쪼개져서 관리가 되는 구조이기 때문에 모듈이 빈번하게 변경되거나 버그 수정에 있어서는 오히려 생산성이 떨어질 수 있다. 결국 내부 팀에서 적절한 선에서 사용하는 것이 제일 좋을 것이다. 위의 예제코드는 [custom-spring-boot-starter-sample](https://github.com/nevercaution/custom-spring-boot-starter-sample) 에 올려놓았다.\n\n\n\n\n\n#### 여담\n처음에는 아주 작은 기능을 하는 모듈을 만들어서 `starter` 를 만들고 끝내려고 했는데 데이터베이스와 repository 를 묶어서 제공하는 모듈을 만들다가 configuration 부분에서 시간을 많이 잡아먹었다. 만들면서도 이런 모듈은 만드는게 아닌가 하는 생각이 계속 들었지만 이미 칼을 뽑았기 때문에 완전하지는 않지만 결국 만들게 되었고 spring 을 조금 더 이해하는데 도움이 되었으니 삽질에 대한 의미는 있었다. -_-ㅋ\n\n\n\n\n\n\n\n","fields":{"slug":"/spring-boot-starter-custom/"},"frontmatter":{"title":"Spring Boot Starter Customize 하기","published":true}}},{"node":{"rawMarkdownBody":"\n### 왜 만들었냐면..\n이미 spring 에서 제공하는 [@Cacheable](https://docs.spring.io/spring/docs/4.1.x/spring-framework-reference/html/cache.html) 이 있다. 기능도 다양하고 [Spel](https://docs.spring.io/spring/docs/4.3.10.RELEASE/spring-framework-reference/html/expressions.html) 를 제공해서 좀 더 동적으로 캐싱을 할 수 있다. 캐싱을 받아주는 구현체만 추가해주면 되는데 redis 의 경우엔 [spring-data-redis](https://docs.spring.io/spring-data/data-redis/docs/current/reference/html/) 를 함께 사용하면 된다.\n처음에는 @Cacheable 를 사용해 구현해볼까 했는데 도입을 고려하는 시점에 있어 몇가지 사용의 불편함이 있었다.\n\n#### 현재 사내에서 사용하고 있는 캐싱키처럼의 생성이 까다롭다.\n코드에서 동적으로 캐싱키를 만들어서 사용하고 있는데, 규격에 맞게 키가 생성되기 때문에 바로 도입시 중복되는 키가 두배로 많아져 부하가 생길 수 있다.\n\n#### 만료 시간을 캐시별로 주기가 어렵다.\n@Cacheable 은 추상화되어 있는 구현체이다. redis 를 client 로 바로 사용은 가능하지만 나머지 부분은 일일히 설정을 해주거나 구현을 해주어야 한다. [https://stackoverflow.com/questions/8181768/can-i-set-a-ttl-for-cacheable](https://stackoverflow.com/questions/8181768/can-i-set-a-ttl-for-cacheable) 에서 보면 client 의 설정으로 가능하다고는 하지만 애초에 ttl 에 대한 구현은 따로 없기 때문에 우회하거나 획일화된 시간뿐이 줄 수 없다.\n\n#### 캐싱 로직을 가져갈 수 없다.\n당연한 얘기겠지만 @Cacheable 을 사용하면 캐싱이 되는 로직은 사용자가 고려하지 않아도 된다. 문제는 여기 있는데 캐싱 로직에 원하는 코드를 추가하거나 수정할 수 없기 때문에 문제가 생기거나 로직이 변경되었을 때 대처가 불가능하다. 물론 애초에 @Cacheable 을 사용했다면 큰 문제가 없었겠지만 입맛에 맞게 캐싱 로직을 편하게 만들 필요가 있었다.\n\n기존에 사용하고 있는 캐싱 로직이 있다. method 를 invoke 해서 method 키값으로 결과물들을 redis 에 담아서 사용하고 있었는데, 이를 좀 더 사용하기 편하게 하기 위해 custom annotation 를 만들어서 사용하고자 했다.\n(결국 @Cacheable 를 뜯어보다가 필요한 부분만 뽑아서 따로 만들게 되었다.)\n\n### 구현 목적\n@RedisCached annotation 이 붙은 method 들에 대해 설정된 값에 따라 캐싱을 할 수 있고 필요에 따라 파라미터 값들도 함께 사용해야 한다면 @RedisCachedKeyParam 를 사용해서 method 의 parameter 와 value 를 함께 cache key 로 사용할 수 있다. 사용예제는 다음과 같다.\n\n```java\n@Service\npublic class PersonService {\n\n    @RedisCached(key = \"person\", expire = 300)\n    public Person getPerson(@RedisCachedKeyParam(key = \"name\") String name) {\n        // do make cache jobs\n        Person person = new Person(name, 10);\n        System.out.println(\"person = \" + person);\n        return person;\n    }\n}\n```\n\nmethod 내부에서는 db job 이나 비용이 큰 작업들을 처리해주고 cache 대한 처리는 annotation 으로 처리한다.\n\n\n### 구현해보자.\n\n####1. build.gradle\n\n```gradle\n\n...\ndependencies {\n    compile(\"org.springframework.boot:spring-boot-starter-web\")\n    compile('org.springframework:spring-aspects')\n    compileOnly('org.projectlombok:lombok')\n    compile('redis.clients:jedis:3.0.1')\n    compile('com.google.code.gson:gson:2.8.0')\n    compile('com.google.guava:guava:22.0')\n    testCompile('org.springframework.boot:spring-boot-starter-test')\n}\n\n```\nsprong-aop 를 사용해서 @RedisCached annotation 이 붙은 method 들을 가져와서 캐싱 로직을 태울 수 있다.\n\n#### @RedisCached\n\n```java\n@Target(value = ElementType.METHOD)\n@Retention(RetentionPolicy.RUNTIME)\npublic @interface RedisCached {\n\n    /**\n     * main cache key name\n     * @return\n     */\n    String key();\n\n    /**\n     * expire time\n     * @return\n     */\n    int expire() default 1800;\n\n    /**\n     * force proceed method\n     * @return\n     */\n    boolean replace() default false;\n}\n```\n- key : 캐싱이 되는 메인 키값을 줄 수 있다.\n- expire : 만료가 되는 시점을 줄 수 있다. 기본설정은 1800초이다.\n- replace : 캐싱 값을 덮어씌울지 여부이다. 필요에 따라 레디스에 저장되어 있는 값들을 override 해줄 때 사용한다.\n\n\n#### @RedisCachedKeyParam\n\n```java\n@Target(ElementType.PARAMETER)\n@Retention(RetentionPolicy.RUNTIME)\npublic @interface RedisCachedKeyParam {\n\n    String key();\n}\n```\nmethod parameter 에 붙여서 사용한다. @RedisCached.key 와 더불어 캐싱키값을 만들고자 할 때 사용한다.\n\n#### RedisCacheAspect.class\n\n```\n\n@Component\n@Aspect\npublic class RedisCacheAspect {\n    private static Map<String, RedisCacheParameterMethodInfo> cacheParameterMethodInfoMap = new HashMap<>();\n\n    @Autowired\n    private RedisDB redisDB;\n\n    // point1\n    @Around(value = \"execution(* *(..)) && @annotation(redisCached)\")\n    public Object aroundAspect(ProceedingJoinPoint joinPoint, RedisCached redisCached) {\n\n        MethodSignature methodSignature = (MethodSignature) joinPoint.getSignature();\n        Method method = methodSignature.getMethod();\n        String methodName = method.getName();\n\n        Class returnType = methodSignature.getReturnType();\n\n        // point2\n        String key = redisCached.key();\n        int expire = redisCached.expire();\n        boolean replace = redisCached.replace();\n\n        // point3\n        // reflect caching key from parameter\n        List<String> parameterKeyList = new ArrayList<>();\n        Object[] args = joinPoint.getArgs();\n        RedisCacheParameterMethodInfo methodInfo = cacheParameterMethodInfoMap.get(methodName);\n        if (methodInfo != null) {\n            List<RedisCacheParameterMethodInfo.IndexInfo> indexInfoList = methodInfo.getIndexInfoList();\n            indexInfoList.forEach(info ->\n                    parameterKeyList.add(makeCacheKey(info.getAnnotation(), args[info.getIndex()].toString())));\n\n        } else {\n            methodInfo = new RedisCacheParameterMethodInfo();\n            Annotation[][] parameterAnnotations = method.getParameterAnnotations();\n            for (int i = 0; i < parameterAnnotations.length; i++) {\n                for (Annotation annotation : parameterAnnotations[i]) {\n                    if (annotation instanceof RedisCachedKeyParam) {\n                        RedisCachedKeyParam keyParam = (RedisCachedKeyParam)annotation;\n                        parameterKeyList.add(makeCacheKey(keyParam, args[i].toString()));\n                        methodInfo.addInfo(keyParam, i);\n                    }\n                }\n            }\n            cacheParameterMethodInfoMap.put(methodName, methodInfo);\n        }\n\n        // point5\n        // make cache key\n        StringBuilder cacheKeyBuilder = new StringBuilder()\n                .append(key).append(\"/\").append(methodName).append(\"/\");\n\n        if (!CollectionUtils.isEmpty(parameterKeyList)) {\n            cacheKeyBuilder.append(Joiner.on(\",\").join(parameterKeyList));\n        }\n        final String cacheKey = cacheKeyBuilder.toString();\n\n        try {\n            Object result;\n            // point6\n            if (!replace) {\n                Long ttl = redisDB.ttl(cacheKey);\n                if (ttl > 0) {\n\n                    result = redisDB.get(cacheKey, returnType);\n                    return result;\n                }\n            }\n\n            // point7\n            result = joinPoint.proceed();\n            redisDB.set(cacheKey, result, returnType);\n            redisDB.expire(cacheKey, expire);\n\n            return result;\n        } catch (Throwable t) {\n            t.printStackTrace();\n        }\n\n        return null;\n    }\n\n    // point 4\n    private String makeCacheKey(RedisCachedKeyParam keyParam, String value ) {\n        return String.format(\"%s=%s\", keyParam.key(), value);\n    }\n}\n```\n\n핵심이 되는 캐싱 구현로직이다. 포인트 별로 살펴보자.\n- point1 : spring-aop 를 사용해서 @Around 로 묶어서 사용한다. @RedisCached 가 붙어 있는 method 들에 대해 캐싱 로직을 태운다.\n- point2 : @RedisCached 에 있는 값들을 가져와서 캐시키나 만료에 대한 정보들을 가져온다.\n- point3 : cacheKey 를 만들어 줄때 method 에 들어있는 parameter 들을 가져와서 만들어주는데 이를 매번 joinPoint 에서 가져와서 reflect 할 필요는 없다. 별도의 hashMap 을 갖고 이미 reflect 된 method 들에 대해서는 갖고 있는 값을 사용하도록 한다.\n- point4 : method 에서 parameter 들을 가져와서 @RedisCachedKeyParam 와 연결을 시켜 캐싱키를 만들어 준다.\n- point5 : 캐싱키는 `person/getPerson/name=teddy` 와 같이 사용하고 있다. @RedisCached.key 와 methodName, @RedisCachedKeyParam 에 설정되어 있는 key, value 를 붙여서 사용하고 있는데 이를 만들어 주는 과정이다.\n- point6 : @RedisCached.replace 로 이미 캐시가 되어 있는 값을 덮어 쓸지 여부에 대한 로직이다. 주기적으로 갱신이 필요한 캐싱들에 대해 값을 덮어 써줄 수 있다. point6 의 분기를 타지 않으면 proceed() 된 값을 cacheKey 에 덮어씌운다.\n- point7 : 실제 redis 에 캐싱이 되는 부분이다. 이미 캐싱이 되어 있다면 point6 의 분기에서 redis 의 저장되어 있는 값을 반환한다.\n\n### 사용해보기\n\n```bash\n$❯ curl localhost:8080/person/teddy\n{\"name\":\"teddy\",\"age\":10}\n\n-- log\n\n// call mapping\nperson = Person(name=teddy, age=10)\n// make cache\ngetPerson = Person(name=teddy, age=10)\n// use cache\ngetPerson = Person(name=teddy, age=10)\n\n-- redis\n127.0.0.1:6379> get person/getPerson/name=teddy\n\"{\\\"name\\\":\\\"teddy\\\",\\\"age\\\":10}\"\n127.0.0.1:6379> ttl person/getPerson/name=teddy\n(integer) 187\n```\n\napi 콜을 하면 redis 에 저장되어 있는지 여부를 검사하고 해당 키값으로 값이 없으면 캐싱을 하고 돌려준다. 그 다음부터 호출을 하면 redis 에 저장되어 있는 값을 돌려준다.\n\n### 한계가 있다!\nspring-aop 를 사용하고 있는데 proxy 기반으로 동작하기 때문에 inner method 호출은 먹지 않는다. [https://stackoverflow.com/questions/13564627/spring-aop-not-working-for-method-call-inside-another-method](https://stackoverflow.com/questions/13564627/spring-aop-not-working-for-method-call-inside-another-method) 에서 보면 우회적으로 해결할 수 있는 방법들이 있는데 좋아보이지는 않는다. 아예 aspectJ 를 이용해서 구현해볼까도 생각중이긴 한데 아직 완전한 해결방법은 찾이 못했다. 이는 interface 로 캐싱 로직을 따로 빼거나 우회적인 방법으로 임시적인 해결을 할 수는 있지만 여전히 inner method call 이 안되는건 마음에 들지 않는다.\n\n### 결론\n@RedisCached 를 이용해서 기존의 캐시 전략을 따라가면서 조금 더 사용이 편한 방법으로 구현이 되었다. 다만 명확한 한계점이 있어 반쪽짜리 캐시긴 하지만.. [LTW 사용하기](https://medium.com/chequer/spring-transactional-caching-%EA%B7%B8%EB%A6%AC%EA%B3%A0-aspectj-2%ED%8E%B8-aspectj-689319db329f) 와 같은 좋은 블로그등을 참고해서 해결방안을 모색중이다. 위의 예제코드는 [Example code](https://github.com/nevercaution/cachedTest) 에서 확인할 수 있다.\n\n\n\n","fields":{"slug":"/redis-cache-annotation-with-aspect/"},"frontmatter":{"title":"redis cache 를 @annotation 으로 하기 (with @Aspect)","published":true}}},{"node":{"rawMarkdownBody":"\n### Retrofit 을 사용기 전에\nREST 호출을 해야할 때 보통 spring 에서 기본적으로 제공해주는 [RestTemplate](https://docs.spring.io/spring/docs/current/javadoc-api/org/springframework/web/client/RestTemplate.html) 을 사용했었다. 간단하게 사용할 수 있고 `spring-boot-starter-web` 내부에 포함되어 있어 다른 의존성이 없어도 된다. 제공되는 method 도 많은데, 워낙 사용이 자유로워서 프로젝트에서 여러 사람의 손을 타다보면 사용하는 방식이 제각각이다. 여기저기서 가이드 없이 사용하다 보니 어떤 api 를 호출하는지 정리가 잘 안되서 리팩토링을 하거나 api 정의가 바뀔 경우 불필요하게 손이 가게 된다. 그리고 호출객체와 응답 객체를 일일이 변환해서 써야 하기 때문에 요청 작업 외적으로 구현해 줘야 하는 부분이 있다.\n\n## Retrofit\n요청은 sync 와 async 로 할 수 있다. 간단한 api 를 만들어서 양쪽 모두 호출을 하게 만들어 보자.\n\n### 프로젝트 설정\n `build.gradle`\n\n ```gradle\ncompile('com.squareup.retrofit2:retrofit:2.3.0')\ncompile('com.squareup.retrofit2:converter-gson:2.3.0')\ncompile('com.squareup.okhttp3:logging-interceptor:3.9.0')\n ```\n gradle 에 retrofit 을 추가해준다.\n\n### model\n\n```java\n@Data\n@AllArgsConstructor\npublic class Person {\n    private String name;\n    private Integer age;\n}\n```\n\n### interface\n\n```java\npublic interface PersonAPI {\n\n    @GET(\"/persons\")\n    Call<List<Person>> getPersonList();\n\n    @POST(\"/person\")\n    Call<Person> getPerson(@Query(value = \"name\") String name);\n}\n```\n\n사용해보면서 가장 마음에 들었던 부분인데 api 요청들을 한곳에 모아서 볼 수 있게 되어 있다.\n파라미터들은 path, parameter, form 에 따라 제공되는 annotation 를 붙여주면 된다. 기본적인 사용법은 다음과 같다.\n- `/person/{name}` 일 경우엔 `@Path` 를 사용한다.\n- `/person?name={name}` 일 경우엔 `@Query` 를 사용한다.\n- form 에 담아 요청을 할 경우엔 `@Field` 로 호출한다.\n\n자세한 내용은 [공식 가이드](https://square.github.io/retrofit/) 에서 설명해준다.\n\n### retrofit service\nretrofit 는 사용할 api 를 등록하고 사용한다. 사용하기 편하게 util 로 만들어줄 수 있다.\n\n```java\npublic abstract class RequestUtil {\n    // 1. 호출할  도메인\n    private static final String BASE_URL = \"http://localhost:8080/\";\n    // 2. log interceptor\n    private static final HttpLoggingInterceptor loggingInterceptor = new HttpLoggingInterceptor().setLevel(HttpLoggingInterceptor.Level.BODY);\n    // 3. 사용할 http client\n    private static final OkHttpClient.Builder httpClient = new OkHttpClient.Builder()\n            .addInterceptor(loggingInterceptor);\n    private static final Retrofit retrofit = new Retrofit.Builder()\n            .baseUrl(BASE_URL)\n            .addConverterFactory(GsonConverterFactory.create())\n            .client(httpClient.build())\n            .build();\n\n    // 4. 서비스 등록\n    public static <T> T createService(Class<T> sClass) {\n        return retrofit.create(sClass);\n    }\n\n    // 5. 서비스 호출\n    public static <T> Optional<T> requestSync(Call<T> call) {\n        try {\n            Response<T> execute = call.execute();\n            System.out.println(\"execute = \" + execute);\n            if (execute.isSuccessful()) {\n                return Optional.ofNullable(execute.body());\n            }\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n        return Optional.empty();\n    }\n\n    public static <T> void requestAsync(Call<T> call, CustomCallback<T> callback) {\n        call.enqueue(callback);\n    }\n}\n```\n1. 호출할 도메인이다. 객체가 생성될 때 넣어준다. 만약 호출해야할 서비스가 여러개일 경우엔 추가로 받아서 등록해줄 수 있다.\n2. 원하는 로그레벨을 걸어 필요한 값들을 모두 확인해볼 수 있다. default 값은 INFO 이다.\n3. http client 로 okhttp 를 사용했다. 다른 client 를 사용할 수도 있지만 같은 회사에서 만들어서 서로의 궁합이 좋다.\n4. interface 로 정의한 서비스를 등록하는 부분이다.\n5. 등록된 서비스에서 Call 객체가 나오는데, 이를 호출해준다. sync/async 모두 호출이 가능하다.\n\n\n비동기 호출시에 기본적으로 `Calback<T>` 를 받아서 처리하는데 사용의 편의를 위해 `CustomCallback` 을 만들어서 사용할 수 있다. 추가적으로 로그를 남길 수 있고 코드가 좀 더 깔끔해진다.\n\n```java\n\npublic abstract class CustomCallback<T> implements Callback<T> {\n\n    @Override\n    public void onResponse(Call<T> call, Response<T> response) {\n        System.out.println(\"response = \" + response);\n    }\n\n    @Override\n    public void onFailure(Call<T> call, Throwable t) {\n        t.printStackTrace();\n    }\n}\n```\n\n## service\n\n```java\nPersonAPI personAPI = RequestUtil.createService(PersonAPI.class);\n```\n\n위에서 정의해준 interface 를 retrofit 에 등록해주고 호출을 할 수 있다. personAPI 객체에서 정의된 api 를 호출하면 Call 객체가 반환되는데 이를 요청하면 된다.\n\n#### sync 호출시\n\n```java\nCall<List<Person>> personList = personAPI.getPersonList();\nRequestUtil.requestSync(personList);\n```\n\n호출하고 반환받을 객체를 Call 로 감싸서 반환된다.\n\n#### async 호출시\n\n```java\nRequestUtil.requestAsync(personAPI.getPersonList(), new CustomCallback<List<Person>>() {\n    @Override\n    public void onResponse(Call<List<Person>> call, Response<List<Person>> response) {\n        super.onResponse(call, response);\n    }\n\n    @Override\n    public void onFailure(Call<List<Person>> call, Throwable t) {\n        super.onFailure(call, t);\n    }\n});\n```\n비동기 호출시엔 enqueue 로 호출하고 응답받을 callback 을 등록해주면 된다. callback 내부에서는 성공과 실패시 메소드를 제공한다. 각자 응답에 따라 필요한 처리를 해주면 된다. 비동기 호출에 대해 응답을 받아주기 위해 `Mono` 로 감싸서 사용할 수 있다.\n\n```java\nMono<List<Person>> mono = Mono.create(sink -> {\n    RequestUtil.requestAsync(personAPI.getPersonList(), new CustomCallback<List<Person>>() {\n        @Override\n        public void onResponse(Call<List<Person>> call, Response<List<Person>> response) {\n            if (!response.isSuccessful()) {\n                sink.error(new Exception(\"response is empty\"));\n                return;\n            }\n\n            sink.success(Objects.requireNonNull(response.body()));\n        }\n    });\n});\nreturn mono;\n```\n\n## 결론\n사용하고 있는 api 들을 한곳에 정리할 수 있고 필요한 파라미터 값들이 코드로 작성되기 때문에 api 문서를 확인하지 않아도 편하게 확인할 수 있다. 스타일이 어느정도 강제가 되어 있긴 하지만 보다 명확하게 명세를 정의할 수 있어 마음에 들었다. 조만간 retrofit 으로 모든 요청들을 정리해 볼 생각이다. [예제코드](https://github.com/nevercaution/retrofitDemo) 에서 위의 코드들을 확인해볼 수 있다.","fields":{"slug":"/retrofit-with-spring-boot/"},"frontmatter":{"title":"spring boot 에서 Retrofit 사용해보기","published":true}}},{"node":{"rawMarkdownBody":"api gatway 를 도입했다. 레거시 프로젝트를 정리하면서 msa 구조로 가게 되었고 필요에 따라 서비스들이 나뉘고 있어서 이를 한곳에서 관리해줄 필요가 있었다. 구조를 설계하면서 어느 레벨까지를 gateway 에서 처리할지에 대해 여러 고민이 있었고 너무나 크지 않은 선에서 일차적으로 도입을 하게 되었다.\ngateway 를 구성하기 위해 아래의 3개의 프로젝트를 설정한다.\n1. 라우팅을 해줄 gateway (zuul gateway)\n2. gateway 와 zuul 설정을 연결해줄 중간자 (spring cloud config)\n3. zuul 설정을 저장할 저장소 (git)\n\n### 1. gateway 구성\nspringboot, gradle 로 구성을 했고 버전은 2를 사용한다. spring cloud zuul 도 있지만 버전2에서는 아직 추가되지 않아 netflix zuul 을 사용하기로 한다.\n#### 빌드 설정\n\n`build.gradle`\n\n```gradle\nbuildscript {\n    ext {\n        springBootVersion = '2.0.6.RELEASE'\n    }\n    repositories {\n        mavenCentral()\n    }\n    dependencies {\n        classpath(\"org.springframework.boot:spring-boot-gradle-plugin:${springBootVersion}\")\n    }\n}\n\napply plugin: 'java'\napply plugin: 'eclipse'\napply plugin: 'groovy'\napply plugin: 'org.springframework.boot'\napply plugin: 'io.spring.dependency-management'\n\ngroup = 'com.nevercaution'\nversion = '0.0.1-SNAPSHOT'\nsourceCompatibility = 1.8\n\ndependencies {\n    compile('org.springframework.cloud:spring-cloud-starter-netflix-zuul')\n    compile('org.springframework.cloud:spring-cloud-starter-config')\n    compile('org.codehaus.groovy:groovy-all')\n    compile('com.googlecode.json-simple:json-simple')\n\n    testCompile('org.springframework.boot:spring-boot-starter-test')\n\n}\n\ndependencyManagement {\n    imports {\n        mavenBom \"org.springframework.cloud:spring-cloud-dependencies:Finchley.SR1\"\n    }\n}\n```\n\n#### 프로젝트 설정\n\n```\nspring-cloud 를 사용할 때엔 application.yml 보다 bootstrap.yml 을 먼저 읽어 들인다.\n먼저 읽은 값을 기반으로 application.yml 에 설정된 값들을 함께 사용하기 위함이다.\n```\n\napplication.name 과 spring.profiles.active 두 값으로 cloud config 에 정보를 요청한다. 이 값을 적지 않을 경우엔 값을 가져오지 못한다. 또한 profiles.active 를 명시해주지 않으면 기본적으로 default profile 로 로드를 시도한다.\n\n`bootstrap.yml`\n\n```yml\nspring:\n  profiles:\n    active: local\n  # 이 이름으로 spring cloud config server 에서 정보를 가져온다.\n  application:\n    name: gateway\n\n---\n########################################\n###              local               ###\n########################################\nspring:\n  profiles: local\n  cloud:\n    config:\n      uri: http://localhost:8889\n---\n```\n\nspring-cloud 에서 zuul route 설정들을 받아온다. 만약 spring-cloud 가 죽었을 경우를 대비해야 한다면 route 설정값들을 application.yml 에 해주면 된다.\n참고로 spring-cloud-config 기본 주소는 localhost:8888 이다. 즉 8888포트로 사용할 거라면 따로 설정하지 않아도 기본으로 이 주소로 접근을 시도한다. 원하는 경로로 사용할 경우에는 반드시 명시해주어야 한다.\n\n필요에 따라 zuul 설정을 여기에서 해줘도 된다. 만약 cloud config 에서 값을 읽어들이지 못할 경우엔 여기에 있는 값을 사용하게 된다.\n\n`application.yml`\n\n```yml\nspring:\n  # groovy template 는 사용하지 않는다.\n  groovy:\n    template:\n      cache: false\n\n\n# 필요한 actuator end point 만 열어둔다.\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        # 원하는 endpoint 를 추가할 수 있다.\n        include: info, routes, filters, refresh\n\n\n---\n########################################\n###              local               ###\n########################################\nspring:\n  profiles: local\n\nserver:\n  port: 8087\n\n# 여기서 설정도 가능하다. 우선순위는 cloud config 가 더 높다.\n#zuul:\n#  routes:\n#    apiService:\n#      stripPrefix: false\n#      path: /api/**\n#      url: https://new-api-service.com\n---\n```\n\napplication 에 @EnableZuulProxy 만 달아주면 gateway는 설정이 모두 끝난다.\n\n`GatewayApplication.java`\n\n```java\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.netflix.zuul.EnableZuulProxy;\n\n@EnableZuulProxy  // 이 annotation 만 추가하면 된다.\n@SpringBootApplication\npublic class GatewayApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(GatewayApplication.class, args);\n    }\n}\n```\n\n### 2. spring-cloud-config-server 설정\nconfig-server 는 cloud-config 에 저장되어있는 설정들을 gateway 들이 가져갈 수 있도록 중간에서 설정값을 가져오는 역할을 한다.\n\n#### 빌드설정\n\n`build.gradle`\n\n```gradle\nbuildscript {\n    ext {\n        springBootVersion = '2.0.6.RELEASE'\n    }\n    repositories {\n        mavenCentral()\n    }\n    dependencies {\n        classpath(\"org.springframework.boot:spring-boot-gradle-plugin:${springBootVersion}\")\n    }\n}\n\napply plugin: 'java'\napply plugin: 'eclipse'\napply plugin: 'org.springframework.boot'\napply plugin: 'io.spring.dependency-management'\n\ngroup = 'com.nevercaution'\nversion = '0.0.1-SNAPSHOT'\nsourceCompatibility = 1.8\n\next {\n    springCloudVersion = 'Finchley.SR1'\n}\n\ndependencies {\n    compile('org.springframework.cloud:spring-cloud-config-server')\n    testCompile('org.springframework.boot:spring-boot-starter-test')\n}\n\ndependencyManagement {\n    imports {\n        mavenBom \"org.springframework.cloud:spring-cloud-dependencies:${springCloudVersion}\"\n    }\n}\n```\n\n#### 프로젝트 설정\n\n`application.yml`\n\n```yml\nserver:\n  port: 8889\n\nspring:\n  cloud:\n    config:\n      server:\n        git:\n          uri: https://github.com/user/cloud-config/config.git\n          username: username\n          password: password\n```\nconfig-server 역시 application 에 @EnableConfigServer 달아주면 끝.\ngateway 에서 필요한 값들은 config.git 에서 받아온다.\n\n`Application.java`\n\n```java\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.config.server.EnableConfigServer;\n\n@EnableConfigServer  // 이 annotation 만 적어주면 끝.\n@SpringBootApplication\npublic class MimirApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(MimirApplication.class, args);\n    }\n}\n```\n\n### 3. config 설정\ncloud-config 는 gateway 에서 사용할 zuul 에 관련된 설정들을 모아놓는 곳이다.\n\n`gateway.yml`\n\n```yml\n---\n########################################\n###              local               ###\n########################################\nspring:\n  profiles: local\n\n# zuul route 설정들.\nzuul:\n  routes:\n    apiService:\n      stripPrefix: false\n      path: /api/**\n      url: https://new-api-service.com\n\n# groovy filter 가 있는 경로를 적어준다.\ngateway:\n  zuul:\n    filters:\n      base-path: /path/to/filter/\n---\n```\n\n### 정리하자면\ngateway 에서는 zuul route 설정들을 이용해서 요청들을 받아서 처리해준다.\ngateway 에서 route 설정들을 받아오기 위해서 spring-cloud-server 에 spring-cloud-config 정보를 요청한다.\nspring-cloud-server 에서는 spring-cloud-config 에 있는 정보를 조회해서 gateway 에 내려준다.\n\n### zuul filter 사용\n특정 상황에 대처하기 위해 필터를 걸 수 있다. java 로 추가할 수도 있지만 내용이 변경시 서비스가 재기동되어야 하기 때문에 groovy 로 필터를 사용한다.\ngateway 에서는 지정된 경로에 groovy filter 들을 로드 시킨다.\ngateway 에서 gateway.zuul.filters.base-path 이 값은 spring-cloud-config 에서 받아와서 로드한다.\nFileManager.init 에서 첫번째 파라미터는 이 경로에 몇초마다 파일들을 갱신할지 여부이다. 짧게 가져갈수록 부하가 있지만 대신 코드가 빠르게 적용된다.\n\n`ZuulFilterCommandLineRunner.java`\n\n```java\nimport com.netflix.zuul.FilterFileManager;\nimport com.netflix.zuul.FilterLoader;\nimport com.netflix.zuul.groovy.GroovyCompiler;\nimport com.netflix.zuul.groovy.GroovyFileFilter;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.boot.CommandLineRunner;\nimport org.springframework.stereotype.Component;\n\n@Component\npublic class ZuulFilterCommandLineRunner implements CommandLineRunner {\n    private static Logger log = LoggerFactory.getLogger(ZuulFilterCommandLineRunner.class);\n\n    // cloud config 에 정의된 경로에서 로드한다.\n    @Value(\"${gateway.zuul.filters.base-path}\")\n    private String filterBasePath;\n\n    @Override\n    public void run(String... args) {\n        FilterLoader.getInstance().setCompiler(new GroovyCompiler());\n        try {\n            log.debug(\"load try file : \" + filterBasePath);\n            FilterFileManager.setFilenameFilter(new GroovyFileFilter());\n            FilterFileManager.init(1, this.filterBasePath + \"pre\", this.filterBasePath + \"route\", this.filterBasePath + \"post\");\n        } catch (Exception e) {\n            log.error(\"load fail \" + filterBasePath, e);\n            throw new RuntimeException(e);\n        }\n    }\n}\n```\n\n뒤에 파일 경로는 성격에 따라 pre, route, post 폴더들로 구분이 된다. 자세한 정보는 [zuul filter](https://cloud.spring.io/spring-cloud-netflix/multi/multi__router_and_filter_zuul.html#_custom_zuul_filter_examples) 문서 에서 확인할 수 있다.\n\ngroovy filter 는 ZuulFilter 를 상속받아서 구현한다.\n\n`Route.groovy`\n\n```groovy\nimport com.netflix.zuul.ZuulFilter\nimport com.netflix.zuul.context.RequestContext\nimport com.netflix.zuul.exception.ZuulException\nimport org.slf4j.Logger\nimport org.slf4j.LoggerFactory\n\nimport static org.springframework.cloud.netflix.zuul.filters.support.FilterConstants.PROXY_KEY\nimport static org.springframework.cloud.netflix.zuul.filters.support.FilterConstants.SIMPLE_HOST_ROUTING_FILTER_ORDER\n\npublic class SimpleRoute extends ZuulFilter {\n\n    private static final Logger logger = LoggerFactory.getLogger(SimpleRoute.class);\n\n    @Override\n    String filterType() {\n        return \"route\"\n    }\n\n    @Override\n    int filterOrder() {\n        return SIMPLE_HOST_ROUTING_FILTER_ORDER - 1\n    }\n\n    @Override\n    boolean shouldFilter() {\n        return true\n    }\n\n    @Override\n    Object run() throws ZuulException {\n        def ctx = RequestContext.getCurrentContext()\n        def req = ctx.getRequest()\n\n        def host = ctx.getRouteHost()\n\n        try {\n\n            RequestContext.currentContext.setRouteHost(new URL(\"https://another-new-api-service.com\"))\n\n            logger.info(\"REQUEST:: \" + req.getScheme() + \" \" + req.getRemoteAddr() + \":\" + req.getRemotePort())\n\n            logger.info(\"REQUEST:: > \" + req.getMethod() + \" \" + req.getRequestURI() + \" \" + req.getProtocol())\n        } catch(Exception e) {\n            logger.error(\"errer handling\")\n            ctx.setRouteHost(host)\n        }\n\n\n        return null\n    }\n}\n```\n\n### zuul route 동적으로 변경하기\nzuul route 정보들은 동적으로 추가하거나 변경할 수 있다. spring-cloud-config 에서 설정값을 추가하거나 변경을 하고 gateway 에서 actuator 를 이용해서 갱신을 시켜주면 route 설정들이 동적으로 갱신된다.\n\nroute 설정을 편집하거나 추가하는 상황이다.\n\n#### 1. route 설정 편집과 추가 하기\n\n`gateway.yml`\n\n```yml\n########################################\n###              local               ###\n########################################\nspring:\n  profiles: local\n\nzuul:\n  routes:\n    apiService:\n      stripPrefix: false\n      path: /api/**\n      # 1. 기존 경로를 변경\n      url: https://some-api-service.com\n    # 2. 새로추가되는 서비스\n    searchService:\n      stripPrefix: false\n      path: /search/**\n      url: https://search-api-service.com\n\n# groovy filter 가 있는 경로를 적어줍니다.\ngateway:\n  zuul:\n    filters:\n      base-path: /path/to/filter/\n```\n\n1 번 주석 부분은 기존에 api 라는 경로로 들어왔을 때 new-api-service.com 에서 some-api-service.com 으로 변경을 해주었다.\n2 번 주석 부분은 새로 추가되는 경로이다.\n이렇게 추가와 수정을 해주고 commit/push 를 해준다. push 를 한다고 해서 바로 변경점이 반영되지 않는다.\n\n#### 2. gateway 에서 refresh\nzuul 은 내부적으로 spring-boot-actuator 가 의존성으로 걸려있다. actuator 를 이용해서 spring-cloud-server 를 통해 spring-cloud-config 값들을 동적으로 가져올 수 있다.\n\n```bash\n$ curl -XPOST localhost:8080/actuator/refresh\n[\"config.client.version\",\"zuul.routes.apiService.url\",\"zuul.routes.searchService.path\",\"zuul.routes.searchService.stripPrefix\",\"zuul.routes.searchService.url\"]\n```\ngateway 에서 /refresh 를 호출하게 되면 처음에 받아왔던 정보에서 변경점들만 가져와서 다시 로딩한다.\n\n## 결론\n### route 동적 편집 가능\nzuul 을 이용해서 경로에 따라 원하는 도메인으로 routing 을 해줄 수 있다. 이 설정값들은 spring-cloud-config 에 저장되어 있는데 이 값들을 동적으로 편집할 수 있다. 이 동작은 spring-actuator 을 이용한다.\n\npost 요청으로 `/actuator/refresh` 를 gateway 에 호출하면 반영이 된다.\n\n### filter 동적 편집 가능\nfilter 역시 동적으로 편집하거나 추가할 수 있는데, 이는 zuul file manager 를 통해 특정 경로에 있는 groovy filter file 들을 로드해서 읽어서 사용한다. file manager 가 주기적으로 파일의 동기화를 하고 있으므로 파일을 수정하면 지정된 시간마다 동기화를 한다.\n\ngateway 를 도입하고나서 추가적으로 할 수 있는 것들이 생겼다. 화이트리스트를 만들어 특정 서버군으로 보낼 수도 있고 사용자별로 A/B 테스트를 해볼 수도 있을 것이다. 블랙리스트들도 걸러줄 수 있게 된다. (nginx 에서도 처리가 가능하지만 좀 더 동적으로 가능하고 형상관리도 가능하겠다.) 추가적으로 로그를 모으거나 springboot admin 등을 통해서 gateway 상태도 살펴볼 수있을듯 하다. 예제코드는 [여기](https://github.com/nevercaution/spring-cloud-zuul-example) 에서 확인할 수 있다.\n","fields":{"slug":"/api-gateway-with-zuul/"},"frontmatter":{"title":"API Gateway 구축하기 - Spring Cloud Zuul","published":true}}},{"node":{"rawMarkdownBody":"\n배포없이 설정값을 변경해야하는 일이 생겼다. 설정값 하나 때문에 전체 api 서버를 재배포 해야하는건 부담이 있었고 실시간으로 설정값을 변경해서 바로바로 코드에 반영해야 하는 일이었다. 이를 구현하기 위해서는 여러가지 방법이 있는데 그중에서 가장 손이 덜가면서 부하가 덜 드는 방법으로 구현을 하고자 했다. 이런 저런 방법을 찾아보면서 찾게 된 방법들이다.\n\n1. [redis](https://redis.io/commands/get) 를 이용해 값을 가져오는 방식\n2. [spring cloud](http://projects.spring.io/spring-cloud/) 를 이용해 설정값을 전파 방는 방식\n3. [Apache commons configuration](https://commons.apache.org/proper/commons-configuration/) 을 이용해 외부파일에 설정값을 전파하는 방식\n\n1번의 경우는 구현이 가장 간단한 방식이다. key 값으로 원하는 설정값을 넣어 바로 꺼내어 사용하면 되기 때문이다. 하지만 redis 에 계속해서 조회를 하기 때문에 부하가 올라감 염려가 있다. 자주 불리는 endpoint 가 아니라면 괜찮지만 login 같이 조회수가 높은 endpoint 라면 단순한 로직 추가임에도 로드가 올라가 redis 에 부담을 줄 수가 있어 이 방법은 최후의 보루(?) 로 남겨두기로 했다.\n2번의 경우는 spring cloud 를 사용해서 yml 파일들을 spring cloud config server 에 올려두고 spring cloud client 들이 해당 설정 파일을 받아가는 형식이라 지금 구현하고자 하는 바를 만족한다. 하지만 spring cloud 는 이런 기능 외에 훨씬 더 많은 기능들을 제공하고 있는데, 동적으로 설정값 하나 넣자고 배보다 배꼽이 더 큰 구현을 하기엔 부적합하다고 판단 되었다.\n3번의 경우는 spring boot 외부에 설정파일을 따로 두어 파일을 읽어 값을 사용하는 방식이다. 역시 지금 구현하고자 하는 목표와 비슷하기도 하고 구현의 난이도나 범위가 크지 않았지만 파일을 주기적으로 읽어야 하는 부담 (file IO 는 부하가 크고 속도도 느린 편이다.) 이 있었고, 전체 api 서버에 변경된 설정값이 담긴 파일을 전파하려면 [ansible](https://www.ansible.com/) 이나 별도의 구현을 통해 전체 서버에 전달을 해주어야 하는데, 이 역시 구현체보다 구현을 해야하는 범위가 넘어서버리게 되어버려 마음에 들지 않았다.\n마감 시간이 급박한건 아니었지만 그렇다고 해서 큰일을 벌일 정도의 규모의 일감이 아니었기 때문에 적당한 선에서 작업하고 넘어가는 것이 좋다고 생각했다. 회사 동료들과 어떻게 할까 고민하다가 문득 redis pub/sub 을 이용해 메모리에 설정값들을 가지고 있으면 어떨까 생각을 했다. 필요할 때만 redis litenser 가 사용되고 file IO 보다 가벼우며 과도하게 redis connection 을 맺지 않아 부하도 크지 않아 괜찮을거라 판단했다.\n~~다들 아시겠지만 Reactive 의 publisher subscriber 가 아니다!~~\n\n## 시나리오 순서\n각 api 서버당 약속된 channel 로 redis 를 통해 subscribe 한다.\n서버가 올라간 직후에는 로컬 메모리에 데이터가 없으니 redis 를 통해 publish 해준다.\n로컬 메모리에 있는 설정값을 사용하고, 필요에 의해 변경될 사항이 있다면 다시 publish 해준다.\n(사족) redis publish 당시에 value 값을 SET 해주고 로컬 메모리에서 접근이 실패했다면 redis 에서 key 값으로 GET 을 한다.\n\n방어코드는 짜기 나름이다. `사족` 부분에서의 동작은 넣어도 되고 안넣어도 그만이다. 서비스단에서 로컬 메모리에 key 에 대응하는 값이 없을 때에 대한 처리를 잘 해준다면 굳이 추가하지 않아도 되는 기능이다.\n\n### 환경 설정\n\n```\ngradle 4.8.1\nspring boot 2.0.3.RELEASE\nredis 3.2.3\nlettuce 5.0.4.RELEASE\n```\n\nredis java client 는 lettuce 를 사용하였다. 사내에서는 기본적으로 jedis 를 사용하는데, redis 의 버전 대응도 느린편이고 async 에 대한 지원도 아직 없는 상태이다. 이번 포스트에서는 redis 의 async 부분을 다루지는 않지만, (다른 포스트에서는 살짝 다룬 부분이 있다) 앞으로 redis client 를 사용할 때는 jedis 는 점점 더 손이 덜 갈듯 싶다.\n\n### build.gradle\n\n```gradle\ndependencies {\n    compile('org.springframework.boot:spring-boot-starter')\n    compile('org.springframework.boot:spring-boot-starter-web')\n    compile('io.lettuce:lettuce-core')\n    compile('com.google.code.gson:gson:2.8.0')\n    compile('org.apache.logging.log4j:log4j-core:2.9.1')\n    compileOnly('org.projectlombok:lombok')\n\n    testCompile('org.springframework.boot:spring-boot-starter-test')\n}\n```\n\nredis 와 이를 전달해 줄 수 있는 json 으로 gson 을 사용하기로 했다. 여기서도 역시 jackson 과의 비교가 들어가는데 이번에 구현해야할 기능에서는 간단한 기능들만 필요하므로 gson 을 사용하는 것으로 한다.\n[jackson vs gson](http://www.baeldung.com/jackson-vs-gson) 에서 기능이나 사용에 대한 차이를 볼 수 있다.\n\n\n## pub/sub 을 이용해 데이터 갱신하기\n\n### thread, subscriber 초기화및 등록\n\n로컬에 스레드와 subscriber 를 최초에 초기화 해주도록 한다.\n\n```java\nprivate CustomSubscriber customSubscriber;\nprivate Thread localThread;\nprivate RedisService redisService;\n\nprivate CustomSubscriber getCustomSubscriber() {\n    if (customSubscriber == null) {\n        customSubscriber = new CustomSubscriber() {\n            @Override\n            public void message(String channel, String message) {\n                update(message);\n            }\n        };\n    }\n    return customSubscriber;\n}\n\n@PostConstruct\npublic void init() {\n    if (localThread == null) {\n        localThread = new Thread(() -> redisService.subscribe(CHANNEL, getCustomSubscriber()));\n    }\n    localThread.start();\n}\n\n@PreDestroy\npublic void destroy() {\n    redisService.unSubscribe(getCustomSubscriber());\n}\n\n```\n\n- redis subscribe 을 하게 되면 blocking 이 걸리게 되므로 별도의 thread 를 생성해 subscribe 하도록 한다.\n- localThread 는 최초에 하나의 subscriber 를 생성해 구독을 하고, destroy 될 때 구독을 해지한다.\n- 등록된 subscriber 에서는 받은 메세지를 `update` 메소드로 전달해준다.\n\n### message 전달 및 데이터 갱신\n\n```java\nprivate void update(String message) {\n    try {\n        String[] split = message.split(\"\\\\|\");\n        String key = split[0];\n        String value = split[1];\n        this.cacheMap.put(key, value);\n        log.debug(\"update success : \" + message);\n    } catch (Exception e) {\n        log.warn(\"update fail : \" + message, e);\n    }\n}\n\npublic Long publish(String key, String value) {\n    String message = key + \"|\" + value;\n    Long publish = redisService.publish(CHANNEL, message);\n    return publish;\n}\n```\n\n- publish 와 subscribe 에서는 약속된 메세지 포멧을 가지도록 한다. 여기서는 간단한 key, value 형태로만 가져할 수 있도록 message 를 `|` 를 구분자로 가져가기로 했다. 즉, `message = key|value` 로 약속을 하고 파싱을 해서 사용하도록 한다.\n- subscribe 에서 받아온 message 를 `|` 구분자로 쪼개 key 와 value 를 가져와 로컬 메모리에 올리도록 한다.\n\n### 데이터 제공\n\n```java\nprivate ConcurrentHashMap<String, String> cacheMap = new ConcurrentHashMap<>();\nprivate static Gson gson = new GsonBuilder().setFieldNamingPolicy(FieldNamingPolicy.LOWER_CASE_WITH_UNDERSCORES)\n        .setDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n        .disableHtmlEscaping()\n        .excludeFieldsWithModifiers(Modifier.TRANSIENT)\n        .create();\n\npublic <T> T get(String key, Class<T> tClass) {\n    try {\n        String valueString = this.cacheMap.get(key);\n        return gson.fromJson(valueString, tClass);\n    } catch (Exception e) {\n        log.warn(\"get error > key \" + key, e);\n        return null;\n    }\n}\n```\n\n- get 함수를 통해 로컬 메모리에 있는 데이터를 제공해주는데, 데이터가 갱신되는 도중에 여러 스레드가 동시에 접근하면서 정합성을 맞춰주기 위해 `ConcurrentHashMap` 를 사용했다.\n\n참고로 `get` 메소드는 Optional 로 감싸주면 더욱 좋다. null 을 반환하는 것보다 Optional.empty() 를 반환해주는 모양이 외부에서도 사용할 때 좀 더 편하게 사용할 수 있을 것이다. 위의 `get` 메소드를 조금 고쳐보자.\n\n### 조금 더 편하게 데이터 제공\n\n```\npublic <T> Optional<T> get(String key, Class<T> tClass) {\n    try {\n        String valueString = this.cacheMap.get(key);\n        return Optional.of(gson.fromJson(valueString, tClass));\n    } catch (Exception e) {\n        log.warn(\"get error > key \" + key, e);\n        return Optional.empty();\n    }\n}\n\npublic <T> T get(String key, Class<T> tClass, T defaultValue) {\n    return get(key, tClass).orElse(defaultValue);\n}\n```\n\n추가적으로 메모리에 없는 key 값을 조회하고자 할 때 null 이나 Optional.empty() 를 반환하면 외부에서는 이에 대한 후속 처리를 해줘야 한다. 이 때 defaultValue 를 지정할 수 있게 해준다면 사용하는 쪽 로직이 좀 더 편해질 수 있다.\n\n## 결론\n\n이제 약속된 채널로 publish 를 하면 subscribe 하고 있는 모든 서버들에서 일괄적으로 특정 설정값을 바꿔 사용할 수 있다. 지금은 단순하게 key, value 값을 동적으로 변경하는 곳에 pub/sub 을 사용하였지만, 조금만 응용하면 여러가지 부분에 사용할 수 있을 것이다. 예제 소스는 [여기](https://github.com/nevercaution/spring-boot-redis-pub-sub) 에서 확인할 수 있다.\n\n## 추신\nredis pub/sub 을 이용하면 기존에 사용하고 있던 부분에서 전혀 다른 페러다임으로 동적인 데이터들을 갱신할 수 있다. 현재 사내에서는 레디스 데이터 캐싱을 시간단위로 기록하고 있는데, 잘 활용하면 실시간으로 캐시 데이터에 반영을 할 수도 있을거라 기대할 수 있다. 하지만 모든 과하면 안된다. publish 하는 채널들이 많아지고 subscribe 하는 구독자들이 많아지거나, redis cluster 에서 subscription 을 과도하게 한다면 오히려 안좋은 결과를 가져올 것이다. 그러므로 충분히 검토한 후 각자 목적에 맞는 곳에 사용하는 것이 좋다고 생각된다.\n\n\n","fields":{"slug":"/spring-boot-redis-pub-sub/"},"frontmatter":{"title":"spring boot 에서 redis pub/sub 을 이용해 동적으로 value 사용하기","published":true}}},{"node":{"rawMarkdownBody":"\n## elasticsearch & spring boot & webflux\n\n### elasticsearch 6.3.0\nelasticsearch 가 [6.3.0](https://www.elastic.co/blog/elasticsearch-6-3-0-released)  으로 올라갔다. 여러가지 기능들이 추가되었는데, 흥미로운 부분은 내부 client 로 SQL 문법을 지원한다는것이다. 생각보다 놀랍게 동작한다! 메인 버전이 올라가면서 java client 버전도 함께 올라갔는데 드디어 java rest high level client 가 쓸만한 모듈이 되었다는 점이다. 예전까지는 java client 만 사용해왔었는데 이젠 슬슬 넘어갈 수 있을 것같아서 후딱 한번 사용해보았다.\n\n### RestHighLevelClient\n[이전 포스트](https://nevercaution.github.io/2018/03/15/elasticsearch-rest-client/) 에서 살짝 훑어만 봤었는데 elasticsearch java client 가 6.3.0 으로 업데이트 되면서 대부분의 기능들을 사용할 수 있게 되었다. 언제나 릴리즈되나 눈빠지게 기다리고 있었는데 6.3.0 이 릴리즈 되자마자 한번 적용해보았다.\n\n### async\nRestHighLevelClient 에서는 대부분의 메소드에 async 를 지원한다. spring webFlux 를 공부하면서 elasticsearch 가 async 를 지원하지 않아서 반쪽짜리로 사용하고 있었는데, 함께 사용해보니 궁합이 꽤나 잘맞았다.\n\n### 오늘의 목표\n이번 포스팅은 webFlux 와 elasticsearch 가 짬뽕이 되었다. 두 꼭지에서 꼭 알고 가야하는 부분만 짚고 가보자. 사용할 제원(?)은 다음과 같다.\n1. spring boot 2.0.3\n2. spring boot webFlux\n3. elasticsearch RestHighLevelClient (6.3.0)\n\n\n## shut up and code\n\n### setting\n\n`build.gradle`\n\n```gradle\nbuildscript {\n    ext {\n        springBootVersion = '2.0.3.RELEASE'\n    }\n    repositories {\n        mavenCentral()\n    }\n    dependencies {\n        classpath(\"org.springframework.boot:spring-boot-gradle-plugin:${springBootVersion}\")\n    }\n}\n\n...\n\ndependencies {\n    // for webFlux\n    compile('org.springframework.boot:spring-boot-starter')\n    compile('org.springframework.boot:spring-boot-starter-webflux') // 1\n    compileOnly(\"org.springframework.boot:spring-boot-configuration-processor\")\n\n    // for elasticsearch\n    compile('org.elasticsearch.client:elasticsearch-rest-high-level-client:6.3.0')\n    compile('org.apache.logging.log4j:log4j-core:2.9.1')\n    compile('org.elasticsearch:elasticsearch:6.3.0') //2\n}\n```\n\n프로젝트는 gradle 로 구성했다. 개인적으로는 maven 보다 가독성이 좋고 문법이 간결하다고 생각한다. xml 을 그닥 좋아하지 않는 경향도 있고, 뎁스가 깊어지거나 설정자체가 길어지면 한눈에 어떤 dependency 가 걸려있는지 파악하기가 쉽지 않기 때문이다. 위의 설정에서 짚고 넘어갈 부분이 있다.\n\n1. spring boot starter 에는 web 과 webflux 가 있다. web 은 `org.springframework.boot:spring-boot-starter-web` 이렇게 사용하는데 web 은 기본적으로 tomcat 으로 동작하고, webflux 는 netty 로 was 가 동작한다. 여기에서 주의할 점은 web과 wenflux 를 함께 넣으면 기본적으로 web 으로 동작하고 webflux 는 무시된다. 해당 내용은 [여기](https://docs.spring.io/spring-boot/docs/current-SNAPSHOT/reference/htmlsingle/#boot-features-webflux) 에서 확인할 수 있다.\n\n2. `org.elasticsearch.client:elasticsearch-rest-high-level-client:6.3.0` 만 추가하면 기본적으로 elasticsearch 5.6.x 버전의 client 가 dependency 로 따라온다. 기본적으로 사용함에 있어서는 큰 문제는 없지만 일부 기능들이 호환이 잘 안되서 엉뚱하게 동작하는 경우가 있다. 이는 rest client 로 사용했을 때에도 비슷한 상황이 있었는데 명시적으로 elasticsearch client 버전을 명시해주면 원하는 버전의 client 를 사용할 수 있다. 되도록 rest high level client 와 동일한 버전을 사용하는걸 추천한다.\n\n\n`ElasticsearchConfig.java`\n\n```java\n@Configuration\npublic class ElasticsearchConfig {\n\n    @Bean\n    public RestHighLevelClient client(ElasticsearchProperties properties) {\n        return new RestHighLevelClient(\n                RestClient.builder(properties.hosts())\n        );\n    }\n}\n```\n\nconfiguration 의 기본 설정은 rest client 와 기본 골격은 비슷하게 생겼다. (설정이나 다른 부분들이 몇가지 있지만) 크게 다른 점을 하나 꼽자면 rest high level client 부터는 tcp connection 이 아닌 http connection 을 맺는다. 그래서 기본 (elasticsearch port 인) 9200 을 사용한다. rest client 에서는 `InetSocketTransportAddress` 를 생성해서 builder 에 전달을 했었지만 restHighLevelClient 부터는 `HttpHost` 를 전달해서 생성하는 부분도 눈여겨 볼만한 부분이다. elasticsearch 설정 관련 클래스는 ElasticsearchProperties 로 묶었다. `@Value` 나 `ConfigurationProperties` annotation 으로 configuration 에서 직접 설정도 가능하지만, 특정 설정값을 service 에서 사용하기 편하기 위해 클래스를 분리했다.\n\n\n\n`ElasticsearchProperties.java`\n\n```java\n@Component\n@Setter\n@ConfigurationProperties(prefix = \"elasticsearch\")\npublic class ElasticsearchProperties {\n\n    private List<String> hosts;\n\n    public HttpHost[] hosts() {\n        return hosts.stream().map(HttpHost::create).toArray(HttpHost[]::new);\n    }\n}\n```\n\n`@Value` annotation 으로 설정해주어도 되지만, 이름이 완전히 같을 때만 설정값을 읽어들인다. `@ConfigurationProperties` annotation 은 [Relaxed Binding](https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-external-config.html#boot-features-external-config-relaxed-binding) 이라는 느슨한(?) 룰이 있어 각자 입맛에 맞는 포멧으로 써도 properties class 에서 찰떡같이 알아 먹는다. (자신만의 스타일을 고집하는 팀에서는 나름 유용하게 사용될지는 모르겠지만 남용은 하지 말자. 버그가 생길 수 있는 포인트이다.)\n\n`application.yml`\n\n```yaml\nelasticsearch:\n  hosts: http://localhost:9200\n\n```\n\n설정파일은 이전보다 심플해졌다. 심지어 cluster name 을 묻지도 따지지도 않는다. 연결을 맺을 때 굳이 너의 이름을 몰라도 상관없다는 뜻이였을까.\n\n### service\n\ndocument 의 생성과 조회를 만들어 보자. (update 와 delete 는 index 와 get/match 와 비슷하기 때문에 생략한다.)\n\n* `index`\n\n```java\npublic Mono<Void> index(String index, String type, String userName, String message) {\n        Gson gson = GsonUtil.gson();\n\n        User user = new User();\n        user.setUser(userName);\n        user.setMessage(message);\n        user.setPostDate(new Date());\n\n        IndexRequest indexRequest = new IndexRequest(index, type);\n        indexRequest.source(gson.toJson(user), XContentType.JSON);\n\n\n        return Mono.create(sink -> {  //1\n            restHighLevelClient.indexAsync(indexRequest, new ActionListener<IndexResponse>() {  //2\n                @Override\n                public void onResponse(IndexResponse indexResponse) {\n                    log.info(\"index success : \"+indexResponse.toString());\n                    sink.success();\n                }\n\n                @Override\n                public void onFailure(Exception e) {\n                    log.error(\"index error \", e);\n                    sink.error(e);\n                }\n            });\n        });\n    }\n```\n\nindex 메소드는 특정한 반환값을 필요로 하지 않기 때문에 `Mono<Void>` 를 사용한다. client 에서는 기존의 index 메소드와 indexAsync 를 제공한다. (대부분의 메소드들이 동일하게 제공) 위의 동작은 공식문서의 [index](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html#docs-index_) 부분의 동작을 코드로 작성한 부분이다. url 로 풀어 쓰면 다음과 같다.\n\n```\n$ curl -X PUT \"localhost:9200/twitter/_doc/1\" -H 'Content-Type: application/json' -d'\n{\n    \"user\" : \"kimchy\",\n    \"post_date\" : \"2009-11-15T14:12:12\",\n    \"message\" : \"trying out Elasticsearch\"\n}\n'\n```\n\nindexAsync 메소드를 사용했고 이를 [MonoSink](https://projectreactor.io/docs/core/release/api/index.html?reactor/core/publisher/Mono.html) 로 감싸주었다. Mono 는 0..1 의 상태이므로 하나의 index 결과에 대한 성공여부를 publish 해준다. controller 에서는 다음과 같이 받아 처리해준다.\n\n`controller.java`\n\n```java\n@RequestMapping(value = \"/index/{index}/{type}\", method = {RequestMethod.POST})\n    public Mono<Void> index(@PathVariable(\"index\") String index,\n                            @PathVariable(\"type\") String type,\n                            @RequestParam(value = \"user_name\") String userName,\n                            @RequestParam(value = \"message\") String message) {\n        return elasticsearchService.index(index, type, userName, message)\n                .onErrorResume(error -> {\n                    log.error(\"index error \", error);\n            return Mono.empty();\n        });\n    }\n```\n\nindex 동작 이후 특정한 값을 반환하고자 한다면 Mono 안의 타입을 원하는 클래스를 정의해주자.\n\n* `get`\n\ndocument 를 넣었으면 값이 잘 들어갔는지 확인이 필요하겠다. [get api](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-get.html#docs-get) 를 추가해보자.\n\n```\n$ curl -X GET \"localhost:9200/twitter/_doc/0\"\n```\n\n이 api 를 rest high level client 로 작성하면 아래와 같다.\n\n```java\npublic Mono<User> getUser(String index, String type, String id) {\n    final Gson gson = GsonUtil.gson();\n    GetRequest getRequest = new GetRequest(index, type, id);\n    return Mono.create(sink -> {\n        restHighLevelClient.getAsync(getRequest, new ActionListener<GetResponse>() {\n            @Override\n            public void onResponse(GetResponse documentFields) {\n                User user = gson.fromJson(documentFields.getSourceAsString(), User.class);\n                sink.success(user);\n            }\n\n            @Override\n            public void onFailure(Exception e) {\n                e.printStackTrace();\n                sink.error(e);\n            }\n        });\n    });\n}\n```\n\nindex 에서와 같이 Mono 로 감싸서 반환을 하였고 이번에는 필요한 값인 User 객체를 사용하였다.\n\n```java\n@GetMapping(\"get/{index}/{type}/{id}\")\npublic Mono<User> getAsync(@PathVariable(\"index\") String index,\n                           @PathVariable(\"type\") String type,\n                           @PathVariable(\"id\") String id) {\n\n   return elasticsearchService.getUser(index, type, id)\n           .onErrorResume(error -> {\n               User defaultUser = new User();\n               defaultUser.setUser(\"default\");\n               defaultUser.setPostDate(new Date());\n               defaultUser.setMessage(\"default message\");\n               return Mono.just(defaultUser);\n           })\n           .defaultIfEmpty(new User());\n}\n\n```\n\nservice 에서 `Mono<User>` 를 받아 반환을 해준다. `onErrorResume` 이나 `defaultIfEmpty` 는 service 에서 정상동작을 하지 않았을 경우의 후처리 이다. 여기에서 짚고 넘어갈 부분이라면, 우리는 restful Api 를 만들고 있기 때문에 controller 가 `@RestController` 이거나 mapping method 에 `@ResponseBody` annotation 이 걸려있으면 위와 같이 바로 `Mono` 를 반환해도 된다.\n하지만 `@Controller` annotation 을 사용하고 있다면 http status 값이 넘어가지 않기 때문에 mapping method 에 `@ResponseBody` 를 걸어주거나 `ResponseEntity` 객체로 한번 더 감싸서 반환해주어야 한다. 다음과 같이 감싸줄 수 있겠다.\n\n```java\n@GetMapping(\"get/{index}/{type}/{id}\")\npublic Mono<ResponseEntity<User>> getAsync2(@PathVariable(\"index\") String index,\n                                           @PathVariable(\"type\") String type,\n                                           @PathVariable(\"id\") String id) {\n\n    return elasticsearchService.getUser(index, type, id)\n            .map(ResponseEntity::ok)\n            .onErrorResume(error -> Mono.just(ResponseEntity.badRequest().build()))\n            .defaultIfEmpty(ResponseEntity.status(HttpStatus.OK).body(new User()));\n}\n```\n\n취향과 상황에 맞게 선택하도록 하자.\n\n* `match all`\n\n[match all api](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-all-query.html#query-dsl-match-all-query) 도 추가해보자.\n\n```\n$ curl -X GET \"localhost:9200/_search\" -H 'Content-Type: application/json' -d'\n{\n    \"query\": {\n        \"match_all\": {}\n    }\n}\n'\n```\n\n코드로 작성하면 다음과 같다.\n\n```java\npublic Flux<User> matchAll(String index) {\n    final Gson gson = GsonUtil.gson();\n\n    SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();\n    searchSourceBuilder.query(QueryBuilders.matchAllQuery());\n\n    SearchRequest searchRequest = new SearchRequest(index);\n    searchRequest.source(searchSourceBuilder);\n\n    return Flux.create((FluxSink<User> sink) -> {\n        restHighLevelClient.searchAsync(searchRequest, new ActionListener<SearchResponse>() {\n            @Override\n            public void onResponse(SearchResponse searchResponse) {\n                searchResponse.getHits().forEach(item -> {\n                    User user = gson.fromJson(item.getSourceAsString(), User.class);\n                    sink.next(user);\n                });\n                sink.complete();\n            }\n\n            @Override\n            public void onFailure(Exception e) {\n                log.error(\"matchAll error \", e);\n                sink.error(e);\n            }\n        });\n    });\n}\n```\n\n\n```java\n@GetMapping(\"/match_all/{index}\")\npublic Flux<User> matchAll(@PathVariable(\"index\") String index) {\n\n    return elasticsearchService.matchAll(index).onErrorResume((Throwable error) -> {\n        log.error(\"err\", error);\n        User user = new User();\n        user.setPostDate(new Date());\n        user.setUser(\"default User\");\n        user.setMessage(\"default message\");\n        return Flux.just(user);\n    });\n}\n```\n\n복수개의 값을 반환할 수 있으므로 Flux 로 반환해준다. 물론 List 로 묶어서 Mono 로 반환해야할 수도 있다. MonoSink 와의 차이점은 next 로 데이터를 넘기다가 모든 데이터가 넘어가면 complete 를 호출 해주는 부분이다.\n\n\n## 결론\nwebflux 와 restHighLevelClient 사용하여 비동기 api 를 제공해줄 수 있다. webflux 는 꽤나 멋지게 동작하지만 아직까지는 모든 서드파티들이 지원을 해주고 있지 않아 하드하게 사용하기에는 주저되는 부분이 있다. (jdbc 를 사용하지 않는다면 추천!)\nrestHighLevelClient 로 넘어오면서 각 클래스들의 역할이 명확해진 느낌이고 선언과 사용도 좀 더 명확해져서 편하다는 인상을 많이 받았다. 위의 예제코드는 [여기](https://github.com/nevercaution/elasticsearch_java_client) 에서 확인할 수 있다.\n\n### 여담\nelasticsearch 에 kibana 를 얹으면 devtool 을 이용해서 (무려 자동완성이 제공되는) 쿼리를 날려볼 수 있다. 지금껏 sense 나 다른 툴들을 조금은 불편하게 사용해왔었는데, 반길만한 부분이다. 다만 개인적으로는 redis-cli 처럼 CLI tool 이 제공되면 좋았을 거라는 생각에 [elasticsearch-cli](https://github.com/nevercaution/elasticsearch-cli) 를 만들어 보는 중이다. 물론 elastic 진영에서는 [curator](https://www.elastic.co/guide/en/elasticsearch/client/curator/current/about-cli.html) 을 제공해주고 있긴 하지만 모니터링 툴이라 간단한 match query 나 analyze 등을 사용할 수는 없다. 나름 일하면서 편하게 사용하는중이다. ㅎㅎ","fields":{"slug":"/elasticsearch-java-high-level-client/"},"frontmatter":{"title":"Elasticsearch Java Rest High Level Client 사용하기","published":true}}},{"node":{"rawMarkdownBody":"\n### 버전을 알고 싶다\n사내에서 서비스 하고 있는 프로젝트에서 버그가 발견되었다. 그런데 이 버그는 원래는 없었는데 어느 순간부터 갑자기 등장했다. 그렇다면 어느 순간에 추가된 코드에 내재되어 있는 버그라는 이야기 인데 이를 추적하기 위해서는 커밋 로그를 모두 찾아야 한다. 로그에 버전 정보를 심을 수 있다면 특정 버전부터 발생했는지 여부를 쉽게 알 수 있을 것이다. 버전은 하루에도 몇번씩 올라갈 수 있기 때문에 버그가 발생한 최초 버전을 파악할 수 있으면 좀 더 유연한 대처가 가능하겠다.\n\n### 더 많은 정보를 알 수는 없을까?\n사내에서 사용하고 있는 spring-boot proejct 는 multi project 로써 사용하고 있고 각 팀별로 module name 을 갖고 있다. 특정 모듈들은 공통으로 사용하고 있기 때문에 (mysql, util, redis 등) 내가 쏘아올린 작은 공이 언제 어느 프로젝트에서 영향을 줄지 금방 파악하기 힘들다. 이전에 작성했던 [ Spring-boot Actuator 포스팅](https://nevercaution.github.io/2018/03/24/spring-boot-actuator/) 을 이용해 git 정보를 특정 endpoint 로 제공하는 기능을 만들어서 사내에서 유용하게 사용하고 있는데, 이와같이 각 팀별로 사용하고 있는 프로젝트에 정보들을 내가 마음대로 사용할 수 있으면 좋을 것 같다.\n\n### Gradle ext info\n우리는 내부적으로 약속된 값을 통해 각 팀별로 특정 정보들을 사용한다.\n\n`build.gradle`\n\n```\next {\n    appVersion = '1.0.1-SNAPSHOT'\n    projectName = \"teddy.bear\"\n}\n```\n사내에서 장애 알림은 [sentry](https://sentry.io/) 를 사용하는데, 어떤 에러가 발생했을 때 현재 배포되어 있는 버전을 함께 표시해주면 해당 에러가 어떤 버전에서 최초 발생했는지 추적이 가능하다. (장애상황에서도 활용이 가능하지만 필요에 따라 gradle 의 정보를 마음대로 사용할 수 있으면 상황에 따라 장점이 있다고 생각한다.)\n\n### Shut up and code\n공식문서의 [Automatic Property Expansion Using Gradle](https://docs.spring.io/spring-boot/docs/current/reference/html/howto-properties-and-configuration.html#howto-automatic-expansion-gradle) 부분을 참고해서 만들었다. 잘 동작한다. :)\n`build.gradle` 에 정의되어 있는 값들을 사용하기 위해선 다음과 같이 설정해주면 된다. 공식문서에서 주석을 달아놓은 부분이 있는데, `SimpleTemplateEngine` 에서는 `$` 를 파싱하는 부분에서 충돌이 발생할 수 있어 별도의 처리를 해야 한다고 명시되어 있다. 참고로 이 부분에서 실제 프로젝트에 적용시킬 때 문제가 조금 있었는데, 단순히 `expand(project.properties)` 만 하게 되면 모든 설정파일을 가져가게 된다. 파일들에 `$` 를 사용했다면 파싱을 하다가 깨질 수 있으니 escape 처리를 해주어야 하는데, 현재 상황은 내가 필요한 설정 파일만 가져가면 되기 때문에 내가 원하는 파일들만 expand 하도록 하자. 별도의 처리를 하지 않으면 에러를 내며 `processResources` 에서 작업이 멈춘다.\n\n```bash\n:processResources FAILED\n\nFAILURE: Build failed with an exception.\n\n* What went wrong:\nExecution failed for task ':processResources'.\n> Could not copy file '/path/to/file/error.ftl' to '/path/to/file/build/resources/main/error.ftl'.\n```\n\n`filesMatching` 로 원하는 파일 포멧만 하도록 감싸주자. (실제 이 부분에서 적용하다가 에러를 발생해서 모든 `$` 에 escape 처리를 할까 하다가 그건 좀 아닌것 같아 아래와 같이 처리했다.)\n\n`build.gradle`\n\n```\nprocessResources {\n    filesMatching('**/application.yml') {\n        expand(project.properties)\n    }\n}\n```\n\n`application.yml`\n\n```yml\next.appVersion: ${ext.appVersion}\next.projectName: ${ext.projectName}\n```\n\n`Info.class`\n\n```java\n@Value(\"${ext.appVersion}\")\nprivate String appVersion;\n\n@Value(\"${ext.projectName}\")\nprivate String projectName;\n```\n\n```bash\n$ curl localhost:8081/main\n\nproject name : teddy.bear, version : 1.0.1-SNAPSHOT\n```\n\n설정과 사용방법은 간단하다. 그렇다면 어떻게 이게 가능한지 조금만 더 살펴보자.\n핵심은 `build.gradle` 에서 명시해준 `expand(project.properties)` 의 동작인데 build 를 하게 되면 다음의 동작을 수행한다.\n\n```bash\n$ ./build\nExecuting task 'build'...\n\n:bootBuildInfo\n:compileJava\n:processResources   <- here!\n:classes\n:bootJar\n:jar SKIPPED\n:assemble\n:compileTestJava\n:processTestResources NO-SOURCE\n:testClasses\n:test\n:check\n:build\n```\n빌드 동작에서 컴파일을 하고 resources 파일들을 말아서(?) 만들어 주는데, `application.xml` 에 명시 되어 있는 `ext` 관련 값들의 매핑을 시켜줄 때 `gradle` 에 명시 되어 있는 값들로 채워준다. 빌드가 완료된 후 `build/resources/main/application.xml` 파일을 보면 매핑된 값들로 채워져 있는 것을 볼 수 있다.\n\n`application.xml`\n\n```xml\next.appVersion: 1.0.1-SNAPSHOT\next.projectName: teddy.bear\n```\n\n저 위에서 작성한 코드가 위와 같이 바뀌어 있음을 확인할 수 있다.\n\n\n### 결론\n구글링을 조금 해보니 gradle 의 명시되어 있는 정보들을 다양하게 활용할 수 있었다. 회사에서 급한 마음에 이리저리 해볼 때는 잘 안되더니 카페에 와 여유롭게 마음 잡고 해봐야지 했더니 30분만에 해결되어서 조금 당황했었다(..) 이 예제 코드 전체는 [예제 코드](https://github.com/nevercaution/gradle_info) 에서 확인할 수 있다. 앞으로는 경건한 마음으로 차분하게 살펴보도록 노력하자.\n\n\n\n\n\n\n\n\n\n\n","fields":{"slug":"/spring-boot-use-gradle-value/"},"frontmatter":{"title":"Spring boot 에서 gradle 에 정의되어있는 정보 가져오기","published":true}}},{"node":{"rawMarkdownBody":"\n### elasticsearch 의 검색을 사용하기 까지\n사내에서 elasticsearch 를 처음 사용하면서 로그 수집정도에만 사용하고 있었다. 워낙에 [elk](https://www.elastic.co/kr/elk-stack) 가 서로 잘 작동하기도 했고 데이터 수집부터 분석까지 너무나도 유려한 기능들을 갖추고 있었다. 서비스에서의 검색을 처음 도입해본 경험은 사내 정산 시스템을 만들면서 였는데, 작품이름 검색이나 작가등의 이름으로 검색을 하게 해달라는 요청이 있어서 였다.\n데이터가 db 에 있어 단순하게 like 검색으로 구현할까 했었는데, 생각보다 결과에 대한 품질이 너무 낮았다. 결과에 대한 점수 부여 방법도 딱히 없고 애초에 조건이 하나만 걸리니 내가 원하는 결과를 얻을 수 없었다.\n그래서 안되겠다 싶어 elasticsearch 와 은전한닢을 이용해 구현했고,  거기서 자신감이 생겨 서비스의 검색도 과감하게 elasticsearch 로 구현하자고 주장했다. 처음에 회사에서는 반기는 분위기는 아니였다.\n기존에 (나름 잘) 동작하는 검색이 구현되어 있었고 한번도 도전해보지 않은 검색이라는 서비스를 잘 구현할 수 있을까에 대한 걱정이 있었다. 그리고 이미 카카오에서 제공하는 검색엔진이 있고 이를 이용하면 우리쪽에서 굳이 만들 이유가 없어서 였다. 카카오에서 제공하는 기능들을 이용하기만 해도 되었지만 앞으로 우리가 우리만의 기술을 갖게 된다면 그 자체로 의미가 있다 생각했고, 결과로 보여드려야겠다고 생각해서 나는 카카오페이지의 검색을 만들어 보기로 했다.\n\n### 기존 구성\n나는 일단 기존의 검색이 어떻게 동작하고 있는지 부터 분석을 해보기로 했다. 지정된 시간마다 단어들을 수집해서 잘게 쪼갠 뒤 테이블에 데이터를 저장해놓고, 실제 서비스에서는 나름의 공식을 이용해 점수를 부여해서 검색 결과를 내어주는 방식이었다. mysql 의 fts 를 이용한 방식이었는데 나름 속도도 괜찮고 완벽하진 않았지만 검색의 결과도 그리 나쁘지는 않았다.\n처음에는 이 검색 서비스부터 이겨보자는 생각으로 다른 검색 서비스들이 검색을 어떻게 서비스 하고 있는지 부터 관찰했다. 개발 초기에는 [리디북스](https://ridibooks.com/)나 [레진코믹스](https://www.lezhin.com/ko) 같은 사이트들의 검색을 많이 참고했다.\n\n### 은전한닢\n한글 형태소 분석기는 크게 3가지가 있다. [elasticsearch 에서 제공하는 형태소분석기 분석](https://www.elastic.co/kr/blog/using-korean-analyzers)에서 보면 arirang, 은전한닢 그리고 open-korean-text(구 twitter-korean-text) 등이 있는데 각 장단점들이 명확해서 아직까지는 형태소 분석기의 절대적인 강자는 없는듯 하다.\n각자 자신에게 맞는 형태소 분석기를 사용하면 되는듯 한데, [리디북스에서 은전한닢을 사용하다는 기술 블로그](https://www.ridicorp.com/blog/2016/04/18/ridibooks-stack/)를 보고 생각의 결과가 많이 굳기도 했지만 음절 분석결과가 가장 자세하고 속도도 괜찮은 은전한닢을 사용하기로 했다.\n\n### 클러스터 구성\n2대의 노드로 클러스터를 구성해서 검색 색인을 하고 있고, 주기적으로 추가/변경되는 정보들을 인덱스에 반영하고 있다. 추가적으로 실제 서버 데이터와 개발에서의 구성도 elasticsearch 로 구성했는데, 테스트 서버군은 하나의 노드로 구성했고 각 테스트군별로 색인을 따로 구성했다.\n\n### 데이터 수집및 정제\n검색 쿼리만큼이나 중요한 부분이 데이터 수집이다. 어찌보면 색인이 얼마나 잘 되어 있느냐에 따라 검색 결과의 품질이 좌우된다. 색인과 검색의 비율을 따져 생각해본다면 6:4 정도라고 생각한다. 아무리 많은 데이터를 사용한다 하더라도 데이터가 잘 저장되어 있지 않으면 아무런 소용이 없기 때문이다.\n일단 데이터를 주기적으로 수집하기로 했고, 카카오 검색팀의 조언을 받아 단어에 대한 점수에 대한 결과도 함께 수집을 한다. logstash 로 처음에는 데이터를 수집하려고 했는데 점수부여에 대한 로직이나 데이터 정제의 편의성을 위해 배치작업을 데이터를 수집했다. 간략하게 수집되는 정보들을 보면 다음과 같다.\n\n* 열람수, 구매자수, 인기도, 기다리면 무료의 대한 점수 계산\n* 랭킹데이터 수집\n* (정산 시스템 구축의 경험으로) 계약 주체에 따른 점수 부여\n* 운영자가 등록한 태그 수집\n* 작품명, 작가명, 발행자명, 출연진등의 정보 수집\n\n가능한 모든 정보를 수집하고자 했고, 은전한닢으로 기분석된 단어와 순수한 단어를 따로 인덱싱을 했다. 이유는 나중에 검색부분에도 언급하겠지만 한글 형태소 분석기로 분석된 단어와 그렇지 않은 단어를 함께 검색 쿼리에 넣어 검색의 결과를 높이기 위함이다. 형태소 분석기를 통해 토크나이징을 하게 되면 자체적으로 단어를 쪼개게 되는데, 이 때 원치 않게 단어가 나뉠수도 있기 때문이다.\n예를 들어 `김비서` 이라는 단어를 형태소 분석기에 넣고 돌리면 엉뚱한 결과가 나온다.\n\n```\n김비서\n김비\tNNP,인명,F,김비,*,*,*,*\n서\tJKB,*,F,서,*,*,*,*\nEOS\n```\n일반적으로 생각했을 때 `김`, `비서` 로 나올거라 예상했지만 결과는 이와는 달리 `김비`, `서` 로 나뉘게 되버린다. 원인은 제공되는 정보가 부족해서인데, 모든 문장을 써주면 정상적으로 표시 된다.\n\n```\n김비서가왜그럴까\n김\tNNP,인명,T,김,*,*,*,*\n비서\tNNG,*,F,비서,*,*,*,*\n가\tJKS,*,F,가,*,*,*,*\n왜\tMAG,문장부사/양상부사,F,왜,*,*,*,*\n그럴까\tVA+EC,*,F,그럴까,Inflect,VA,EC,그렇/VA/*+ᆯ까/EC/*\n```\n\n이런식의 토크나이징은 검색에서도 문제가 된다. elastisearch 에서 검색을 태우면 색인되어 있는 타입에 따라 기분석해서 나온 토큰들로 검색을 하게 되는데 사용자가 `김비서` 라고 검색을 하게 되면 `서` 라는 토큰이 나오게 되기 때문에 엉뚱한 검색 결과가 나오게 된다.\n추가적으로 난감했던 단어는 `하라간` 과 같이 사전에 등록되지 않은 고유명사들의 경우였다. 은전한닢은 세종 말뭉치를 사용하고 있는데, 비교적 최근에 추가되는 신조어들이나 작품에서 의도적으로 사용되는 고유명사들에 대한 데이터는 존재 하지 않는다. `하라간` 의 경우엔 형태소 분석기에서는 아예 알아들을 수 없는 단어가 되어버린다.\n\n```\n하라간\n하\tVV,*,F,하,*,*,*,*\n라\tEC,*,F,라,*,*,*,*\n간\tNNG,*,T,간,*,*,*,*\nEOS\n```\n(...여기서 멘붕이 왔었다.)\n임시방편으로 사용했던 방법은 mecab 에서 제공하는 사용자 사전이었는데, 인명이나 지명등의 단어를 넣으면 형태소 분석기에서 의도적으로 단어를 쪼개지 않고 단어 그자체로서 인식을 가능하게 한다.\n하지만 4만개가 넘는 모든 작품들을 모니터링하고 일일히 결과를 들여다보면서 사용자 사전에 필요한 단어들을 넣는건 나 혼자 작업으로는 불가능했었고, 처음에는 사용자 cs 나 내부적으로 결과가 이상한 단어들에 대해서만 사전에 등록했었다. (사용자 사전은 나중에 좀 더 자세하게 이야기 해보자.)\n\n### 데이터 색인\n데이터는 항상 추가된다. 하지만 판매중이었다가 여러 이유로 판매가 중지되는 작품도 있고 운영자에 의해 작품의 정보가 변경되는 일도 꽤나 잦았다. 이미 존재하는 필드에 값을 갱신하려고 했더니 경우의 수가 너무 많았다. 모든 상황에 대해 신시간으로 대응하기 위해서 [다른 곳에서 아이디어](https://www.elastic.co/kr/blog/changing-mapping-with-zero-downtime)를 얻었는데 elasticsearch 에서 제공하는 [alias](https://www.elastic.co/guide/en/elasticsearch/reference/6.2/indices-aliases.html) 와 일반적인 [blue-green deploy](http://en.dwhwiki.info/concepts/blue-green-development) 방법을 이용해서 모든 데이터를 주기적으로 새로 갱신하는 방법을 이용했다. `index-blue`, `index-green` 를 하나씩 생성하고 `index` 라는 `alias` 를 걸어 실제 검색 쿼리는 `index alias` 를 바라보게 했다. 데이터 수집 배치는 독립적으로 동작하고 있기에 실시간으로 필드를 추가하는 장점도 함께 가져갈 수 있다.\n\n### 검색 쿼리\n초기의 버전에서는 사용자 사전을 최대한 사용하지 않는 방향으로 진행했었다. 위에서 언급한 바와 같이 모든 고유명사들을 수동으로 쪼개는 작업이 물리적으로 불가능했었기 때문인데, 이런 제한을 극복하고자 최대한 검색 쿼리를 세부적으로 쪼개서 검색의 결과를 높이고자 했다.\nmapping 에서 title 에 관련된 정보는 다음과 같이 구성했다.\n\n```\n\"title\": {\n    \"type\": \"text\",\n    \"analyzer\": \"seunjeon_analyzer\"\n},\n\"titleBigram\": {\n    \"type\": \"text\",\n    \"analyzer\": \"han_bigrams\"\n}\n\"titleStandard\": {\n    \"type\": \"text\",\n    \"analyzer\": \"exact\"\n}\n```\n* [은전한닢](http://eunjeon.blogspot.kr/)\n* [bigram](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-cjk-bigram-tokenfilter.html)\n* 띄어쓰기등을 제거한 [standard analyzer](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-standard-analyzer.html)\n\n검색 쿼리를 수행시 3개의 필드에 대해 각기 다른 boost 를 줘서 가장 근접한 결과에 대해 점수를 높게 부여해서 결과를 만들었다. bigram 을 사용한 이유는 앞서 언급한 형태소 분석기의 한계를 대체하기 위해 색인되는 단어들을 ngram 으로 조개어 저장해서 엉뚱한 결과가 나오는 것에 대한 대처였다. (나중에 다시 언급하겠지만 결과적으로는 `bigram` 을 사용하지 않게 되었다.)\n `MatchQuery` 로 검색을 하는데 위에서 언급한 대로 기분석된 색인이 엉뚱하게 저장되는 경우들이 꽤나 빈번했기 때문에 초기 검색결과는 가히 충격적이었다. 왜 이 단어가 검색이 되었지? 하는 결과들이 포함되어 있었기 때문이었다. 이후에 카카오 검색셀의 조언을 받아 쿼리는 대대적으로 튜닝하게 되었다.\n\n### elasticsearch 버전 관리\n개발 초기에는 elasticsearch 최신 버전이 5.1.1 이었다. 검색 서비스를 라이브에 반영하게 되고 보니 버전은 6.x 까지 올라가버렸다. 요즘 추세인지는 모르겠지만 어째 버전업이 너무나도 빠르게 진행되고 있었다. 이전 포스트에서도 얘기 했었던 [elasticsearch 기술 지원](https://www.elastic.co/support/eol) 때문에라도 elasticsearch 버전을 주기적으로 (꽤나 빈번하게) 올리기도 했다. rolling upgrade 에 대한 내용은 [여기](https://nevercaution.github.io/2018/03/15/elasticsearch-rest-client/)에서 자세하게 다루었다.\n\n### 쿼리 튜닝\n사용자 사전 없이 `title`, `titleBigram`, `titleStandard` 만들어 검색쿼리를 날리면 정말 검색 결과가 엉뚱하게 나오는 경우가 있다. 위에서도 언급한 토크나이징 때문인데 이는 사용자 사전 없이는 사실상 불가능 하다. 사용자 사전을 추가하고 난 후에는 bigram 은 사용하지 않고 `title`, `titleStandard` 두 필드를 사용해서 `MatchQuery` 를 날린다. `bigram` 도 결국에는 단어를 ngram으로 추출하는 방식인데 근본적인 해결책이 되진 못하기 때문이다.\n그리고 `FunctionScoreQuery`, `FilterFunction` 를 이용해서 결과에 대한 점수 부여를 추가했다. 점수 부여에 관한 로직은 카카오 검색셀(hao) 의 도움을 받아 구현했는데, 위에서 데이터 수집당시에 점수를 부여할 수 있는 판매량이나 열람수와 같은 정보를 이용해서 내부적인 공식을 적용해서 점수를 부여했다.\n쿼리 튜닝이라고는 했지만 결과적으로는 인덱싱할 때부터 데이터를 잘 만드는게 큰 도움이 되었다. 잘 만들어진 데이터가 있어야 좋은 검색이 나올 수 있기 때문이다.\n\n### 사용자 사전 관리\n내가 의도한대로 토크나이징이 되기 위해 결국엔 사용자 사전을 추가하게 되었다. 또한 신조어들의 대한 대응을 하기 위함이었는데, 예를 들어 `레벨업` 이라는 단어가 포함된 작품을을 검색할 때 기존에는 `레벨` 과 `업` 으로 쪼개버린다.\n\n```\n레벨업\n레벨\tNNG,*,T,레벨,*,*,*,*\n업\tNNG,*,T,업,*,*,*,*\nEOS\n```\n이 경우에는 `업` 이라는 단어가 포함된 작품들도 함께 결과에 포함되기 때문에 전혀 엉뚱한 작품들이 포함되기도 한다. `레벨업` 이라는 단어를 사전에 고유명사로 등록해서 최대한 엉뚱한 결과가 없게끔 했다.\n그렇다면 전체 작품에 대해 어떻게 원하는 고유명사를 추출할 수 있을까?\n이 고민에 대한 해결책은 기존의 검색용 수집 데이터를 사용했다. 운영자가 작품을 등록할 때 `태그` 를 함께 등록하는데 이는 기존에 검색에서 사용되었던 검색 태그들이다. `김비서가 왜그럴까` 같은 경우엔 `김비서` 나 `김명미` 등의 태그들을 걸어두어서 기존의 검색에서 사용할 수 있는 데이터를 제공했었다. 이 데이터를 약간 손봐서 사용가능한 단어들에 대해 사용자 사전에 등록해서 완벽하진 않지만 신조어나 고유명사들에 대한 대응을 했다.\n데이터가 없는 경우에 뉴스기사나 다른 데이터를 학습해서 토크나이징을 할 수 있는 [soynlp](https://github.com/lovit/soynlp) 같은 훌륭한 오픈소스도 참고할까 싶었지만 개발 시간에 대한 한계도 있었고(핑계지만), 검색 결과에 대해 스스로 보장하기 어렵다는 이유로 도입을 하진 않았다. 나중에 여유가 되면 학습을 통한 토크나이져를 추가해볼까도 고민중인 부분이다.\n추가적으로 구글에서는 [자연어분석](https://cloud.google.com/natural-language/?hl=ko) 서비스를 내놓았다. 어쩌면 가까운 미래에는 한글 형태소 분석기가 없어도 한글 검색이 가능해질지도 모르겠다.\n\n### 결론\n기술적인 이야기를 거의 하지 않았더니 개발일기 처럼 되어 버렸다. 이제 곧 웹에서도 검색기능이 들어갈 예정이니 앞으로는 검색을 좀 더 편하게 할 수 있을것이다. 아직은 좀 더 가다듬고 완성도를 높히는데 목표를 두어야 할 것이다. 약 1년동안 개발했던 내용을 몇문장으로 압축해서 쓰려고 하니 하고 싶은 말은 많은데 모두 다 담지 못한 부분도 있다. 나중에 더 생각 나는 부분들에 대해서 좀 더 내용을 써봐야 겠다.","fields":{"slug":"/elasticsearch-with-kakaopage/"},"frontmatter":{"title":"카카오페이지 검색 개발 후기","published":true}}},{"node":{"rawMarkdownBody":"\n### 썰의 시작\n회사에서 msa 를 눈여겨 보면서 하나로 합쳐져 있는 거대한 프로젝트를 각각 쪼개려는 시도들을 하고 있다. 일에 치여 마음속에서만 맴돌고 있다가 최근에 새로운 기능을 추가해야할 일이 생겨 이때다 싶어 과감히 프로젝트를 쪼개서 작업해보기로 했다.\n기존에 있던 api server 와 internal 호출을 하면서 데이터를 주고받는 작은 api service 이다. view 가 없기 때문에 Restful 하게 만들었다. (~~내가 front 작업이 약해서는 아니다!~~)\n새로 추가된 서비스(`mini api server` 라고 하자)에서만 사용하는 테이블들이 있고, 기존 `api server` 는 `mini api server` 과 통신을 해서 해당 정보들에 대한 CRUD 를 할 수 있다.\n최대한 기존 서비스와는 별도로 작업을 하고자 했고 추후에 물리 디비가 나뉘어도 큰 작업없이 옮겨갈 수 있도록 결합도를 낮추고자 했다.\nuser 에 대한 데이터는 sharding 되어있고 현재는 디비를 같은 곳에 사용하고 있기 때문에 같은 sharding 전략을 가져가야 한다.\n\nannotation 얘기를 해야하는데 갑자기 엉뚱한 소리가 나왔네라고 생각할지도 모른다. 결론적으로 내가 해야할 일은 user 가 어느 shard 번호를 배정받았는지를 알아야 한다. 이 작업을 annotation 을 이용해서 유려하게 처리해도록 하자.\n\n\n\n## 구현목표\ncontroller 에서 `@RequestParam` 으로 받은 값중에 특정값은 데이터가 sharding 처리가 되어 있어 각 repository 에서는 각 분배되어 있는 데이터를 잘 조회하기 위해 해당 디비번호를 설정해주어야 한다. 내가 구현해야할 일을 정리해보자.\n\n* user_uid 값은 sharding 처리가 되어 있다.\n* 사용자는 user 가 어느 디비에 sharding 이 되어있는지 모른다.\n* repository 에서는 각 분배가 되어 있는 디비에 접근을 해야한다.\n\n이런 상황에서의 해결법은 여러가지가 있다. 간단하게 처리할 수도, 복잡하게 처리할 수도 있다.\n하나씩 정리하면서 넘어가도록 하자.\n\n## 개발 컨셉\nshard 번호를 처리를 해야한다고 할 때 두가지 방법으로 처리할 수 있다.\n\n* 사용자가 api 콜을 할 때 shard 번호를 함께 넘겨받는다.\n* shard 번호가 배정되어 있는 디비에 해당 사용자의 번호를 가져온다.\n\n### shard 번호 넘겨받기\n전자의 경우에는 `new api server` 에서 shard 번호를 알고 있을 필요가 없기 때문에 아무런 처리없이 해당 shard 번호를 세팅해주면 된다.\n현재는 물리디비가 함께 있으므로 이렇게 처리해도 큰 문제가 없다. user 데이터가 같은 디비에 있어 shard 번호만 받으면 동일한 번호에 데이터가 있기 때문이다. 하지만 추후에 디비가 나뉘어져서 shard 전략이 다르게 될 때는 문제가 된다. `api server` 에서는 1번 shard 인데 new api server 에서는 2번 shard 번호를 배정받을 수 있기 때문이다. 이런 이유로 후자의 방법으로 방향을 잡게 되었다.\n\n### shard 번호 찾아가기\nshard 번호를 `mini api server` 에서 찾아가는 방법도 여러가지가 있다.\n* 별도의 service 를 만들어 명시적으로 호출해줘 값을 지정한다.\n* 상위 cnotroller 를 만들어 전체 controller 는 부모 controller 를 상속받아 부모에서 처리한다 (기존 `api server` 방식)\n* interceptor, annotation 를 이용해 controller 전처리\n\n1, 2 번의 경우 각 api 호출별로 명시적으로 호출해서 처리할 수 있다. 다만 사용자가 모든 api 에 호출을 해주어야 하는 단점이 있고, 일단 마음에 들지 않는다. user 정보를 조회하는 api 에 모두 같은 로직을 넣어주어야 하기 때문에 코드 중복이 생기고 만약 실수로 코드를 넣지 않았다면 의도치 않은 에러를 발생할 수 있다. 최대한 중복코드를 줄이면서 사용자가 사용하기 편하게 하기 위해 annotation 을 사용해서 처리해보자.\n\n## custom annotation\n최종결과물부터 보면 다음과 같다. `@RequestParam` 을 사용하지 않고 `@RequestCustomParam` 을 이용한다. 이 annotation 을 붙여두면 알아서 shard 정보를 조회해서 DataSource 에 세팅하고 결과값을 Integer 값으로 반환받아 비지니스 로직에서 사용할 수 있다.\n\n`FirstController`\n\n```java\n@RestController\npublic class FirstController {\n\n    @RequestMapping(value = \"/\", method = RequestMethod.GET)\n    public String index(@RequestCustomParam(value = \"uid\") Integer uid) {\n    }\n}\n```\n\n자 이게 가능하게 하기 위해서 하나씩 처리과정을 따라가보자.\n\n```\n.\n├── TeddyApplication.java\n├── annotation\n│   └── RequestCustomParam.java\n├── config\n│   └── InterceptConfig.java\n├── controller\n│   └── FirstController.java\n└── handler\n    └── CustomResolver.java\n\n```\n\n1. `RequestCustomParam` 을 생성\n2. `InterceptConfig` 에서 customResolver 를 등록\n3. `CustomResolver` 에서 `RequestCustomParam` 여부를 검사해 필요한 로직 실행\n4. `FirstController` 에서 필요한 param 값에 `@RequestCustomParam` 로 등록을 하고, 전처리 후 shard 세팅이 된 uid 정보 이용\n\n\n### 하나씩 살펴보자\n하나씩 뜯어보면 그리 어렵지 않다. 대체 어떻게 동작하는 것일까.\n\n`RequestCustomParam`\n\n```java\n@Target({ElementType.PARAMETER})\n@Retention(RetentionPolicy.RUNTIME)\npublic @interface RequestCustomParam {\n\n    String value() default \"\";\n}\n\n```\n\nvalue 만 갖고 있으면 되고 다른 설정들은 일단 사용하지 않아서 `RequestParam` 에서 필요한 값만 가져왔다. 주목해야할 점은 Target 과 Retention 이다. Parameter 타입을 이용해 처리하고 RunTime 까지 annotation 을 가져가서 처리하도록 했다.\n\n`InterceptConfig`\n\n```java\n@Configuration\npublic class InterceptConfig implements WebMvcConfigurer {\n\n    @Override\n    public void addArgumentResolvers(List<HandlerMethodArgumentResolver> resolvers) {\n        resolvers.add(getCustomResolver());\n    }\n\n    @Bean\n    public CustomResolver getCustomResolver() {\n        return new CustomResolver();\n    }\n}\n```\n\n`WebMvcConfigurer` 를 붙여서 내가 생성한 resolver bean 을 등록해준다.\n\n`CustomResolver`\n\n```java\n@Configuration\npublic class CustomResolver implements HandlerMethodArgumentResolver {\n\n    @Override\n    public boolean supportsParameter(MethodParameter parameter) {\n        return parameter.hasParameterAnnotation(RequestCustomParam.class);\n    }\n\n    @Override\n    public Integer resolveArgument(MethodParameter parameter, ModelAndViewContainer mavContainer, NativeWebRequest webRequest, WebDataBinderFactory binderFactory) throws Exception {\n\n        RequestCustomParam customParam = parameter.getParameterAnnotation(RequestCustomParam.class);\n        String value = webRequest.getParameter(customParam.value());\n        // do something\n        return Integer.valueOf(value);\n    }\n}\n```\n\n핵심이 되는 부분이다. `RequestCustomParam` 가 포함되어 있는지 여부를 검사해서 annotation 이 등록되어 있는 request 만 통과를 시킨다. 그리고 사용자가 등록한 value 값을 뽑아와서 필요한 로직처리를 해주면 된다.\n`// do sonething` 부분에서 shard 를 찾아가는 로직을 추가해주도록 하자.\n전처리가 끝나면 controller 에서 uid 값을 이용해 비지니스 로직을 수행해야 하기 때문에 value 로 뽑아온 값을 다시 돌려준다.\n여기서 참고할 부분은 parameter 로 뽑아온 값은 무조건 String 타입으로 반환된다. 각자 필요한 타입으로 변환해서 사용하도록 하자.\n(아마 일반적으로 처리하기 위해 String 으로만 반환받게 한듯 싶다. 여러 타입으로 반환하는 것보다 처리가 간단하고 여러 타입으로 변환하는 작업 자체가 불필요하다고 느꼈을까)\n신경써야할 부분이 있다면 `parameter.getParameterAnnotation`, `webRequest.getParameter` 은 `@Nullable` 이기 때문에 NPE 처리를 해주면 좋다.\n\n이제 필요한 부분에만 `@RequestCustomParam(value = \"uid\") Integer uid` 으로 명시해서 처리하면 사용자는 sharding 에 대한 로직을 신경쓰지 않아도 된다.\n별도의 service 로 나누어 처리하는 것보다 명시적이고 중복코드 없이 간단하게 처리할 수 있다.\n\n\n## 결론\n지금까지 작업한 전체 코드는 [여기](https://github.com/nevercaution/customAnnotation) 에서 확인할 수 있다.\nannotation 을 사용하면 특정 로직들은 뒤에서 처리되기 때문에 관심을 갖지 않고 보면 어떻게 동작하는지 이해하기 힘들다. 반대로 annotation 에 대한 이해가 있으면 코드는 훨씬 간결해지고 직관적으로 처리할 수 있게 된다. 이전까지는 annotation 은 알아서 동작하겠지 하며 큰 신경을 쓰고 있지 않았었는데, 직접 사용해보니 기존의 로직을 좀 더 깔끔하게 처리할 수 있다는 부분이 마음에 든다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","fields":{"slug":"/spring-boot-custom-annotation/"},"frontmatter":{"title":"Spring boot Custom Annotation 사용하기","published":true}}},{"node":{"rawMarkdownBody":"\n### 지금 dev 서버에 배포되어 있는 서버 버전이 몇인가요?\n회사에 서버환경은 dev, sandbox, test, beta, alpha, real 으로 나뉘어 있다. 각 단계별로 기능 테스트를 하고 QA를 통해 모든 기능이 개발 완료 되고 QA도 끝나면 real 서버에 배포가 된다.\n나와 클라이언트 개발자는 먼저 dev 서버에서 새로운 기능이나 버그 수정등을 테스트 하는데 일정에 쫓길 때는 메이져 버전과 마이너 버전이 동시에 작업되서 두개의 버전을 두고 개발해야 하는 상황이 생기기도 한다.\n젠킨스를 통해 해당 서버에 어플리케이션을 배포하는데 지금까지는 배포된 버전을 보기 위해선 젠킨스의 배포이력을 찾아 배포된 버전을 찾았어야 했었다. 여간 귀찮은 일이 아닐 수가 없다. 하지만 이 방법밖에는 없으니 얼른 클라이언트 개발자에게 현재 배포된 버전을 찾아서 알려주자.\n`build.gradle` 에 배포된 버전을 명시해놓고 있긴 하지만 jar로 묶인 패키지에는 버전을 알 수 있는 방법이 없다. 이런 설정정보들을 쉽게 확인할 수 있었으면 좋겠다.\n\n### 지금 필요한 기능과 제약을 정리해보자.\n처음엔 무작정 덤벼들까도 했지만 침착하게 숨을 고르고 내가 해야할 일을 정리해보았다.\n* 배포되는 서버환경별로 어플리케이션의 버전정보를 알고 싶다.\n* 알기 쉽게 endpoint등의 방법으로 버전을 알려주고 싶다.\n* 모듈이 여러개가 있기 때문에 현재 돌아가고 있는 어플리케이션의 이름정보등 여러가지를 알고 싶다.\n\n## 프로젝트의 구조를 살펴보자\nspring-boot 로 구성된 프로젝트는 여러개의 모듈로 구성되어 있다. 각 프로젝트별로 기능들을 모아놓고 있으며 jar로 묶여 runnable한 모듈도 있고 common 이나 utils 같이 기능들을 모아놓은 모듈도 있다.\n프로젝트의 설정은 아래와 같이 구성되어 있다.\n\n```\n├── README.md\n├── api\n│   ├── README.md\n│   ├── build.gradle\n│   └── src\n├── batch\n│   ├── build.gradle\n│   └── src\n├── build.gradle\n├── common\n│   ├── build.gradle\n│   └── src\n├── elasticsearch\n│   ├── build.gradle\n│   └── src\n├── gradle\n│   └── wrapper\n├── gradlew\n├── gradlew.bat\n├── mongodb\n│   ├── build.gradle\n│   └── src\n├── mysql\n│   ├── build\n│   ├── build.gradle\n│   └── src\n├── redis\n│   ├── build.gradle\n│   └── src\n├── settings.gradle\n└── utils\n    ├── build.gradle\n    └── src\n\n```\nrunnable한 모듈들의 `build.gradle` 에는 각자 버전들이 명시되어 있다. `api` 모듈의 `build.gradle`을 살펴보자.\n\n```gradle\napply plugin: 'rebel'\napply plugin: 'org.springframework.boot'\n\next {\n    baseName = 'api'\n    version = '9.2.8-SNAPSHOT'\n}\n\ndependencies {\n    compile project(':common')\n    compile project(':mysql')\n    compile project(':mongodb')\n    compile project(':redis')\n    compile project(':utils')\n    compile project(':elasticsearch')\n\n\t...\n}\n\nbootRun {\n    systemProperties System.properties\n}\n```\n\n`ext` 안에 모듈의 이름과 관리되고 있는 버전의 정보가 들어있다. 어떤 기능이 개발되어 질 때는 버전을 먼저 따고 그 버전에 기능들 개발해 넣게 된다. 클라이언트 개발자들과의 소통은 저 버전으로 하게 된다.\n하지만 빌드를 하면 저 정보는 jar에 따라 오지 않기 때문에 jar파일만 가지고는 버전정보를 알 수가 없다.\n안되는게 어디있는가 이제부터 저 정보들을 제공하는 방법들을 알아보도록 하자.\n\n\n## Spring boot actuator\n\nspring boot 에서 제공하고 있는 공식 모듈중에 [spring boot actuator](https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#production-ready) 라는 모듈이 있다. 이게 무엇인고 하고 살펴보니 아래와 같이 정의가 되어 있다.\n\n```\nDefinition of Actuator\n\nAn actuator is a manufacturing term that refers to a mechanical device for moving or controlling something.\nActuators can generate a large amount of motion from a small change.\n```\n\n여러 정보들을 작은 작업으로 제공해준다니 내가 찾고 있던 것이 틀림없다. 기본적으로 여러 endpoint 들을 제공하고 있고 info 뿐만 아니라 health 나 metrics 등 여러 기능들을 제공하고 있다. 지금 필요한건 버전에 대한 정보들만 표시할 수있으면 되기 때문에 옆길로 새지말고 직진하도록 하자.\n아무런 설정을 하고 않고 빌드를 하게 되면 build.gradle 에 명시되어 있는 버전 정보들은 빠지고 빌드가 되게 된다. actuator 을 추가해보자.\n\n`build.gradle`\n\n```\n\n// 1\next {\n   baseName = 'api'\n   version = '9.2.8-SNAPSHOT'\n}\n\ndependencies {\n\t...\n\t// 2\n\tcompile 'org.springframework.boot:spring-boot-starter-actuator'\n}\n\n// 3\nspringBoot{\n    buildInfo {\n        additionalProperties = [\n                'version': \"${project.ext.version}\",\n                'name': \"${project.ext.baseName}\"\n        ]\n    }\n}\n```\n1. `ext`에는 해당 모듈의 이름과 버전 정보가 들어가게 된다. 클라이언트와 QA가 원하는 값은 저 버전 정보이다.\n2. 모듈을 추가만 해주면 끝이다. 이제 여러 정보들을 endpoint 로 제공받을 수 있다. 하지만 이번 포스팅에서는 다른 기능들을 알아보는건 스킵하도록 한다. (조금 써보았는데 굉장히 많은 정보들을 제공해주고 있다. 나중에 좀 더 깊게 살펴보는게 좋겠다.)\n3. [buildInfo](https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#howto-build-info) 값을 넣어주면 gradle 로 빌드할 때 기본적인 빌드 정보들을 `build-info.properties` 파일 안에 적어주는데 추가적인 정보들을 넣어줄 수 있다.\n\n이제 빌드를 해서 결과값을 살펴보자. `build/resources/main/META-INF/build-info.properties` 안에 buildInfo의 기본적인 값들과 추가적으로 기술해준 정보들이 아래와 같이 들어가있다.\n\n`build-info.properties`\n\n```\n#Properties\n#Sat Mar 24 16:17:58 KST 2018\nbuild.time=2018-03-24T16\\:17\\:58+0900\nbuild.artifact=api\nbuild.group=teddy\nbuild.name=api\nbuild.version=9.2.8\n```\n\n### dev 서버에 배포된 버전은 9.2.8입니다.\n\n값이 잘 들어와있음을 확인했다. 이제 actuator에서 제공하는 endpoint `info`로 이 값들을 확인할 수 있다.\n\n```shell\n$ > curl localhost:8080/info\n{\n\t\"build\":{\n\t\t\"version\":\"9.2.8-SNAPSHOT\",\n\t\t\"artifact\":\"teddy\",\n\t\t\"name\":\"api\",\n\t\t\"group\":\"com.nevercaution\",\n\t\t\"time\":1521877296000\n\t}\n}\n```\n\n설정과 적용이 완료 되었다. 이제 어느 누가 서버의 버전을 묻는다면 버벅이지 말고 자신있게 버전을 알려주도록 하자.\n\n\n## 결론\n요즘 [spring boot reference guide](https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#production-ready) 를 읽고 있는데 개인적인 느낌으로는 [django document](https://docs.djangoproject.com/en/2.0/) 보다 훨씬 상세하고 직관적으로 문서를 작성해놓았다고 생각한다. ~~django문서보면서 추상적인 표현에 적잖이 당황했었다~~\n그리고 spring boot starter 에 많은 기능들이 있어 따로 작업해야 하는 부분이 줄어들었고 적용도 간단히 된다.\n조금 뜬금없는 생각이지만 spring boot 모듈을 한번쯤은 만들어 봐야 겠다는 생각이 들었다.\n\n\n\n\n\n\n\n","fields":{"slug":"/spring-boot-actuator/"},"frontmatter":{"title":"Spring Boot Actuator 를 이용해 버전 정보 제공하기","published":true}}},{"node":{"rawMarkdownBody":"\n### elasticsearch 버전을 올려야 한다.\n검색 서비스를 개선하면서 사내에서 elasticsearch 를 사용하고 있다.\n처음에 사용했던 버전은 개발 당시의 가장 최근 버전인 [elasticsearch 5.1.1](https://www.elastic.co/guide/en/elasticsearch/reference/5.1/index.html) 버전이다.\nspring boot 에 연동했는데 [spring-boot-data-elasticsearch-starter](https://mvnrepository.com/artifact/org.springframework.boot/spring-boot-starter-data-elasticsearch) 는 (글을 작성하는) 아직까지도 2.x 버전만 지원하고 있어 직접 [client](https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.1/client.html) 를 붙이기로 했다.\n\n  개인적인 생각으로는 나중에 개발이 되어질 spring-boot-data-elasticsearch-starter 로 재구현해도 되긴 하겠지만 elasticsearch를 사용하면서 느낀점은 굳이 starter를 사용할 필요는 없다고 생각한다.\nrepository와 model로 나뉘어 orm처럼 사용하면 편하기야 할테지만 직접 쿼리를 만들면서 튜닝하는 부분이 재미있기도 하고 좀 더 세세하게 만질 수 있다고 생각하기 때문이다.\n\nelasticsearch 는 버전업이 빠른 편이다. 6.x 도 꾸준히 올라가는 추세고 곧있으면 7.x 이 나오고 9.x 까지 로드맵이 그려져 있다. 다행히 5.1.x 버전대에서 [rolling upgrade](https://www.elastic.co/guide/en/elasticsearch/reference/5.6/rolling-upgrades.html) 가 가능하기 때문에 5.x 의 마지막 버전인 5.6.8 까지 따라 올라가야 겠다고 생각했다. 그러다가 문득 [State of the official Elasticsearch Java Clients](https://www.elastic.co/blog/state-of-the-official-elasticsearch-java-clients) 라는 포스팅을 보게 되었다. 처음에 java client 를 붙이려면 공식 문서에서는 [transport client](https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.1/transport-client.html) 로 연동하도록 안내한다. 하지만 위의 포스팅의 여러 문제점으로 인해 앞으로는 [Rest client 를 사용하도록 권장하고 있다.](https://www.elastic.co/guide/en/elasticsearch/client/java-api/6.2/client.html)\n\nJava Rest Client 는 두가지 방식이 있다.\n- Java Low Level REST Client\n- Java High Level REST Client\n\nlow level 에서는 요청을 직접 만들어서 호출하는 방식인데, 나는 high level로 작업하기로 했다.\n(참고로 rest client는 5.6부터 제공되었다.)\n\n### 서론이 조금 길었다.\n내가 elasticsearch 버전을 올리기로 생각한 이유는 [elasticsearch 기술 지원](https://www.elastic.co/support/eol) 때문이다.\n내가 사용하고 있는 5.1.x 는 2018-06-08 까지만 공식지원을 하고 있다.\n물론 공식 지원이 끊기더라도 검색 서비스를 구동함에는 큰 지장은 없지만, 메이져 버전이 두개 이상 차이가 나게 되었을 때 버전을 올려야 하는 상황이 오면 rolling upgrade도  사용할 수 없기 때문에 이참에 6.2 로 올라가기로 마음먹었다. 현재 버전에서 한번에 올라가는건 안되기 때문에 다음과 같이 버전올림 순서를 정하기로 했다.\n\n## 업데이트 순서\n1. elasticsearch 5.1.1 -> 5.6.8 로 rolling upgrade\n2. spring-boot elasticsearch java client 5.1.1 -> 5.6.8 로 업데이트 후 배포\n3. elasticsearch 5.6.8 -> 6.2.2 로 [rolling upgrade](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-upgrade.html)\n4. spring-boot elasticsearch java client 5.6.8 -> 6.2.2 로 업데이트 후 배포\n\n[메이져 버전별로의 호환성](https://www.elastic.co/guide/en/elasticsearch/client/java-rest/5.6/java-rest-high-compatibility.html)에 따라 5.x 버전끼리는 문제없이 통신을 할 수 있다. 물론 6.x 끼리의 버전도 문제가 없었는데, 테스트 결과 5.6.8 에서 6.2.2 도 호출이 가능했다.\n클라이언트를 보니 모든 기능을 사용할 수 있지는 않고 부분적인 기능들만 사용가능할 것으로 보인다. 그리고 상위 버전 호환은 괜찮아도 하위 버전 호환은 기능이 구현되지 않을 가능성이 있어 문제의 여지가 있다.\n\n### rolling upgrade\n현재 사용하고 있는 rolling upgrade 스크립트이다. 구글링 해보니 좀 더 유려한 스크립트 들이 많이 있는데 나는 간단한 동작들만 사용하는 중이다.\n\n일단 구동중인 es를 내린다.\n`shut_down.sh`\n\n```bash\n#!/usr/bin/env bash\n\n# disable shard allocation\ncurl -XPUT 'localhost:9200/_cluster/settings?pretty=true' -d '{\n    \"transient\" : {\n        \"cluster.routing.allocation.enable\" : \"none\"\n    }\n}'\n\n# shutdown\nsudo service elasticsearch stop\n```\n\n정상적으로 내려간 것을 확인한 후 elasticsearch 버전을 올려 재설치하도록 한다. 그리고 다시 구동시켜 주자.\n\n`start.sh`\n\n```bash\n#!/usr/bin/env bash\n\nsudo service elasticsearch start\n\nSTATUS=\"\"\nwhile ! [[ \"$STATUS\" =~ (\\\"tagline\\\" : \\\"You Know, for Search\\\") ]];\ndo\n    echo \"fetching http://localhost:9200\"\n    STATUS=`curl -sS -XGET http://localhost:9200`\n    sleep 1\ndone\n\ncurl -XPUT 'localhost:9200/_cluster/settings?pretty=true' -d '{\n    \"transient\" : {\n        \"cluster.routing.allocation.enable\" : \"all\"\n    }\n}'\n```\n자 이제 버전도 올렸으니 client 를 변경해보도록 하자.\n\n## client 에서 rest client\n위에서 설명한 것과 같이 앞으로는 rest client 를 써야만 하는 시점이 온다.\n지금 당장은 바꾸지 않아도 되지만 (완성이 되지 않아 할 수도 없지만) 어떻게 바뀌는지 느낌만 살펴보자.\n기존에 transport layer로 붙는 client를 rest client로 변경하는 작업을 해보자. 일단 설정부터 변경해야 한다.\n\n### setting\n`build.gradle`\n\n```gradle\ndependencies {\n    compile 'org.elasticsearch:elasticsearch:6.2.2'\n    compile 'org.elasticsearch.client:elasticsearch-rest-high-level-client:6.2.2'\n}\n```\n\n그리고 기존에 붙었던 `9300`포트가 아닌 `9200`포트로 접속해야 한다.\n`application.yml`\n\n```yml\nelasticsearch:\n  hosts: host1.com, host2.com\n  port: 9200\n```\n\n### configuration\n설정파일을 고쳤으면 configuration쪽을 손봐주도록 하자.\n기존의 config와 비교를 해보면 다음과 같다.\n\n`기존 ElascitsearchConfig.java`\n\n```java\n@Configuration\npublic class ElasticsearchConfig {\n\n    @Value(\"#{'${elasticsearch.hosts}'.split(',')}\")\n    private List<String> hosts;\n\n    @Value(\"${elasticsearch.port}\")\n    private int port;\n\n    @Bean\n    public Client client() throws Exception {\n        final Settings settings = Settings.builder()\n                .put(\"client.transport.sniff\", true)\n                .build();\n\n        PreBuiltTransportClient client = new PreBuiltTransportClient(settings);\n        for(String host : hosts) {\n            InetSocketTransportAddress item = new InetSocketTransportAddress(InetAddress.getByName(host), port);\n            client.addTransportAddresses(item);\n        }\n\n        return client;\n    }\n}\n```\n\n`변경된 ElascitsearchConfig.java`\n\n```java\n@Configuration\npublic class ElasticsearchConfig {\n    @Value(\"#{'${elasticsearch.hosts}'.split(',')}\")\n    private List<String> hosts;\n\n    @Value(\"${elasticsearch.port}\")\n    private int port;\n\n    @Bean\n    public RestHighLevelClient getRestClient() {\n\n        List<HttpHost> hostList = new ArrayList<>();\n        for(String host : hosts) {\n            hostList.add(new HttpHost(host, port, \"http\"));\n        }\n\n        RestClientBuilder builder = RestClient.builder(hostList.toArray(new HttpHost[hostList.size()]));\n        return new RestHighLevelClient(builder);\n    }\n}\n```\n\n### service\n\n약간씩 달라진 부분들을 살펴보자. 큰틀은 변경되진 않았고 호출하는 클래스나 메소드들이 조금씩 변경되었다.\n개인 적인 느낌으로는 각 클래스별로 역할이 좀 더 충실해졌다고 생각이 들었다.\n\n`create`\n\n```java\n// before\nClient client;\nIndicesAdminClient adminClient = client.admin().indices();\nCreateIndexResponse createIndexResponse = adminClient.prepareCreate(\"index_name\")\n            .setSettings(seriesSettings())\n            .addMapping(\"type name\", seriesIndex()).get();\n\n// after\nRestHighLevelClient client;\nCreateIndexRequest request = new CreateIndexRequest(\"index_name\");\nrequest.settings(seriesSettings(), XContentType.JSON);\nrequest.mapping(\"type_name\", seriesIndex(), XContentType.JSON);\nclient.indices().create(request);\n```\n\n`search query`\n\n```java\n// common\nQueryBuilder qb = QueryBuilders.matchQuery(\"text\", text);\n\n// before\nClient client;\nSearchResponse response = client.prepareSearch(\"index_name\").setTypes(\"type_name\").setQuery(qb).get();\n\n// after\nRestHighLevelClient client;\nSearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder().query(qb);\nSearchRequest searchRequest = new SearchRequest(\"index_name\").types(\"type_name\").source(searchSourceBuilder);\nSearchResponse response = client.search(searchRequest);\n\n// common\nresponse.getHits().forEach(item -> {\n\t// do something\n});\n```\n\n간단하게 index 를 만들고 검색하는 부분까지의 메소드들을 보았다. 나는 여기에 추가로 alias, exists, multi_search 등을 사용하고 있지만 아직까지 6.2.x 에는 해당 메소드가 없다. 어떻게든 해보려고 엔진소스를 들어가서 한참을 살펴보다가 혹시나 해서 master 브랜치를 받아보니 해당 메소드들이 있더라.. (현재 master 브랜치의 버전은 7.0.0-alpha1 이다.)\n\n\n## 결론\n미리 적용을 해놓을까 해서 버전별로 소스를 살펴 보았지만 아직까지 모든 메소드들이 구현되어 있지 않기도 하고 [java client가 없어지려면 8.0 까지 올라가야 하니](https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.6/client.html) 아직은 조금 여유가 있어보인다.\n그래도 버전이 올라가는 속도를 보면 7.0 메이져 버전이 나오고 rest client가 완성이 되었을 때 슬슬 적용하면 될 것 같다. ~~세수하러 왔다가 물만 먹고 가는 느낌~~\n\n\n\n\n\n\n","fields":{"slug":"/elasticsearch-rest-client/"},"frontmatter":{"title":"Spring-boot 에서 Elasticsearch java rest client 사용하기","published":true}}},{"node":{"rawMarkdownBody":"\n### view를 추가해야한다.\n\nspring 을 사용하다가 spring boot 로 넘어오면서 front, back 을 나누어서 백단은 나름 Restful 하게 해서 api 콜만 처리하는 방식으로 변경하는 중이다.(front 는 react로 구성하는 중이다.) 그래서 spring boot 에서는 따로 view 처리해야할 일이 없었는데 기존에 spring 에서 view 처리를 해주는 요청을 가져와야 할일이 생겼다.\n하지만 찾아보니 기존에 spring 에서 하던 방법으로는 안될 것 같다.\n왜냐하면 [spring boot 에서는 jar 로 사용할 때는 jsp를 사용할 수 없다고 한다.](https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-developing-web-applications.html#boot-features-jsp-limitations)\n내용을 읽어보니 boot에 내장 tomcat에 하드코딩 패턴때문에 jar형식으로는 webapp내용을 가져올 수 없다고 한다. 그리고 공식적으로 jsp를 지원하지 않는다고 한다. boot에서 밀고 있는 template engine 들이 여러개 있었는데 간단한 view 하나 추가하는데 공수가 많이 들게되면 좋지 않을꺼라 생각해서 jsp로 view 를 구성하는 방법을 시도해보았다.\n일단 작업을 시작하기 전에 현재 사용하고 있는 버전들을 정리하고 간다.\n\n### 사용하고 있는 버전은 다음과 같다.\n* spring boot 1.5.7\n* gradle 4.4\n\n### 나중에는 없어질 view 이지만\nfront작업이 react로 완료되면 이 view 는 더이상 필요하지 않다.\n그래서 나는 최소한의 공수로 기존에 있는 jsp 파일을 사용하여 가볍게 포팅만 하고자 했다.\n\n---\n\n## 1차 시도\nspring boot 에서 jsp view를 사용하기 위해 spring에서 구성하는 방법과 추가적으로 필요한 설정들을 해주었다.\nspring boot 의 내장 tomcat에는 jsp parser가 없기 때문에 의존 패키지를 추가해주어야 한다.\n\n- build.gradle\n\n```bash\nderendencies {\n    compile('javax.servlet:jstl')\n    compile(\"org.apache.tomcat.embed:tomcat-embed-jasper\")\n}\n```\n\n그리고 구조는 아래와 같이 구성했다. main밑에 webapp폴더를 추가해서 jsp파일을 추가해준다.\n\n```bash\n.\n├── build.gradle\n├── gradlew\n├── gradlew.bat\n└── src\n    ├── main\n    │   ├── java\n    │   │   └── com\n    │   │       └── example\n    │   │           └── demo\n    │   │               ├── DemoApplication.java\n    │   │               └── MyController.java\n    │   ├── resources\n    │   │   └── application.properties\n    │   └── webapp\n    │       └── WEB-INF\n    │           └── jsp\n    │               └── index.jsp\n    └── test\n```\n\nspring boot 는 webapp의 위치를 모르기 때문에 설정파일에 경로를 명시해주어야 한다.\n* application.properties\n\n```bash\nspring.mvc.view.prefix=/WEB-INF/jsp/\nspring.mvc.view.suffix=.jsp\n```\n\n설정은 다했다. 이제 controller에서 view를 호출해보자.\n* MyController.java\n\n```java\npackage com.example.demo;\n\nimport org.springframework.stereotype.Controller;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.servlet.ModelAndView;\n\n@Controller\npublic class MyController {\n\n    @RequestMapping(value = \"/\")\n    public ModelAndView main() {\n        ModelAndView view = new ModelAndView(\"index\");\n        view.addObject(\"text\", \"world\");\n        return view;\n    }\n}\n```\n설정파일에서 prefix, suffix를 적어주었기 때문에 view이름은 파일이름만 넣어주면 된다.\n\n* index.jsp\n\n```html\n<html>\n    <body>\n        <h1>Hello world</h1>\n        hello ${text}\n    </body>\n</html>\n```\n이제 bootRun 을 하면 build 를 하고 테스트를 해볼 수 있다.\n\n```bash\n$ ./gradlew clean bootRun\n:compileJava\n:processResources\n:classes\n:findMainClass\n:bootRun\n```\n\n* localhost:8080/\n```html\nHello world\n\nhello world\n```\n\nbuild 명령어로 jar파일을 만들어 보자.\n```sh\n$ ./gradlew clean build\n:compileJava\n:processResources\n:classes\n:findMainClass\n:jar\n:bootRepackage\n:assemble\n:compileTestJava\n:processTestResources NO-SOURCE\n:testClasses\n:test\n:check\n:build\n\nBUILD SUCCESSFUL\n\nTotal time: 1.494 secs\n```\n\n`./build/libs/testGradle-0.0.1-SNAPSHOT.jar` 에 jar가 만들어졌다. 이걸로 직접 띄워서 호출해보자.\n\n```bash\n$ java -jar build/libs/testGradle-0.0.1-SNAPSHOT.jar\n```\n잘 뜨는 것을 확인 할 수 있다. 그런데 더이상 boot진영에서도 jsp파일을 그대로 쓰는걸 권장하고 있지 않으니 그냥 추천해주는 template engine로 넘어가야겠다는 생각이 들었다.\n\n## 2차 시도\n찾아보니 정말 [여러가지 template engine](\"http://www.baeldung.com/spring-template-engines\")들이 있었다. 곰곰히 찾아보다 간단해 보이는 [Free Marker](\"https://freemarker.apache.org/\")를 써보기로 했다.\n설정을 추가해주는 것도 간단하다. 추가를 해보자.\n\n* build.gradle\n\n```bash\ndependencies {\n    compile('org.springframework.boot:spring-boot-starter-freemarker')\n}\n```\n\n그리고 jsp로 인한 설정들을 모두 제거해준다.\n* application.properties\n\n```bash\n#spring.mvc.view.prefix=/WEB-INF/jsp/\n#spring.mvc.view.suffix=.jsp\n```\n\n그리고 free marker 의 확장자는 `.ftl` 이다. 기본적인 파일 위치는 `resources/templates/` 이다.\n이에 따라 파일명을 수정해주고 이동까지 하면 아래와 같이 된다.\n\n```bash\n.\n├── build.gradle\n├── gradle\n│   └── wrapper\n│       ├── gradle-wrapper.jar\n│       └── gradle-wrapper.properties\n├── gradlew\n├── gradlew.bat\n└── src\n    ├── main\n    │   ├── java\n    │   │   └── com\n    │   │       └── example\n    │   │           └── demo\n    │   │               ├── DemoApplication.java\n    │   │               └── MyController.java\n    │   └── resources\n    │       ├── application.properties\n    │       └── templates\n    │           └── index.ftl\n    └── test\n        └── java\n            └── com\n                └── example\n                    └── demo\n                        └── DemoApplicationTests.java\n```\n\n위와 같이 설정하고 빌드를 하고 jar파일을 띄워주면 1차시도와 동일한 결과가 나온다.\n처음에는 jsp파일을 가지고 어떻게든 띄워보려고 노력했지만 다른 template engine을 보니 복잡한 jsp파일이 아니라면 굳이 사용하지 않아도 될거라는 생각이 들었다.\n\n## 결론\n아무래도 이제는 spring boot를 사용하면서 jsp를 사용하기는 어려울듯 싶다. 나도 결국에는 다른 template engine을 사용했는데 설정부터 적용이 너무 편해진 느낌.\n각 template engine마다 문법이 조금 달라서 개인의 기호에 맞게 써야겠다만..(예전에 node를 할 때 [jade](\"https://www.npmjs.com/package/jade\")같은 경우엔 적잖은 충격을 받았었다.)\n아무래도 아직까지는 html친화적인 문법이 조금은 더 익숙한 느낌이다.","fields":{"slug":"/spring-boot-jsp/"},"frontmatter":{"title":"spring boot 에서 jsp view 만들기 (feat freemarker)","published":true}}},{"node":{"rawMarkdownBody":"필요에 의해서 배치 작업을 만들어야 했다.\n간단하게 python으로 만들까 했는데, 단순 cursor로 사용하지 않고 django orm 으로 만들어 보고 싶은 생각이 들었다.\n이미 작업하고 있는 django project 가 있었고, 그 안에서 사용하는 모델들을 사용해서 스크립트들을 만들면 좋겠다 생각이 들었다.\n이미 django 구조에 익숙한 사람이라면 편하게 사용할 수 있을거라 생각했고, 관리가 용이하다는 장점이 있다.\n\n\n### 해야할 일은..\n- mysql, mongo에서 데이터를 주기적으로 가져온다.\n- 데이터를 가공해서 다시 mysql이나 mongo로 데이터를 주기적으로 쌓아준다.\n- 여러 서버의 상황에 맞게 대응을 해야한다.(real, sandbox, test등등)\n\n이미 사내에서 사용하고 있는 훌륭한 배치 스크립트 뭉치가 있다. 이 스크립트는 cursor를 통해서 mysql 데이터를 가져오고 넣는다.\n물론 cursor로 쿼리를 작성해도 되지만 django-orm을 이용해서 이 작업을 조금 더 수월하게 할 수는 없을까? 하는 생각에 django template 를 사용하지 않고 orm 만 사용할 수 있는 프로젝트를 만들어보기로 했다.\n\n### django-orm standalone으로 가자\n\n일단 간단한 django 프로젝트를 하나만든다.\n\n```\n$ django-admin startproject django-orm\n```\n\n설정과 최소한의 파일들을 제외하고 모두 제거해버리자. 구조는 아래와 같다.\n\n~~~\n.\n├── Dockerfile\n├── build.sh\n├── db\n│   ├── __init__.py\n│   └── models.py\n├── manage.py\n├── requirements.txt\n├── run.sh\n├── scripts\n│   ├── __init__.py\n│   └── test.py\n└── settings.py\n~~~\n\n1. requirements.txt : 스크립트를 수행할 떄 필요한 모듈등을 기술해준다.\n2. manage.py : django-extension의 runscript 나 shell 기능을 사용할 수 있도록 한다.\n3. settings.py : 여러 저장소의 접속 환경이나 환경변수들을 선언해준다.\n\n위의 구조를 뼈대로 필요한 부분을 채워서 사용하기로 한다. requirements.txt를 살펴보자.\n\n- requirements.txt\n\n~~~\nDjango==1.11.2\ndjango-extensions==1.7.9\nmysqlclient==1.3.10\npytz==2017.2\nsix==1.10.0\n~~~\n\n[django-extension](https://github.com/django-extensions/django-extensions) 을 사용해서 스크립트들을 돌리고, mysql 의 데이터를 사용하기 위해 둘다 설치해주었다. 각자 필요한 부분이 있다면 추가해주면 된다.\n이번에는 설정파일을 보도록 하자.\n\n- settings.py\n\n~~~\nimport os\n\n# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.mysql',\n        'NAME': 'test',\n        'USER': 'root',\n        'PASSWORD': '',\n        'HOST': '127.0.0.1',\n        'PORT': '3306'\n    },\n}\n\nINSTALLED_APPS = (\n    'db',\n    'django_extensions',\n)\n\n# SECURITY WARNING: Modify this secret key if using in production!\nSECRET_KEY = '{your_secret_key}'\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'Asia/Seoul'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n~~~\n\n기본 설정파일에서 DATABASES 부분만 수정해 준 상태이고 model들이 KST로 시간을 붙어여 했기 때문에 타임존 설정을 서울로 바꾸어 주었다.\n여기에서 로그에 대한 설정이나 다른 저장소에 대한 정보를 적어주면 된다.\n\n구조를 잡았으니 추가해야할 패키지들을 설치해주도록 한다. 여기서부터는 익히 알고 있는 패턴이므로 간단하게 살펴보자.\n\n```\n$ pip install -r requiremwnts.txt\n```\n\n모델은 현재 자신의 db에 있는 테이블을 정의해주고 orm으로 붙기만 하면 된다. 모델을 한번 보자.\n\n- model.py\n\n~~~\nfrom django.db import models\n\n# Create your models here.\n\n\nclass User(models.Model):\n    id = models.IntegerField(primary_key=True)\n    name = models.CharField(max_length=11)\n\n    class Meta:\n        managed = False\n        db_table = 'user'\n\n~~~\n\ndb 설정을 해주었고 필요한 패키지를 설치했고, 모델들을 정의해주었다면 한번 붙어보자.\n\n### hello orm\n\npython shell 로 들어가서 모델을 통해 데이터를 가져오면 된다.\n\n~~~\n$ python manage.py shell\n~~~\n\n~~~\n>>> from db.models import User\n>>> User.objects.all()\n<QuerySet [<User: User object>, <User: User object>, <User: User object>, <User: User object>, <User: User object>]>\n~~~\n\n모델을 통해 데이터를 가져올 수 있고, 작업해야할 부분은 스크립트를 만들어서 사용하도록 한다. 기본적인 스크립트를 하나 만들어보자.\n\n- test.py\n\n~~~\nfrom db.models import User\n\n\ndef run(*script_args):\n\n\tuser_list = User.objects.all()\n\n\tfor user in user_list:\n\t\tprint('name : ', user.name)\n~~~\n\n간단하다. 스크립트를 만들었으니 수행해보자. runscript 는 위에서도 언급했지만 django-extension의 기능중 하나이다.\n\n~~~\n$ python manage.py runscript test\nname :  teddy\nname :  canel\nname :  twght\n~~~\n\n이제 필요한 스크립트를 만들어서 crontab에 걸어두면 django orm 을 이용해서 스크립트를 사용할 수 있다.\n여기까지 했으면 간단하게 끝날텐데, 이 배치 스크립트를 여러 환경에 배포하고 돌려야 하는 일이 남아있었다.\n물론 서버가 많지 않아 git으로 땡기거나 손으로 직접 옮겨도 되지만, 칼을 뽑은 김에 docker image 로 만들어서 써보자. 어렵지 않다.\n\nDocker iamge 로 만드는 작업은 완전 단순하다. 사용법도 전혀 어렵지 않다. 일단 Dockerfile을 보자.\n\n- Dockerfile\n\n~~~\nFROM python:3.5-onbuild\n~~~\n\n사족으로 [pythpn:3.5-onbuild](https://github.com/docker-library/python/blob/9a9021f2134d953165b31d98cacb95aa34076f90/3.5/onbuild/Dockerfile) 에서 하는 일은 아래와 같다.\n\n~~~\n#\n# NOTE: THIS DOCKERFILE IS GENERATED VIA \"update.sh\"\n#\n# PLEASE DO NOT EDIT IT DIRECTLY.\n#\n\nFROM python:3.5\n\nRUN mkdir -p /usr/src/app\nWORKDIR /usr/src/app\n\nONBUILD COPY requirements.txt /usr/src/app/\nONBUILD RUN pip install --no-cache-dir -r requirements.txt\n\nONBUILD COPY . /usr/src/app\n~~~\n\n이렇게만 써주고 build 하면 끝이다.\n\n~~~\n$ docker build --tag django-orm:0.1 .\n~~~\n\n이미지를 만들었다면 다음과 같이 생성이 된다.\n\n~~~\n$ docker images\nREPOSITORY            TAG                 IMAGE ID            CREATED             SIZE\ndjango-orm            0.1                 c8a6c10c8233        About an hour ago   715 MB\n~~~\n\n이 이미지를 사용하기 위해서는 run 명령으로 사용하도록 하자.\n\n~~~\ndocker run --name django-orm django-orm python manage.py runscript test\n~~~\n\n### docker로 사용할 때 신경써야할 부분이 있다면\ncontainer 내부에서는 host에 바로 접근할 수가 없기 때문에 mysql localhost 를 바라봐야 한다면 localhost 로 명시해주면 안된다.\n물론 회피 방법은 조금만 구글링해도 나오지만 원칙적으로는 접근을 할 수 없으므로 이 부분을 신경써서 작업해주어야 한다.\n~~나는 mysql이 다른 서버에 동작하고 있었기 떄문에 별 문제는 없었다.~~\n\n\n### 결론\n이미 django project 를 구동해본 경험이 있다면 큰 어려움 없이 사용할 수 있을 것이다.\n물론 해당 프로젝트에서 배치 스크립트를 작성할 수도 있지만, 성격이 맞지 않을 수 있기 때문에 굳이 억지로 넣을 필요는 없다.\ncursor로 작업하는게 간단하고 편할 수도 있지만, 여러 패키지를 함께 써야하고 여러 db 설정을 바라봐야 한다면 django 의 골격을 그대로 사용하는 방법이 손쉬울 수 있겠다.\n해당 예제는 [django-orm standalone](https://github.com/nevercaution/django-orm) 에서 확인할 수 있다.","fields":{"slug":"/django-orm-standalone/"},"frontmatter":{"title":"Django orm standalone feat(docker)","published":true}}},{"node":{"rawMarkdownBody":"\n회사에서 필요에 의해 [Single Sign On](https://ko.wikipedia.org/wiki/%ED%86%B5%ED%95%A9_%EC%9D%B8%EC%A6%9D, \"Single Sign On\")을 구현해야 할 일이 있었다. 3개의 독립된 사이트가 있었고, 각자 로그인을 따로 해야 한다는 점이 사용자 입장에서는 번거로운 일이 될 수 있다는 이유에서 였다.\n\n 3개의 사이트는 하나의 유저테이블을 바라보고 있었기 때문에 충분히 합당한 이유였다. 하지만 django에서 제공하는 기본 유저 테이블을 사용하지 않기 때문에 찾아봤었던 다른 인증 툴들은 그대로 사용하기 까다로워 보였다. 내가 구현해야 하는 상황에는 몇가지 제약사항이 있었다.\n\n1. 로그인을 시도할 때 기존에 제공되는 auth_user 테이블이 아닌 <U>별도의 user 테이블을 사용하고 있다.</U>\n\n2. 기존에 제공되는 user 모델을 사용하지 않기 때문에 <U>reatframework 에서 기존에 제공하는 인증방식을 그대로 사용할 수 없었다.</U>\n\n3. 이미 테이블이 존재하고 <U>django 프로젝트에서 migration 을 할 수 없는 상황이다.</U> 기존에 생성되어 있는 테이블들의 구성을 변경할 수는 있지만 그 과정과 절차가 까다롭고 (회사의 사정에 의해), 외래키를 사용하지 않았기 때문에 모델과 테이블간의 100% 싱크가 맞지 않는 상황이었다.\n\n4. 위와 같은 이유로 찾아봤었던 [MamaCAS](https://github.com/jbittel/django-mama-cas, \"MamaCas\") 같은 외부 인증툴을 사용할 수 없었다. (migration을 할 수 없었기에 인증을 위해 별도의 테이블을 생성하기 까다로운 상황이었다.)\n\n3개의 프로젝트들이 출시가 얼마 남지 않은 상황이고, 인증을 위해서 별도의 커다란 작업이 있는 것은 별로 좋은 선택이 아니였다. 물론 외부의 다른 툴들을 이용할 수도 있었지만, 단순 인증을 위해 외부툴을 사용하기엔 부담스러운 부분이 있었고, 볼륨을 크게 잡고 싶지 않았다.\n\n여러 방법을 찾아보면서 몇가지 아이디어가 나왔었다.\n\n1. [redis](https://redis.io/, \"redis\") 를 이용해서 인증 정보를 저장하는 방식\n\n2. [jwt](https://jwt.io/, \"jwt\")를 이용해서 javascript 내부에서 localStorage 에 인증 정보를 저장하는 방식\n\n3. cookie에 저장하는 방식\n\n나는 일단 1번의 redis는 제외하기로 했다. 이미 각각의 프로젝트에서 redis를 사용하고 있었고, 인증을 위해 별도의 redis를 띄우는게 번거롭다고 생각했다. 그리고 2번의 localStorage 방식은 javascript에서 ajax통신을 각 페이지별로 날려 유효한 인증을 판단하는 방식이었는데, django 단에서 각 request마다 헤더에 인증 정보를 넣기가 까다로웠다. (하지만 jwt로 토큰을 암호화 하는건 괜찮은 방법이라 생각했다.) 3번의 cookie 방식은 별도의 작업 없이 구현이 가능했지만 3개의 사이트가 같은 메인 도메인 하에 있어야 가능했었다. 어차피 모든 사이트가 회사 내부에서 사용할 목적으로 만들어 지고 있는 프로젝트 였고 같은 메인 도메인만 사용하면 되었기 때문에 나는 3번째의 방법을 사용하기로 했다.\n\n서론이 길었지만.. 그리하여 jwt와 cookie를 이용해서 sso를 구현해 보기로 했다.\n\n여러 사이트가 있고 각각 로그인(유저 인증)이 필요하고 기능 마다 해당 유저가 접근할 수 있는(허가) 기능이 필요하다.\n\n(single sign on 을 하다보니 authentication 과 permission 처리가 필요했다..)\n\n여러 개의 사이트가 공유하는 유저 정보가 있고, 이 정보로 로그인을 하기 때문에 모든 사이트에서 한번의 로그인을 하면 되는 기능이 필요했다.추가적으로 각 사이트 별로는 각자의 허가 정보가 있으므로 이 정보는 공유하지 않고 각자 구현한다. (템플릿만 제공)\n\n한마디로 요약하면 쿠키에 암호화된 토큰 정보를 도메인이 같은 사이트들끼리 로그인 정보를 공유를 해서 사용하는 방법이다.\n\n## 예상 시나리오\n\nalpha.mysite.com, beta.mysite.com, charlie.mysite.com 3개의 사이트가 있다.\nbeta 에서 로그인을 하고 charlie 사이트에 접속할 경우 이미 로그인이 되어 있다.\n3개의 사이트는 각자 permission 이 따로 존재한다.\nalpha 에서 로그아웃을 할 경우, 나머지 사이트에서도 모두 로그아웃이 된다.\n로그인 정보가 만료되었을 경우에도 로그아웃 처리된다.\n\n## 시나리오 순서 별 설명\n\n1. 로그인 시 : <U>jwt token</U> 을 발급해서 request cookie에 저장한다. (이 때, <U>cookie</U> 의 domain은 main domain 값을 넣어준다.<mysite.com>)\n2. 인증이 필요한 페이지에 접속할 경우\nrest framework 의 <U>authentication</U> class 에서 cookie 에 저장되어 있는 token 값의 유효성을 검사한다.\nrest framework 의 <U>permission</U> class 에서 해당 유저가 기능에 접근 허가 여부를 판단한다.\n3. 로그아웃 시 : cookie 의 token 값을 지워준다.\n4. 추가 정보\n\t* 인증에 실패한 경우 : cookie 값을 삭제하고 login 페이지로 보낸다.\n\t* 기능 접근 허가가 거부된 경우 : block page 를 표시한다.\n\t* 토큰이 만료된 경우 : cookie 값을 삭제하고 login 페이지로 보낸다.\n5. 제약사항\n\t* 메인 도메인이 동일해야한다.\n\t* permission 정보는 공유하지 않기 때문에 각자 사이트에서 구현해야 한다.\n\n## 환경 설정\n\n```\npython 3.4.3\ndjango 1.8.4\ndjangorestframework 3.3.0\n```\n\n하나씩 시작해보자.\n\n### 패키지 추가하기\n\n```\n$ pip install djangorestframework-jwt\n```\n\n### 로그인 하기\n\n#### 토큰 생성하기\n\n유저 정보가 맞을 경우 이 정보를 토대로 jwt token을 생성해준다.\njwt_util.py\n\n```\nfrom calendar import timegm\nfrom datetime import datetime\n\nimport jwt\nfrom rest_framework_jwt.settings import api_settings\n\nfrom user.models.CustomUser import CustomUser\n\n\ndef obtain_token(user: CustomUser):\n    jwt_payload_handler = api_settings.JWT_PAYLOAD_HANDLER\n    jwt_encode_handler = api_settings.JWT_ENCODE_HANDLER\n\n    payload = jwt_payload_handler(user)\n\n    # Include original issued at time for a brand new token,\n    # to allow token refresh\n    if api_settings.JWT_ALLOW_REFRESH:\n        payload['orig_iat'] = timegm(\n            datetime.utcnow().utctimetuple()\n        )\n\n    return jwt_encode_handler(payload)\n```\n\njwt 변수들에 대한 자세한 설정은 Rest Framework JWT 에서 확인 할 수 있다. 그리고 default 로 설정된 변수들을 재선언 하고 싶다면 setting 에 해주면 된다.\n\nsettings.py\n\n```\nJWT_AUTH = {\n    'JWT_EXPIRATION_DELTA': datetime.timedelta(hours=24),\n}\n```\n\n#### 쿠키에 저장하기\n\n사용자가 입력한 아이디와 암호가 맞을 경우 이 정보를 cookie 에 저장을 한다. 저장할 때 domain option 을 main domain(mysite.com) 로 설정해준다. domain 설정을 해주지 않을 경우 다른 사이트에서 쿠키정보를 공유할 수 없다.\n\n\nview.py\n\n```\nclass LoginView(View):\n\n    def get(self, request):\n        return TemplateResponse(request, 'account/login.html')\n\n    def post(self, request):\n        username = request.POST.get('username')\n        password = request.POST.get('password')\n\n        try:\n            admin = CustomUser.objects.get(username=username)\n            # 유저 암호 검사하기.\n            if security.matches_password(password, admin.password):\n                # obtain jwt token\n                token = obtain_token(admin)\n\n                # set cookie\n                response = HttpResponseRedirect(reverse('root'))\n                response.set_cookie(key='token', value=token, domain=settings.COOKIE_DOMAIN)\n                return response\n            else:\n                context = {\n                    'error': '비밀번호가 일치하지 않습니다.'\n                }\n        except TAdmin.DoesNotExist:\n            context = {\n                'error': '존재하지 않는 ID입니다.'\n            }\n\n        return TemplateResponse(request, 'account/login.html', context)\n}\n```\n\n공통으로 사용할 메인 도메인 이름은 각 사이트 마다 통일 해줘야 한다.\n\nsettings.py\n\n```\nCOOKIE_DOMAIN = 'mysite.com'\n```\n\n쿠키에 토큰 값을 저장 했으므로 이제 각 사이트에 로그인 되었다고 할 수 있는 첫번 째 단계는 달성했다.\n\n#### 페이지 인증정보 확인하기\n로그인이 되어 있어야 접근할 수 있는 페이지의 경우 로그인 정보를 확인한 후에 정보를 로드한다. RestFramework 에서 제공하는 APIView 를 사용해서 인증 정보를 확인한다.\n\nview.py\n\n```\nclass RootView(APIView):\n    authentication_classes = (CustomAuthentication, )\n    def get(self, request):\n        \"\"\"\n        :param request:\n        :return:\n        \"\"\"\n        redirect_url = \"/home\"\n        return HttpResponseRedirect(redirect_url)\n\n}\n```\n\n 여기서 authentication class 를 명시적으로 선언해주었는데, 모든 APIView(인증과 허가가 필요한 페이지의 경우) 에 적용될 수 있도록 settings 에 선언해줄 수 있다.\n\n(인증 실패나 허가 요청 실패시 처리해줄 수 있는 핸들러도 미리 추가해놓자.)\n\nsettings.py\n\n```\nREST_FRAMEWORK = {\n    'DEFAULT_AUTHENTICATION_CLASSES': (\n        'common.utils.authentication.CustomAuthentication',\n    ),\n    'EXCEPTION_HANDLER': 'common.utils.custom_exception_handler.custom_exception_handler'\n}\n```\n\n인증 확인은 쿠키에 저장되어 있는 토큰 정보로 확인한다. 토큰으로 정상적인 유저를 가져올 수 있다면 유효한 유저라고 판단한다.\n\nauthentication.py\n\n```\nfrom rest_framework import authentication\n\nfrom common.utils.jwt_util import obtain_user\n\n\nclass CustomAuthentication(authentication.BaseAuthentication):\n    def authenticate(self, request):\n        \"\"\"\n        쿠키에 저장되어 있는 token 값으로 유저를 가져옵니다. 가져오는 상황에서 예외가 발생할 수 있으며 정상적으로 가져온 경우에는 인증된 유저라고 판단합니다.\n        :param request:\n        :return:\n        \"\"\"\n        token = request.COOKIES.get('token', None)\n        user = obtain_user(token)\n        return (user, None)\n```\n\n쿠키에 저장되어 있는 토큰을 decode 하여 유효한 정보인지를 판단한다.\n\njwt_util.py\n\n```\ndef obtain_user(token: str):\n    \"\"\"\n    payload = {'username': 'adm001', 'email': 'test@mysite.com', 'user_id': 1, 'exp': 1462187582}\n    :param token:\n    :return:\n    \"\"\"\n    jwt_decode_handler = api_settings.JWT_DECODE_HANDLER\n    jwt_get_username_from_payload = api_settings.JWT_PAYLOAD_GET_USERNAME_HANDLER\n\n    if not token or len(token) == 0:  # case1: 토큰을 분실 했을 경우\n        raise serializers.ValidationError('Invalid token header. Non credentials provided.')\n        # raise exceptions.AuthenticationFailed({'error': 'Invalid token header. No credentials provided.'})\n\n    try:\n        payload = jwt_decode_handler(token)\n\n    except jwt.ExpiredSignature:  # case2: 토큰이 만료되었을 경우\n        raise serializers.ValidationError('Signature has expired.')\n        # raise exceptions.AuthenticationFailed({'error': 'Signature has expired.'})\n    except jwt.DecodeError:  # case3: 디코드 실패\n        raise serializers.ValidationError('Error decoding signature.')\n        # raise exceptions.AuthenticationFailed({'error': 'Error decoding signature.'})\n\n    username = jwt_get_username_from_payload(payload)\n\n    if not username:  # case4: payload 가 잘못되어 username을 가져올 수 없는 경우\n        raise serializers.ValidationError('Invalid payload.')\n        # raise exceptions.AuthenticationFailed({'error': 'Invalid payload.'})\n\n    try:\n        user = CustomUser.objects.get(username=username)\n    except CustomUser.DoesNotExist:  # case5: 해당 유저가 존재하지 않는 경우\n        raise serializers.ValidationError(\"User doesn't exists.\")\n        # raise exceptions.AuthenticationFailed({'error': \"User doesn't exists.\"})\n\n    return user\n```\n\n여러 이유로 인증에 실패할 경우엔 인증 실패 예외가 발생하게 되는데 이는 따로 핸들러로 처리해주어야 한다. (에러 정보는 response.data 에 담겨온다.)\n(여기서는 인증 실패 예외가 발생할 경우 쿠키에 저장되어 있는 토큰값을 지워버리고 로그인 페이지로 보내버린다.)\n\ncustom_exception_handler.py\n\n```\nfrom django.conf import settings\nfrom django.core.urlresolvers import reverse\nfrom django.http import HttpResponseRedirect\nfrom rest_framework import status\nfrom rest_framework.views import exception_handler\n\n\ndef custom_exception_handler(exc, context):\n    # 발생한 exception을 가져온다.\n    response = exception_handler(exc, context)\n\n    if response is not None:\n        # response_data = response.data['detail']\n        # print('response : ', response.__dict__)\n\n        # 예외가 발생할 경우엔 token쿠키를 지워준다\n        response.delete_cookie('token', domain=settings.COOKIE_DOMAIN)\n\n        return HttpResponseRedirect(reverse('login'))\n    else:\n        return None\n```\n\n#### 페이지 접근 허가 정보 확인하기\n접근하고자 하는 페이지가 각 유저별로 인증 정보가 필요하다면 유저의 인증 정보를 확인할 수 있어야 한다.\n\npermission.py\n\n```\nfrom rest_framework.permissions import BasePermission\n\n\nclass CustomPermission(BasePermission):\n    def has_permission(self, request, view):\n        # TODO: do something\n        print('CustomPermission user : ', request.user)\n        return True\n```\n\n(현재는 어떤 허가 정보도 판단하지 않지만 이 부분에서 유저 정보를 가지고 해당 페이지에 대한 허가 정보를 판단해서 boolean 값으로 반환해준다. True일 경우 허가된 경우고 False일 경우엔 허가되지 않는 유저이다.)\n\nAPIView 에서 허가 정보도 명시적으로 선언해서 확인할 수 있다.\n\nview.py\n\n```\nclass RootView(APIView):\n    authentication_classes = (CustomAuthentication, )\n    permission_classes = (CustomPermission, )\n    def get(self, request):\n        \"\"\"\n        :param request:\n        :return:\n        \"\"\"\n        redirect_url = \"/home\"\n        return HttpResponseRedirect(redirect_url)\n```\n\n마찬가지로 모든 APIView 에서 허가 정보를 확인해야 한다면 settings 에 설정해줄 수 있다. 좀 전에 인증 정보를 작성한 부분에 추가해주자.\n\nsettings.py\n\n```\nREST_FRAMEWORK = {\n    'DEFAULT_AUTHENTICATION_CLASSES': (\n        'common.utils.authentication.CustomAuthentication',\n    ),\n    'DEFAULT_PERMISSION_CLASSES': (\n        'common.utils.permission.CustomPermission',\n    ),\n    'EXCEPTION_HANDLER': 'common.utils.custom_exception_handler.custom_exception_handler'\n}\n```\n\n허가 요청에 대한 부분이 추가되었으므로 예외 처리 핸들러에서 이 부분에 대한 처리도 추가해줘야 한다. 요청이 실패할 경우 403 에러가 발생하는데 이 부분에 대한 처리를 해준다.\n\ncustom_exception_handler.py\n\n```\nfrom django.conf import settings\nfrom django.core.urlresolvers import reverse\nfrom django.http import HttpResponseRedirect\nfrom rest_framework import status\nfrom rest_framework.views import exception_handler\n\n\ndef custom_exception_handler(exc, context):\n    # 발생한 exception을 가져온다.\n    response = exception_handler(exc, context)\n\n    if response is not None:\n        # response_data = response.data['detail']\n        # print('response : ', response.__dict__)\n\n        # permission 이 안되는 경우엔 block page 로 넘겨준다.\n        if response.status_code == status.HTTP_403_FORBIDDEN:\n            return HttpResponseRedirect(reverse('block'))\n\n        # 예외가 발생할 경우엔 token쿠키를 지워준다\n        response.delete_cookie('token', domain=settings.COOKIE_DOMAIN)\n\n        return HttpResponseRedirect(reverse('login'))\n    else:\n        return None\n```\n\n인증 실패인 경우에는 쿠키에서 토큰 정보를 삭제하고\n로그인 페이지로 보내버리고,\n허가 요청 거부일 경우에는 block 페이지로 보내버렸는데,\n이 부분은 각 사이트의 상황에 맞게 처리해주면 된다.\n\n#### 로그아웃 하기\n로그아웃은 간단하다. 단지 쿠키에 저장되어 있는 토큰 값을 지워버리면 되기 때문이다.\n\nview.py\n\n```\nclass LogoutView(View):\n\n    def get(self, request):\n        response = HttpResponseRedirect(reverse('login'))\n        response.delete_cookie('token', domain=settings.COOKIE_DOMAIN)\n        return response\n```\n\n#### 요약\n지금까지 추가된 파일들과 클래스를 정리하자면 다음과 같다.\n\n```\ncommon/utils/custom_exception_handler.py\n    - custom_exception_handler\ncommon/utils/authentication.py\n    - CustomAuthentication\ncommon/utils/permission.py\n    - CustomerPermission\ncommon/utils/jwt_util.py\n    - obtain_token\n    - obtain_user\n```\n\n여기까지 따라왔다면 sso를 완성 할 수 있다. 하나의 사이트에서 동작하는지 여부를 검사하고 싶다면 hosts 에 도메인을 추가해서 테스트 해보자.\n\n```\n$ sudo vim /etc/hosts\n```\n\n/etc/hosts\n\n```\n127.0.0.1   alpha.mysite.com\n127.0.0.1   beta.mysite.com\n```\n\n위와 같이 추가하고 테스트를 해볼 수 있다.\n\n## 결론\n\n유저의 로그인 검증 후 유저 정보를 jwt 를 이용해서 토큰을 생성한 후, 이 정보는 메인 도메인으로 쿠키에 저장을 한다. 인증이 필요한 페이지의 경우 cookie 에 저장되어 있는 토큰으로 인증된 유저인지 여부를 판단한다. 이 때, 이 토큰 값이 유효하지 않을 경우엔 강제로 로그아웃 처리를 한다. jwt 토큰의 만료 시간을 주어 일정 시간이 지나면 로그인이 풀리도록 했다. 메인 도메인 하위의 3개의 사이트중 하나의 사이트에서만 로그인을 하면 나머지 두개의 사이트에서도 로그인이 되어 있다(같은 쿠키의 토큰을 사용하기 때문)\n\n\n### 추신\n\n나름 간단하게 기능을 구현하고자 했는데 로그인 정보를 저장하고 그 토큰을 처리하기 위해 여러 처리가 붙었다. 그리고 토큰 값을 만들고 검증하고 이를 인증하는 코드들을 직접 쓰다보니 조금은 번거로운 작업들이 포함되어 있는 건 사실이다.. 때로는 오픈소스를 별다른 이해 없이 기능을 사용하기 위해 가져다 쓰는 것 보다 내부 로직을 이해하고 내가 필요한 부분들만 가져다 쓰는 작업을 하면 재미있기도 하고 후에 문제가 생기거나 기능 변경이 생겼을 때 대처가 빠르다. 물론 이런 방식이 정답을 아닐지는 몰라도 덕분에 재미있는 작업을 했다는 점에 만족한다.\n\n\n","fields":{"slug":"/django-sso/"},"frontmatter":{"title":"Django에서 SSO하기","published":true}}},{"node":{"rawMarkdownBody":"View에서 get, post요청을 할 때 template단에서 ajax요청을 날려야할 때가 있다.\nAPIVIew, View 두개의 경우를 살펴보자.\n1.APIView ↔ ajax (post, get)\n(post의 케이스로 설명을 하였지만 get도 동일한 방식이다.)\n가장 빈번하게 사용하는 케이스이다. template 에서 버튼을 눌러 요청을 ajax로 날려 결과값을 받아와서 전체 화면을 다시 그리지 않고 필요한 부분만 갱신해줄 수 있다.\n\n- template.html\n\n~~~\n$.ajax({\n    url: \"{% url 'request:url' %}\",\n    type: 'POST',\n    data: {\n        'user_id': user_id\n    },\n    success: function (response) {\n        // TODO: do something.\n    },\n    error: function (err) {\n        console.log(err);\n    }\n});\n~~~\n\n\n\n- url.py\n\n ```\nurl(r'^request/url$', views.TestView.as_view(), name='url'),\n  ```\n\n- view.py\n\n```\nclass TestView(APIView):\n    def post(self, request):\n        user_id = request.POST.get('user_id')\n      # do something\n       return Response()\n```\n\n\nView class를 상속받고 post 메소드로 받는다면 ajax 요청시 csrf 토큰이 없다고 하면서 403에러가 발생한다.\n이를 해결해주기 위해 APIView를 상속받아서 처리하면 csrf 인증을 피해갈 수 있다.  (피해간다기보단 이미 인증이 되어있는 요청이라고 판단하는 것이지만)\n이는 APIView의 as_view() 메소드를 보면 알 수 있는데\n\n- api_view.py\n\n```\ndef as_view(cls, **initkwargs):\n    \"\"\"\n    Store the original class on the view function.\n\n    This allows us to discover information about the view when we do URL\n    reverse lookups.  Used for breadcrumb generation.\n    \"\"\"\n    if isinstance(getattr(cls, 'queryset', None), models.query.QuerySet):\n        def force_evaluation():\n            raise RuntimeError(\n                'Do not evaluate the `.queryset` attribute directly, '\n                'as the result will be cached and reused between requests. '\n                'Use `.all()` or call `.get_queryset()` instead.'\n            )\n        cls.queryset._fetch_all = force_evaluation\n        cls.queryset._result_iter = force_evaluation  # Django <= 1.5\n\n    view = super(APIView, cls).as_view(**initkwargs)\n    view.cls = cls\n\n    # Note: session based authentication is explicitly CSRF validated,\n    # all other authentication is CSRF exempt.\n    return csrf_exempt(view)\n```\n\n마지막 라인에 csrf_exempt로 view를 감싸준다. 이는 해당 API요청이 csrf 인증이 되어있다고 명시해주는 것이다.\n그리고, 내부로 파고 들어가면  APIView에서 authentication_classes = api_settings.DEFAULT_AUTHENTICATION_CLASSES 로 설정이 되어있는데, restframework의 setting을 살펴보면 아래와 같이 정의가 되어있음을 알 수 있다.\n\n- api_settings.py\n\n```\n'DEFAULT_AUTHENTICATION_CLASSES': (\n    'rest_framework.authentication.SessionAuthentication',\n    'rest_framework.authentication.BasicAuthentication'\n),\n\n# Authentication\n'UNAUTHENTICATED_USER': 'django.contrib.auth.models.AnonymousUser',\n'UNAUTHENTICATED_TOKEN': None,\n```\n\n인증 처리를 rest_framework의 SessionAuthentication으로 하겠다는 내용과 인증 유저를 AnonymousUser 로 설정을 해준다.\n기존의 인증절차에서 default인증 객체를 검사하게 되는데, 이 때 user객체의 active를 검사할 때 이 객체가 유효하지 않으면 unauthenticated_user로 AnonymousUser객체를 생성하게 된다. 접근 허용 체크를 할 때는 APIView에 정의된 permission class로 체크를 하는데 아래와 같다.\n\n- api_settings.py\n\n```\n'DEFAULT_PERMISSION_CLASSES': (\n    'rest_framework.permissions.AllowAny',\n),\n```\n\nAllowAny Class는 permission요청에 대해 무조건 True를 반환해주므로, APIView의 check_permissions 메소드에서 권한 체크를 할 때 권한이 있다고 판단을 한다.\n\n- permissions.py\n\n```\nclass AllowAny(BasePermission):\n    \"\"\"\n    Allow any access.\n    This isn't strictly required, since you could use an empty\n    permission_classes list, but it's useful because it makes the intention\n    more explicit.\n    \"\"\"\n    def has_permission(self, request, view):\n        return True\n```\n\n즉, APIView는 기존의 django 에서 체크하고 있는 csrf 공격방어에 대한 인증을 제외하고 따로 인증절차를 진행하게되는데, 인증 유저를 익명유저로 설정하여 유저에 대한 권한검사를 하지 않고 default로 설정이 된 권한체크 클래스로 검사를 한다.\nAllowAny는 접근 허용여부를 모두 True로 반환하여 접근에 대해 유효하다고 판단하여 csrf토큰 없이 요청 수행이 가능한 것이다.\n데이터를 모두 처리하고 나서의 응답값은 Response 객체를 반환함으로서 처리한다.\n\n2.View ↔ ajax (post, get)\najax를 통해 View class에 메소드를 호출하는 경우는 많지는 않았는데, 이유는 굳이 ajax로 호출하지 않아도 할 수 있는 방법들이 있었기 때문이었다.\n하지만 종종 사용해야할 경우가 있었는데, 이를테면 내부 검색창에서 검색결과를 내부 포멧에 맞게 내용을 채워줘야 하는데 그 template가 별도의 파일로 있는 경우였다.\n\n\n- template.html\n\n~~~\n$(\"#searchFrom\").submit(function () {\n    var user_id = $(\"input[name=user_id]\").val();\n\n    $.ajax({\n        url: \"{% url 'user:search' %}\",\n        type: \"POST\",\n        data: {\n            \"csrfmiddlewaretoken\": \"\\{\\{ csrf_token \\}\\}\",\n            'user_id': user_id,\n        },\n        success: function (response) {\n            $('#user_list tbody').html(response);\n        },\n        error: function (err) {\n            console.log(err);\n        }\n    });\n\n    return false;\n});\n~~~\n\n{% raw %}\n위의 tempalate에서 입력받은 데이터를 통해 ajax로 호출을 한다. 이 때 호출되는 class는 View를 상속받은 class 이다. form의 submit으로 호출을 한 경우인데 form에 {% csrf_token %}이 들어가있기 때문에 post요청을 날릴 경우엔 403이 떨어진다.(get요청은 안넣어도 상관없다.)\n이런 경우를 방지하기 위해 data필드에 csrf token 을 넣어준다.\n{% endraw %}\n\n~~~\n\"csrfmiddlewaretoken\": \"{{ csrf_token }}\",\n~~~\n\n요청을 받고 데이터를 만들고 나면 TemplateResponse를 사용하여 분리되어 생성된 페이지를 반환하게 된다.\n\n- view.py\n\n```\nclass UserSearchView(View):\n    def post(self, request):\n        user_id = request.POST.get('user_id')\n        user_list = service.search_user(user_id)\n        context = {\n            'user_list': user_list\n        }\n        return TemplateResponse(request, 'user/user_list.html', context)\n```\n\n다시 template에서는 넘어온 html덩어리를 미리 정의해둔 위치에 그대로 붙여줌으로서 화면을 다시 그리지 않고, 필요한 부분만 넣어줄 수 있다.\n(APIView등을 통해 데이터만 받아와 다시 그리는 방법도 있지만 성격이 맞지 않는다고 생각하였고, 일반 호출을 하게 되면 페이지로딩을 다시 하므로 그 방법은 피했다)\n\n\n","fields":{"slug":"/django-ajax/"},"frontmatter":{"title":"Django에서 ajax요청하기","published":true}}},{"node":{"rawMarkdownBody":"Django framework를 이용해서 프로젝트를 만들면서 했던 여러가지 삽질들과 넘어야했던 많은 산들과 몰랐던 부분들에 대해서 내 나름 공부 겸 기록을 위해 남겨둬야 겠다는 생각이 들었다. 프로젝트로 한창 바빴던 시기에는 당장 앞에 놓여진 일들에 치여 이런 생각을 못하고 있다가 지금은 조금 여유가 되어 그 때 있었던 일들에 대한 나름의 경험을 기록해보고자 한다.\n\n그 중에 한가지가 인앱 아이템 결제하기 였다. 나는 예전에 cocos2d-x로 게임을 개발했던 시절엔 결제 프로세스를 클라이언트(단말기)사이드만 알고 있었고 서버사이드의 로직은 전혀 모르고 있었는데, 오히려 이번에 서버 로직을 구현하면서 그 때 그 시절의 경험이 많은 도움이 되었다. 앱 내에 아이템을 구매하고 구매한 아이템을 유저에게 잘 넣어주는 로직이야 구현방식도 다양하고 여러 기교들이 들어가 있는데 오늘은 그 중에서 나름 간단한(?) 파트인 두 플랫폼의 결제 영수증 검증 로직의 삽질 경험을 적어보고자 한다.\n\n이번에 대응했던 클라이언트는 google play store와 iOS app store 두개의 플랫폼이다. 차례차례 훑어보자.\n각 플랫폼별 스토어의 앱 등록과 각종 키발급 같은 내용은 다루지 않고 순수하게 영수증 검증만을 볼 것이다.\n\n1. google In-app billing\n구글 결제 영수증 검증의 경우 생각보다 단순해서 처음에는 이게 맞나? 싶었다. 일반적으로 생각하기에는 구글의 결제서버에 영수증 정보를 넘겨 유효한 영수증인지 여부를 판단해서 진행이 되는줄 알았는데(이렇게 하는 방법도 있다고 한다.), 훨씬 간단한 방법이 있었다. 암호화된 영수증 정보를 클라이언트로 부터 받아와서 local에서 검증하는 방식이다.\n\n일단 pip를 이용해서 Crypto 라이브러리를 설치하도록 하자.\n\n```\npip install pycrypto\n```\n\n\n다음은 영수증 검증 코드이다. signed_data 는 안드로이드 결제완료시 넘어온 암호화된 영수증 문자열이고 signature는 클라이언트와 약속된 특정 문자열이다. 안드로이드에서 결제를 요청할 당시 이 값을 함께 넘겨주면 signed_data 내부에 이 signature 값이 심어들어가게 되고, 복호화를 하면 이 값이 풀어져 나와 영수증 검증을 할 수 있다.\n\n```\nfrom base64 import b64decode\nfrom Crypto.Hash import SHA\nfrom Crypto.PublicKey import RSA\nfrom Crypto.Signature import PKCS1_v1_5\n\n# Your base64 encoded public key from Google Play.\nPUBLIC_KEY_BASE64 = 'YOUR_PUBLIC_KEY_BASE64'\n\ndef verify_for_google(signed_data, signature):\n    \"\"\"Returns whether the given data was signed with the private key.\"\"\"\n    key = RSA.importKey(_pem_format(PUBLIC_KEY_BASE64))\n    verifier = PKCS1_v1_5.new(key)\n    data = SHA.new(signed_data.encode('utf8'))\n    sig = b64decode(signature)\n\n    return verifier.verify(data, sig)\n\n\ndef _pem_format(key):\n    return '\\n'.join([\n        '-----BEGIN PUBLIC KEY-----',\n        '\\n'.join(_chunks(key, 64)),\n        '-----END PUBLIC KEY-----'\n    ])\n\n\ndef _chunks(s, n):\n    for start in range(0, len(s), n):\n        yield s[start:start+n]\n```\n\n\n먼저 스토어에서 발급받은 공개키가 필요하다. 이 키를 이용해 넘어온 데이터가 유효한지 여부를 판단할 수 있다.\n1. RSA 암호 방식으로 암호화가 되어 있기 때문에 PUBLIC KEY 를 이용해 key객체를 생성한다.\n2. PKCS(Public Key Cryptography Standard)를 이용해서 암호화된 영수증을 검증 할것이다. 1번에서 만들어진 key를 이용해 verifier 객체를 생성하자.\n3. SHA(Secure Hash Algorithm)으로 된 signed_data를 객체로 생성한다.\n4. signature 를 decode 한다.\n5. 2번에서 생성한 verifier를 통해 data 와 sig 의 유효성을 판단한다. 유효한 영수증은 True 그렇지 않으면 False 를 반환한다.\n이 부분에 대해서는 좀 더 공부가 필요해 보인다.. 현재는 암호화에 대한 꼭지들이 의미하는 바를 인지해야겠지만 이들이 어떤 작업을 하는지에 대한 공부도 반드시 필요할 것이다.\n코드 내용이 그리 간단하지는 않지만 비교적 빠르게 구글 영수증 검증을 할 수 있었다. 구글의 개발자 사이트를 들어가 이런 저런 글도 읽오보고 여러 포스팅을 찾아보면서 여러가지 자료를 봐왔었는데 stackoverflow 에서 찾은 글들을 추려 간단하게 메소드로 정리했다. 내가 여기서 삽질 했던 부분은 대체 signed_data 와 signature 가 무엇을 뜻하는지 알 수가 없었다는 점이였다. 대부분의 자료들에서는 저 둘이 무엇을 의미하는지 자세히 기술을 해놓지 않거나 아예 어떤 값을 의미하는지를 써놓지 않은 글들이 많았다. 나도 처음에는 이리저리 고민을 해보다가 일단 맨땅에 헤딩을 하고보자는 마음으로 여러 땅에 삽집을 하다가 찾은 결과물이다.\n\n\n2. iOS in-app purchase\n이번에는 iOS결제 영수증 검증을 해보도록 하자. apple은 구글과는 다르게 결제 서버에 영수증을 보내서 넘어온 값을 통해 영수증의 유효성을 판단한다. 직접 apple의 결제 api를 호출해서 검증을 해도 되지만 역시나 Python은 있을만하다고 생각하는 라이브러리는 이미 존재하거나 누군가가 만들어 놨다. 우리는 이 라이브러리를 만든분 께 감사를 드리며 코드를 받아가도록 하자.\n\n```\npip install itunes-iap\n```\n\nitunes-iap를 설치하면 의존성 라이브러리들이 이것저것 왕창 설치된다. 궁금하면 ($pip list 를 통해 확인해볼것)\n이번에는 iOS 영수증 검증 코드를 보도록 하자. 이 때 transaction_id 는 결제 당시 결과값으로 오는 구매 영수증 id이고, raw_data는 암호화된 영수증 문자열이다.\n\n```\nimport itunesiap\n\ndef _verify_for_ios(transaction_id: str, raw_data: str):\n    \"\"\"\n    seeAlso : https://developer.apple.com/library/ios/releasenotes/General/ValidateAppStoreReceipt/Chapters/ValidateRemotely.html\n    :param transaction_id: 결제 transaction_id\n    :param raw_data: base64-encoded data\n    :return: boolean\n    :raises: Otherwise raise a request exception (RuntimeError, itunesiap.exc.InvalidReceipt)\n    \"\"\"\n    try:\n        # for sandbox environment.\n        #     with itunesiap.env.sandbox:\n        #         response = itunesiap.verify(raw_data)\n\n        # for production environment. (default)\n        response = itunesiap.verify(raw_data)  # base64-encoded data\n\n        def _get_key(re):\n            \"\"\" 영수증리스트에서 비교 키를 반환합니다. \"\"\"\n            return re.purchase_date_ms\n\n        # 넘어온 in_app 영수증 리스트에서 구매 시각이 가장 마지막인 영수증을 가져와서 transaction_id를 비교한다.\n        # 오름 차순으로 정렬해서 구매시각이 가장 마지막 영수증을 가져옵니다.\n        receipts = sorted(response.receipt.in_app, key=_get_key)\n        last_receipt = receipts[len(receipts) - 1]\n        if last_receipt.transaction_id != transaction_id:\n            #  구매시각이 가장 마지막인 영수증의 transaction_id 가 일치 하지 않는다.\n            return False\n\n        return response.status == 0\n```\n\niOS 영수증 유효성 검증은 간단하다. itunesiap.verify 메소드를 통해 iOS에서 넘어온 데이터를 넣고 호출하면 복호화된 영수증 데이터가 나온다. 이 때 status 값이 0 이라면 유효한 영수증이라고 판단한다. 뭔가 apple다운 api라고 생각한다. 처음 iOS영수증 검증을 할 당시엔 verify 메소드만 호출하고 그 뒤에 넘어온 데이터의 status 값만 확인하고 유효성을 검증했다.\n여기서 나의 삽질이 시작되었다.\n\n1. 일단 주석에서 보는것과 같이 sandbox환경과 real환경일 때에 호출하는 api가 다르다. itunesiap의 환경을 설정을 해주는 방법은 여러가지가 있다. 각자 개발환경에서 영수증 유효성을 검사할 때엔 sandbox와 real환경을 잘 구분해서 api를 호출해주도록 하자. 아무리 유효한 영수증이라 할지라도 틀린 환경의 api를 호출하면 유효하지 않다고 판단하기 때문이다.\n\n2. 보통 verify 메소드를 호출하면 receipt 값 안에 하나의 영수증 정보만 가져온다. 하지만 특정 상황의 경우 receipt 데이터 안에 1개 이상의 영수증 정보가 딸려 오는 경우가 있다. 이 때에 한가지 데이터만이 유효한 영수증 값이다. 이 중에서 한가지 영수증만을 가지고 검증을 해야 하는데 여러 삽질 후에 깨달은 바는 receipt 안의 데이터들의 purchase_date_ms 값을 이용해 가장 마지막에 결제된 영수증 정보가 파라메터로 넘어온 transaction_id 와 동일하다는 점을 알게 되었다. 그래서 _get_key 메소드를 통해 receipt 리스트를 정렬하고, 그 중에 가장 마지막 영수증 정보를 뽑아 transaction_id 가 같은지 여부를 판단한다.\n\n자 이제 우리는 python을 이용해서 android, iOS의 영수증을 검증할 수 있게 되었다. 간단하지도 복잡하지도 않고, 몰랐을 때는 어려워 난해했지만 알고나니 더 난해한 비교적 간단하게 할 수 있는 파트였다. 이제 어디가서 이 두 플랫폼의 결제 영수증쯤은 아무렇지도 않게 검증 할 수 있다고 당당히 얘기하자.\n\n\n","fields":{"slug":"/store-inapp-in-python/"},"frontmatter":{"title":"Python에서 iOS, Android 스토어 인앱 결제 검증하기","published":true}}}]}},"pageContext":{"isCreatedByStatefulCreatePages":true}}}